# enhanced_semantic_segmentation_detector.py
import torch
import cv2
import numpy as np
from groundingdino.util.inference import load_model, load_image, predict
from groundingdino.util import box_ops
from PIL import Image
import torchvision
import time
import os
import colorsys
import glob
from pathlib import Path
import json
from datetime import datetime

# SAM imports
try:
    from segment_anything import SamPredictor, sam_model_registry

    SAM_AVAILABLE = True
    print("âœ… SAM is available")
except ImportError:
    SAM_AVAILABLE = False
    print("âŒ SAM not available. Install with: pip install segment-anything")


class EnhancedSemanticSegmentationDetector:
    """å¢å¼ºçš„è¯­ä¹‰åˆ†å‰²æ£€æµ‹å™¨ - ç”Ÿæˆè¯­ä¹‰ç±»åˆ«maskå›¾"""

    def __init__(self, grounding_dino_config, grounding_dino_checkpoint,
                 sam_checkpoint=None, device="cuda"):
        self.device = device
        self.grounding_dino_model = None
        self.sam_predictor = None

        # åŠ è½½Grounding DINO
        self.load_grounding_dino(grounding_dino_config, grounding_dino_checkpoint)

        # åŠ è½½SAM
        if sam_checkpoint and SAM_AVAILABLE:
            self.load_sam(sam_checkpoint)
        else:
            print("âš ï¸  SAMæœªå¯ç”¨ï¼Œå°†ä»…ä½¿ç”¨è¾¹ç•Œæ¡†")

        # å®šä¹‰è¯­ä¹‰ç±»åˆ«å’Œé¢œè‰²
        self.setup_semantic_categories()

        # æ£€æµ‹å‚æ•°
        self.box_threshold = 0.15
        self.text_threshold = 0.15
        self.min_confidence = 0.15

    def load_grounding_dino(self, config_path, checkpoint_path):
        """åŠ è½½Grounding DINOæ¨¡å‹"""
        print("â³ åŠ è½½Grounding DINOæ¨¡å‹...")
        try:
            self.grounding_dino_model = load_model(config_path, checkpoint_path, device=self.device)
            print("âœ… Grounding DINOæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ Grounding DINOæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise e

    def load_sam(self, sam_checkpoint):
        """åŠ è½½SAMæ¨¡å‹"""
        print("â³ åŠ è½½SAMæ¨¡å‹...")
        try:
            if "vit_h" in sam_checkpoint:
                model_type = "vit_h"
            elif "vit_l" in sam_checkpoint:
                model_type = "vit_l"
            else:
                model_type = "vit_b"

            print(f"   ä½¿ç”¨SAMæ¨¡å‹ç±»å‹: {model_type}")
            sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
            sam.to(device=self.device)
            self.sam_predictor = SamPredictor(sam)
            print("âœ… SAMæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ SAMæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.sam_predictor = None

    def setup_semantic_categories(self):
        """è®¾ç½®è¯­ä¹‰ç±»åˆ«å’Œé¢œè‰²"""

        # å®šä¹‰è¯¦ç»†çš„è¯­ä¹‰ç±»åˆ«
        self.semantic_categories = {
            # åœ°é¢ç›¸å…³ - ç´«çº¢è‰²ç³»
            "ground": {
                "keywords": ["road", "street", "pavement", "sidewalk", "ground", "asphalt",
                             "concrete", "pathway", "walkway", "lane", "highway"],
                "color": [128, 0, 128],  # ç´«çº¢è‰²
                "id": 1
            },

            # å»ºç­‘ç‰© - ç°è‰²ç³»
            "building": {
                "keywords": ["building", "house", "structure", "architecture", "edifice",
                             "construction", "facility", "office", "residential"],
                "color": [128, 128, 128],  # ç°è‰²
                "id": 2
            },

            # å¢™ä½“å’Œå›´æ  - è¤è‰²ç³»
            "wall_fence": {
                "keywords": ["wall", "fence", "barrier", "railing", "guardrail", "partition"],
                "color": [139, 69, 19],  # è¤è‰²
                "id": 3
            },

            # æ¤è¢« - ç»¿è‰²ç³»
            "vegetation": {
                "keywords": ["tree", "bush", "plant", "vegetation", "grass", "shrub",
                             "foliage", "garden", "park", "lawn"],
                "color": [0, 128, 0],  # ç»¿è‰²
                "id": 4
            },

            # è½¦è¾† - çº¢è‰²ç³»
            "vehicle": {
                "keywords": ["car", "truck", "bus", "van", "suv", "vehicle", "automobile",
                             "motorcycle", "motorbike", "taxi", "sedan"],
                "color": [255, 0, 0],  # çº¢è‰²
                "id": 5
            },

            # äºº - è“è‰²ç³»
            "person": {
                "keywords": ["person", "people", "pedestrian", "human", "individual",
                             "man", "woman", "child"],
                "color": [0, 0, 255],  # è“è‰²
                "id": 6
            },

            # è‡ªè¡Œè½¦ - æ©™è‰²ç³»
            "bicycle": {
                "keywords": ["bicycle", "bike", "cycling", "cyclist"],
                "color": [255, 165, 0],  # æ©™è‰²
                "id": 7
            },

            # äº¤é€šè®¾æ–½ - é»„è‰²ç³»
            "traffic": {
                "keywords": ["traffic light", "traffic sign", "sign", "signal", "pole",
                             "street lamp", "light pole", "stop sign", "traffic signal"],
                "color": [255, 255, 0],  # é»„è‰²
                "id": 8
            },

            # å¤©ç©º - é’è‰²ç³»
            "sky": {
                "keywords": ["sky", "cloud", "air", "atmosphere"],
                "color": [0, 255, 255],  # é’è‰²
                "id": 9
            },

            # å…¶ä»–ç‰©ä½“ - ç™½è‰²
            "other": {
                "keywords": ["object", "item", "thing"],
                "color": [255, 255, 255],  # ç™½è‰²
                "id": 10
            }
        }

        # åˆ›å»ºé¢œè‰²æ˜ å°„å­—å…¸
        self.color_map = {}
        self.id_to_color = {}
        for category, info in self.semantic_categories.items():
            for keyword in info["keywords"]:
                self.color_map[keyword.lower()] = info["color"]
            self.id_to_color[info["id"]] = info["color"]

        print("ğŸ¨ è¯­ä¹‰ç±»åˆ«è®¾ç½®å®Œæˆ:")
        for category, info in self.semantic_categories.items():
            print(f"   {category}: {info['color']} (ID: {info['id']})")

    def classify_semantic_category(self, phrase):
        """å°†æ£€æµ‹åˆ°çš„ç‰©ä½“åˆ†ç±»åˆ°è¯­ä¹‰ç±»åˆ«"""
        phrase_lower = phrase.lower().strip()

        # éå†æ‰€æœ‰è¯­ä¹‰ç±»åˆ«
        for category, info in self.semantic_categories.items():
            for keyword in info["keywords"]:
                if keyword in phrase_lower:
                    return category, info["color"], info["id"]

        # é»˜è®¤è¿”å›otherç±»åˆ«
        return "other", self.semantic_categories["other"]["color"], self.semantic_categories["other"]["id"]

    def create_semantic_prompts(self):
        """åˆ›å»ºè¯­ä¹‰åˆ†å‰²çš„æç¤ºè¯"""
        # æ”¶é›†æ‰€æœ‰å…³é”®è¯
        all_keywords = []
        for category, info in self.semantic_categories.items():
            all_keywords.extend(info["keywords"][:3])  # æ¯ä¸ªç±»åˆ«å–å‰3ä¸ªå…³é”®è¯

        # åˆ›å»ºæç¤ºè¯
        prompt = " . ".join(all_keywords) + " ."
        return prompt

    def generate_semantic_masks(self, image_path, save_dir="./semantic_results"):
        """ç”Ÿæˆè¯­ä¹‰åˆ†å‰²maskå›¾"""
        print(f"ğŸ¯ å¼€å§‹ç”Ÿæˆè¯­ä¹‰åˆ†å‰²mask: {image_path}")

        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        os.makedirs(os.path.join(save_dir, "semantic_masks"), exist_ok=True)
        os.makedirs(os.path.join(save_dir, "overlay_results"), exist_ok=True)

        # åŠ è½½å›¾åƒ
        image_source, image = load_image(image_path)
        h, w, _ = image_source.shape
        print(f"   å›¾åƒå°ºå¯¸: {w}x{h}")

        # è®¾ç½®SAMå›¾åƒ
        if self.sam_predictor:
            self.sam_predictor.set_image(image_source)

        # åˆ›å»ºè¯­ä¹‰æç¤ºè¯
        semantic_prompt = self.create_semantic_prompts()
        print(f"ğŸ¯ è¯­ä¹‰æç¤ºè¯: {semantic_prompt[:100]}...")

        # æ‰§è¡Œæ£€æµ‹
        detections = self._detect_semantic_objects(image, semantic_prompt, w, h)

        # ç”Ÿæˆè¯­ä¹‰mask
        semantic_mask, overlay_image = self._create_semantic_mask(image_source, detections, w, h)

        # ä¿å­˜ç»“æœ
        base_name = os.path.splitext(os.path.basename(image_path))[0]
        self._save_semantic_results(semantic_mask, overlay_image, image_source, save_dir, base_name, detections)

        return semantic_mask, overlay_image, detections

    def _detect_semantic_objects(self, image, prompt, w, h):
        """æ£€æµ‹è¯­ä¹‰ç‰©ä½“"""
        detections = []

        try:
            with torch.no_grad():
                boxes, logits, phrases = predict(
                    model=self.grounding_dino_model,
                    image=image,
                    caption=prompt,
                    box_threshold=self.box_threshold,
                    text_threshold=self.text_threshold,
                    device=self.device
                )

            print(f"   æ£€æµ‹åˆ° {len(boxes)} ä¸ªç‰©ä½“")

            # å¤„ç†æ¯ä¸ªæ£€æµ‹ç»“æœ
            for i, (box, logit, phrase) in enumerate(zip(boxes, logits, phrases)):
                confidence = logit.item()

                if confidence < self.min_confidence:
                    continue

                # è½¬æ¢åæ ‡
                box_xyxy = box_ops.box_cxcywh_to_xyxy(box.unsqueeze(0)) * torch.tensor([w, h, w, h])
                x1, y1, x2, y2 = box_xyxy[0].cpu().numpy().astype(int)

                # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
                x1, y1 = max(0, x1), max(0, y1)
                x2, y2 = min(w, x2), min(h, y2)

                # è¯­ä¹‰åˆ†ç±»
                category, color, category_id = self.classify_semantic_category(phrase)

                detection = {
                    'box': box,
                    'box_xyxy': box_xyxy[0],
                    'confidence': confidence,
                    'phrase': phrase,
                    'category': category,
                    'color': color,
                    'category_id': category_id,
                    'sam_mask': None,
                    'sam_success': False
                }

                # SAMåˆ†å‰²
                if self.sam_predictor:
                    try:
                        input_box = np.array([x1, y1, x2, y2])
                        masks, sam_scores, _ = self.sam_predictor.predict(
                            point_coords=None,
                            point_labels=None,
                            box=input_box[None, :],
                            multimask_output=False,
                        )

                        if len(masks) > 0 and masks[0] is not None:
                            sam_mask = masks[0].astype(np.uint8)
                            detection['sam_mask'] = sam_mask
                            detection['sam_success'] = True
                            detection['sam_score'] = sam_scores[0] if len(sam_scores) > 0 else 0.0

                    except Exception as e:
                        print(f"   âŒ SAMåˆ†å‰²å‡ºé”™: {phrase} - {e}")

                detections.append(detection)

            # æŒ‰ç½®ä¿¡åº¦æ’åº
            detections.sort(key=lambda x: x['confidence'], reverse=True)
            print(f"   ä¿ç•™ {len(detections)} ä¸ªæœ‰æ•ˆæ£€æµ‹")

        except Exception as e:
            print(f"âŒ è¯­ä¹‰æ£€æµ‹å¤±è´¥: {e}")

        return detections

    def _create_semantic_mask(self, image_source, detections, w, h):
        """åˆ›å»ºè¯­ä¹‰åˆ†å‰²mask"""
        print("ğŸ¨ ç”Ÿæˆè¯­ä¹‰åˆ†å‰²mask...")

        # åˆ›å»ºè¯­ä¹‰mask (H, W)ï¼Œåˆå§‹åŒ–ä¸º0ï¼ˆèƒŒæ™¯ï¼‰
        semantic_mask = np.zeros((h, w), dtype=np.uint8)

        # åˆ›å»ºå½©è‰²å åŠ å›¾åƒ
        overlay_image = image_source.copy()

        # ç»Ÿè®¡å„ç±»åˆ«æ•°é‡
        category_counts = {}

        # åº”ç”¨NMSå»é‡
        filtered_detections = self._apply_semantic_nms(detections)

        # æŒ‰ç±»åˆ«IDæ’åºï¼Œç¡®ä¿é‡è¦ç±»åˆ«ï¼ˆå¦‚åœ°é¢ï¼‰ä¼˜å…ˆå¤„ç†
        priority_order = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # åœ°é¢ä¼˜å…ˆ
        sorted_detections = []

        for priority_id in priority_order:
            for det in filtered_detections:
                if det['category_id'] == priority_id:
                    sorted_detections.append(det)

        # å¤„ç†æ¯ä¸ªæ£€æµ‹ç»“æœ
        for det in sorted_detections:
            category = det['category']
            category_id = det['category_id']
            color = det['color']
            sam_mask = det.get('sam_mask')
            box_xyxy = det['box_xyxy']

            # ç»Ÿè®¡ç±»åˆ«
            category_counts[category] = category_counts.get(category, 0) + 1

            # åˆ›å»ºmaskåŒºåŸŸ
            if sam_mask is not None and det['sam_success']:
                # ä½¿ç”¨SAMç²¾ç¡®mask
                mask_bool = sam_mask.astype(bool)
            else:
                # ä½¿ç”¨è¾¹ç•Œæ¡†ä½œä¸ºmask
                x1, y1, x2, y2 = box_xyxy.cpu().numpy().astype(int)
                x1, y1 = max(0, x1), max(0, y1)
                x2, y2 = min(w, x2), min(h, y2)

                mask_bool = np.zeros((h, w), dtype=bool)
                mask_bool[y1:y2, x1:x2] = True

            # æ›´æ–°è¯­ä¹‰maskï¼ˆåªåœ¨å½“å‰ä½ç½®ä¸ºèƒŒæ™¯æ—¶æ›´æ–°ï¼Œé¿å…è¦†ç›–é‡è¦ç±»åˆ«ï¼‰
            update_mask = mask_bool & (semantic_mask == 0)
            semantic_mask[update_mask] = category_id

            # åˆ›å»ºåŠé€æ˜å åŠ 
            alpha = 0.6
            overlay_layer = np.zeros_like(overlay_image)
            overlay_layer[mask_bool] = color
            overlay_image = cv2.addWeighted(overlay_image, 1 - alpha, overlay_layer, alpha, 0)

            # æ·»åŠ è½®å»“
            if sam_mask is not None:
                contours, _ = cv2.findContours(sam_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                cv2.drawContours(overlay_image, contours, -1, color, 2)

        print(f"âœ… è¯­ä¹‰maskç”Ÿæˆå®Œæˆï¼ŒåŒ…å«ç±»åˆ«: {category_counts}")
        return semantic_mask, overlay_image

    def _apply_semantic_nms(self, detections, iou_threshold=0.5):
        """å¯¹è¯­ä¹‰æ£€æµ‹ç»“æœåº”ç”¨NMS"""
        if len(detections) <= 1:
            return detections

        # æŒ‰ç±»åˆ«åˆ†ç»„åº”ç”¨NMS
        category_groups = {}
        for det in detections:
            category = det['category']
            if category not in category_groups:
                category_groups[category] = []
            category_groups[category].append(det)

        filtered_detections = []

        for category, dets in category_groups.items():
            if len(dets) <= 1:
                filtered_detections.extend(dets)
                continue

            # å¯¹æ¯ä¸ªç±»åˆ«åº”ç”¨NMS
            boxes = torch.stack([det['box_xyxy'] for det in dets])
            scores = torch.tensor([det['confidence'] for det in dets])

            keep_indices = torchvision.ops.nms(boxes, scores, iou_threshold)
            category_filtered = [dets[i] for i in keep_indices.cpu().numpy()]
            filtered_detections.extend(category_filtered)

        return filtered_detections

    def _save_semantic_results(self, semantic_mask, overlay_image, original_image, save_dir, base_name, detections):
        """ä¿å­˜è¯­ä¹‰åˆ†å‰²ç»“æœ"""
        print("ğŸ’¾ ä¿å­˜è¯­ä¹‰åˆ†å‰²ç»“æœ...")

        # 1. ä¿å­˜è¯­ä¹‰mask (ç°åº¦å›¾ï¼Œåƒç´ å€¼ä»£è¡¨ç±»åˆ«ID)
        semantic_mask_path = os.path.join(save_dir, "semantic_masks", f"{base_name}_semantic_mask.png")
        cv2.imwrite(semantic_mask_path, semantic_mask)

        # 2. åˆ›å»ºå½©è‰²è¯­ä¹‰mask
        colored_semantic_mask = self._create_colored_semantic_mask(semantic_mask)
        colored_mask_path = os.path.join(save_dir, "semantic_masks", f"{base_name}_colored_semantic.png")
        cv2.imwrite(colored_mask_path, colored_semantic_mask)

        # 3. ä¿å­˜å åŠ ç»“æœ
        overlay_path = os.path.join(save_dir, "overlay_results", f"{base_name}_overlay.jpg")
        cv2.imwrite(overlay_path, overlay_image)

        # 4. åˆ›å»ºå¯¹æ¯”å›¾
        comparison = self._create_comparison_grid(original_image, colored_semantic_mask, overlay_image)
        comparison_path = os.path.join(save_dir, f"{base_name}_comparison.jpg")
        cv2.imwrite(comparison_path, comparison)

        # 5. ä¿å­˜æ£€æµ‹ä¿¡æ¯JSON
        detection_info = {
            'image_name': base_name,
            'total_detections': len(detections),
            'categories': {},
            'detections': []
        }

        for det in detections:
            category = det['category']
            detection_info['categories'][category] = detection_info['categories'].get(category, 0) + 1

            detection_info['detections'].append({
                'phrase': det['phrase'],
                'category': det['category'],
                'category_id': det['category_id'],
                'confidence': float(det['confidence']),
                'color': det['color'],
                'sam_success': det.get('sam_success', False)
            })

        json_path = os.path.join(save_dir, f"{base_name}_semantic_info.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(detection_info, f, indent=2, ensure_ascii=False)

        print(f"âœ… è¯­ä¹‰åˆ†å‰²ç»“æœä¿å­˜å®Œæˆ:")
        print(f"   ğŸ¯ è¯­ä¹‰mask: {semantic_mask_path}")
        print(f"   ğŸŒˆ å½©è‰²mask: {colored_mask_path}")
        print(f"   ğŸ“Š å åŠ ç»“æœ: {overlay_path}")
        print(f"   ğŸ“‹ å¯¹æ¯”å›¾: {comparison_path}")
        print(f"   ğŸ“„ æ£€æµ‹ä¿¡æ¯: {json_path}")

    def _create_colored_semantic_mask(self, semantic_mask):
        """åˆ›å»ºå½©è‰²è¯­ä¹‰mask"""
        h, w = semantic_mask.shape
        colored_mask = np.zeros((h, w, 3), dtype=np.uint8)

        # ä¸ºæ¯ä¸ªç±»åˆ«IDåˆ†é…é¢œè‰²
        for category_id, color in self.id_to_color.items():
            mask_bool = (semantic_mask == category_id)
            colored_mask[mask_bool] = color

        return colored_mask

    def _create_comparison_grid(self, original, colored_mask, overlay):
        """åˆ›å»ºä¸‰å›¾å¯¹æ¯”ç½‘æ ¼"""
        h, w = original.shape[:2]
        target_size = (w // 3, h // 3)

        # è°ƒæ•´å¤§å°
        orig_small = cv2.resize(original, target_size)
        mask_small = cv2.resize(colored_mask, target_size)
        overlay_small = cv2.resize(overlay, target_size)

        # æ·»åŠ æ ‡é¢˜
        def add_title(img, title):
            cv2.putText(img, title, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,
                        0.8, (255, 255, 255), 3)
            cv2.putText(img, title, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,
                        0.8, (0, 0, 0), 2)
            return img

        orig_small = add_title(orig_small, "Original")
        mask_small = add_title(mask_small, "Semantic Mask")
        overlay_small = add_title(overlay_small, "Overlay")

        # æ°´å¹³æ‹¼æ¥
        comparison = np.hstack([orig_small, mask_small, overlay_small])
        return comparison

    def batch_generate_semantic_masks(self, input_folder, output_base_dir="./batch_semantic_results"):
        """æ‰¹é‡ç”Ÿæˆè¯­ä¹‰åˆ†å‰²mask"""
        print("ğŸš€ å¼€å§‹æ‰¹é‡è¯­ä¹‰åˆ†å‰²å¤„ç†")
        print("=" * 60)

        # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
        image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff', '*.tif']

        # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
        image_files = []
        input_path = Path(input_folder)

        for ext in image_extensions:
            image_files.extend(glob.glob(str(input_path / ext)))
            image_files.extend(glob.glob(str(input_path / ext.upper())))

        if not image_files:
            print(f"âŒ åœ¨æ–‡ä»¶å¤¹ {input_folder} ä¸­æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡æ–‡ä»¶")
            return []

        print(f"ğŸ“ æ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_base_path = Path(output_base_dir)
        output_base_path.mkdir(parents=True, exist_ok=True)

        # æ‰¹é‡å¤„ç†ç»Ÿè®¡
        batch_results = {
            'total_images': len(image_files),
            'processed_images': 0,
            'failed_images': 0,
            'total_categories': {},
            'processing_time': 0,
            'results_per_image': []
        }

        start_time = datetime.now()

        # å¤„ç†æ¯å¼ å›¾ç‰‡
        for i, image_path in enumerate(image_files, 1):
            image_name = Path(image_path).stem
            print(f"\nğŸ” å¤„ç†å›¾ç‰‡ [{i}/{len(image_files)}]: {image_name}")
            print("-" * 50)

            try:
                # ä¸ºæ¯å¼ å›¾ç‰‡åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
                image_output_dir = output_base_path / image_name
                image_output_dir.mkdir(parents=True, exist_ok=True)

                # ç”Ÿæˆè¯­ä¹‰åˆ†å‰²
                semantic_mask, overlay_image, detections = self.generate_semantic_masks(
                    image_path, str(image_output_dir)
                )

                # ç»Ÿè®¡ç±»åˆ«
                image_categories = {}
                for det in detections:
                    category = det['category']
                    image_categories[category] = image_categories.get(category, 0) + 1
                    batch_results['total_categories'][category] = batch_results['total_categories'].get(category, 0) + 1

                # ä¿å­˜å›¾ç‰‡ç»“æœä¿¡æ¯
                image_result = {
                    'image_name': image_name,
                    'image_path': image_path,
                    'output_dir': str(image_output_dir),
                    'total_detections': len(detections),
                    'categories': image_categories
                }

                batch_results['results_per_image'].append(image_result)
                batch_results['processed_images'] += 1

                print(f"âœ… {image_name} å¤„ç†å®Œæˆ:")
                print(f"   ğŸ“Š æ£€æµ‹æ€»æ•°: {len(detections)}")
                print(f"   ğŸ¨ ç±»åˆ«ç»Ÿè®¡: {image_categories}")

            except Exception as e:
                print(f"âŒ å¤„ç† {image_name} æ—¶å‡ºé”™: {e}")
                batch_results['failed_images'] += 1
                continue

        # è®¡ç®—å¤„ç†æ—¶é—´
        end_time = datetime.now()
        batch_results['processing_time'] = (end_time - start_time).total_seconds()

        # ä¿å­˜æ‰¹é‡å¤„ç†æŠ¥å‘Š
        self._save_batch_semantic_report(batch_results, output_base_path, start_time, end_time)

        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰¹é‡è¯­ä¹‰åˆ†å‰²å®Œæˆ!")
        print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"   ğŸ“· æ€»å›¾ç‰‡æ•°: {batch_results['total_images']}")
        print(f"   âœ… æˆåŠŸå¤„ç†: {batch_results['processed_images']}")
        print(f"   âŒ å¤„ç†å¤±è´¥: {batch_results['failed_images']}")
        print(f"   ğŸ¨ æ€»ç±»åˆ«ç»Ÿè®¡: {batch_results['total_categories']}")
        print(f"   â±ï¸  æ€»è€—æ—¶: {batch_results['processing_time']:.2f} ç§’")

        return batch_results

    def _save_batch_semantic_report(self, batch_results, output_base_path, start_time, end_time):
        """ä¿å­˜æ‰¹é‡è¯­ä¹‰åˆ†å‰²æŠ¥å‘Š"""

        # ä¿å­˜JSONæŠ¥å‘Š
        summary_json = {
            **batch_results,
            'start_time': start_time.isoformat(),
            'end_time': end_time.isoformat(),
            'average_time_per_image': batch_results['processing_time'] / max(1, batch_results['processed_images']),
            'success_rate': batch_results['processed_images'] / batch_results['total_images'] * 100
        }

        json_summary_path = output_base_path / "batch_semantic_summary.json"
        with open(json_summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary_json, f, indent=2, ensure_ascii=False)

        # åˆ›å»ºæ–‡æœ¬æŠ¥å‘Š
        txt_summary_path = output_base_path / "batch_semantic_report.txt"
        with open(txt_summary_path, 'w', encoding='utf-8') as f:
            f.write("ğŸ¨ æ‰¹é‡è¯­ä¹‰åˆ†å‰²å¤„ç†æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            f.write(
                f"å¤„ç†æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')} - {end_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»å¤„ç†æ—¶é•¿: {batch_results['processing_time']:.2f} ç§’\n\n")

            f.write("ğŸ“Š å¤„ç†ç»Ÿè®¡:\n")
            f.write(f"  æ€»å›¾ç‰‡æ•°é‡: {batch_results['total_images']}\n")
            f.write(f"  æˆåŠŸå¤„ç†: {batch_results['processed_images']}\n")
            f.write(f"  å¤„ç†å¤±è´¥: {batch_results['failed_images']}\n\n")

            f.write("ğŸ¨ è¯­ä¹‰ç±»åˆ«ç»Ÿè®¡:\n")
            for category, count in batch_results['total_categories'].items():
                f.write(f"  {category}: {count}\n")

        print(f"ğŸ“‹ æ‰¹é‡è¯­ä¹‰åˆ†å‰²æŠ¥å‘Šå·²ä¿å­˜:")
        print(f"   ğŸ“„ JSONæŠ¥å‘Š: {json_summary_path}")
        print(f"   ğŸ“„ æ–‡æœ¬æŠ¥å‘Š: {txt_summary_path}")


# ä¸»å‡½æ•°
def main_semantic_single():
    """å•å¼ å›¾ç‰‡è¯­ä¹‰åˆ†å‰²ä¸»å‡½æ•°"""
    print("ğŸ¨ å¯åŠ¨è¯­ä¹‰åˆ†å‰²æ£€æµ‹ç³»ç»Ÿ")
    print("=" * 50)

    # é…ç½®è·¯å¾„
    grounding_dino_config = "/home/zwk/ä¸‹è½½/S3PO-GS-main/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py"
    grounding_dino_checkpoint = "/home/zwk/ä¸‹è½½/S3PO-GS-main/groundingdino_swint_ogc.pth"
    sam_checkpoint = "/home/zwk/ä¸‹è½½/S3PO-GS-main/sam_vit_h_4b8939.pth"

    # åˆå§‹åŒ–æ£€æµ‹å™¨
    detector = EnhancedSemanticSegmentationDetector(
        grounding_dino_config=grounding_dino_config,
        grounding_dino_checkpoint=grounding_dino_checkpoint,
        sam_checkpoint=sam_checkpoint,
        device="cuda"
    )

    # æµ‹è¯•å›¾åƒ
    test_image = "/home/zwk/dataset/SemanticKITTI/SemanticKITTI/dataset/sequences/04/image_2/000004.png"

    if os.path.exists(test_image):
        print("\nğŸ¯ å¼€å§‹è¯­ä¹‰åˆ†å‰²...")
        semantic_mask, overlay_image, detections = detector.generate_semantic_masks(test_image, "./semantic_results")

        print(f"\nğŸ“Š è¯­ä¹‰åˆ†å‰²ç»“æœ:")
        category_counts = {}
        for det in detections:
            category = det['category']
            category_counts[category] = category_counts.get(category, 0) + 1

        print(f"   æ€»æ£€æµ‹æ•°: {len(detections)}")
        print(f"   è¯­ä¹‰ç±»åˆ«: {category_counts}")

    else:
        print(f"âš ï¸ æµ‹è¯•å›¾åƒä¸å­˜åœ¨: {test_image}")

    print("\nâœ… è¯­ä¹‰åˆ†å‰²å®Œæˆ!")


def main_semantic_batch():
    """æ‰¹é‡è¯­ä¹‰åˆ†å‰²ä¸»å‡½æ•°"""
    print("ğŸ¨ å¯åŠ¨æ‰¹é‡è¯­ä¹‰åˆ†å‰²ç³»ç»Ÿ")
    print("=" * 50)

    # é…ç½®è·¯å¾„
    grounding_dino_config = "/home/zwk/ä¸‹è½½/S3PO-GS-main/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py"
    grounding_dino_checkpoint = "/home/zwk/ä¸‹è½½/S3PO-GS-main/groundingdino_swint_ogc.pth"
    sam_checkpoint = "/home/zwk/ä¸‹è½½/S3PO-GS-main/sam_vit_h_4b8939.pth"

    # åˆå§‹åŒ–æ£€æµ‹å™¨
    detector = EnhancedSemanticSegmentationDetector(
        grounding_dino_config=grounding_dino_config,
        grounding_dino_checkpoint=grounding_dino_checkpoint,
        sam_checkpoint=sam_checkpoint,
        device="cuda"
    )

    # æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹
    input_folder = "/home/zwk/dataset/SemanticKITTI/SemanticKITTI/dataset/sequences/04/image_2/"
    output_folder = "./batch_semantic_results"

    if os.path.exists(input_folder):
        print("\nğŸ¯ å¼€å§‹æ‰¹é‡è¯­ä¹‰åˆ†å‰²...")
        batch_results = detector.batch_generate_semantic_masks(input_folder, output_folder)
        print("âœ… æ‰¹é‡è¯­ä¹‰åˆ†å‰²å®Œæˆ!")
    else:
        print(f"âš ï¸ è¾“å…¥æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {input_folder}")


if __name__ == "__main__":
    print("ğŸ“‹ è¯·é€‰æ‹©å¤„ç†æ¨¡å¼:")
    print("1. å•å¼ å›¾ç‰‡è¯­ä¹‰åˆ†å‰²")
    print("2. æ‰¹é‡è¯­ä¹‰åˆ†å‰²")

    try:
        choice = input("è¯·è¾“å…¥é€‰æ‹© (1 æˆ– 2): ").strip()
        if choice == "2":
            main_semantic_batch()
        elif choice == "1":
            main_semantic_single()
        else:
            print("âš ï¸ æ— æ•ˆé€‰æ‹©ï¼Œé»˜è®¤è¿è¡Œå•å¼ å›¾ç‰‡å¤„ç†")
            main_semantic_single()
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")