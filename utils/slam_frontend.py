import time
import numpy as np
import torch
import torch.multiprocessing as mp
import os
import cv2
from PIL import Image
import torch.nn.functional as F

# Grounding DINO imports with fallback handling
try:
    from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection

    GROUNDING_DINO_AVAILABLE = True
except ImportError:
    print("âš ï¸  Warning: transformers not available. Grounding DINO will be disabled.")
    GROUNDING_DINO_AVAILABLE = False

# SAM imports with fallback handling
try:
    from segment_anything import SamPredictor, sam_model_registry

    SAM_AVAILABLE = True
except ImportError:
    print("âš ï¸  Warning: segment_anything not available. SAM will be disabled.")
    SAM_AVAILABLE = False

from gaussian_splatting.gaussian_renderer import render
from gaussian_splatting.utils.graphics_utils import getProjectionMatrix2, getWorld2View2
from gui import gui_utils
from utils.camera_utils import Camera
from utils.eval_utils import eval_ate, save_gaussians
from utils.logging_utils import Log
from utils.multiprocessing_utils import clone_obj
from utils.pose_utils import update_pose
from utils.slam_utils import get_loss_tracking, get_median_depth
from utils.init_pose import get_pose, get_depth
from utils.depth_utils import process_depth


class ScenePromptManager:
    """åœºæ™¯æ£€æµ‹å’ŒPromptç®¡ç†å™¨"""

    def __init__(self, default_scene="outdoor_street"):
        self.current_scene = default_scene
        self.scene_prompts = {
            "outdoor_street": {
                "dynamic_objects": [
                    "car", "cars", "vehicle", "vehicles",
                    "truck", "trucks", "bus", "buses",
                    "motorcycle", "motorcycles", "bike", "bicycle", "bicycles",
                    "person", "people", "pedestrian", "pedestrians", "human",
                    "scooter", "e-scooter", "skateboard",
                    "delivery robot", "mobile robot"
                ],
                "confidence_threshold": 0.35,
                "description": "Urban street scene with vehicles and pedestrians"
            },

            "parking_lot": {
                "dynamic_objects": [
                    "car", "cars", "parked car", "moving car",
                    "truck", "trucks", "van", "vans",
                    "suv", "sedan", "hatchback",
                    "person", "people", "pedestrian", "walking person",
                    "shopping cart", "trolley",
                    "motorcycle", "bike"
                ],
                "confidence_threshold": 0.2,
                "description": "Parking lot with stationary and moving vehicles"
            },

            "highway": {
                "dynamic_objects": [
                    "car", "cars", "vehicle", "vehicles",
                    "truck", "trucks", "semi truck", "trailer",
                    "bus", "coach", "van", "suv",
                    "motorcycle", "motorbike"
                ],
                "confidence_threshold": 0.25,
                "description": "Highway scene with fast-moving vehicles"
            },

            "residential": {
                "dynamic_objects": [
                    "car", "cars", "parked car",
                    "person", "people", "child", "children", "adult",
                    "bicycle", "bike", "scooter", "skateboard",
                    "dog", "cat", "pet", "animal",
                    "stroller", "wheelchair"
                ],
                "confidence_threshold": 0.18,
                "description": "Residential area with people and pets"
            },

            "indoor": {
                "dynamic_objects": [
                    "person", "people", "human", "visitor",
                    "chair", "rolling chair", "office chair",
                    "robot", "cleaning robot", "vacuum robot",
                    "cart", "trolley", "wheelchair",
                    "door", "opening door", "moving door"
                ],
                "confidence_threshold": 0.3,
                "description": "Indoor environment with people and movable objects"
            },

            "construction": {
                "dynamic_objects": [
                    "construction vehicle", "excavator", "bulldozer",
                    "dump truck", "crane", "forklift",
                    "worker", "construction worker", "person",
                    "vehicle", "truck", "van"
                ],
                "confidence_threshold": 0.2,
                "description": "Construction site with heavy machinery"
            },

            "campus": {
                "dynamic_objects": [
                    "person", "people", "student", "students",
                    "bicycle", "bike", "scooter", "skateboard",
                    "car", "vehicle", "bus", "shuttle bus",
                    "delivery robot", "robot", "cart"
                ],
                "confidence_threshold": 0.2,
                "description": "University campus with students and vehicles"
            }
        }

        # åœºæ™¯è‡ªåŠ¨æ£€æµ‹å…³é”®è¯
        self.scene_keywords = {
            "highway": ["highway", "freeway", "motorway", "interstate"],
            "parking_lot": ["parking", "garage", "lot"],
            "residential": ["residential", "neighborhood", "suburb"],
            "indoor": ["indoor", "inside", "interior", "office", "building"],
            "construction": ["construction", "building", "work", "site"],
            "campus": ["campus", "university", "college", "school"]
        }

    def detect_scene_from_config(self, config_scene_hint=None):
        """ä»é…ç½®æˆ–å…¶ä»–ä¿¡æ¯æ£€æµ‹åœºæ™¯ç±»å‹"""
        if config_scene_hint and config_scene_hint in self.scene_prompts:
            self.current_scene = config_scene_hint
            return self.current_scene

        # å¯ä»¥æ‰©å±•ä¸ºåŸºäºå›¾åƒå†…å®¹çš„åœºæ™¯æ£€æµ‹
        return self.current_scene

    def detect_scene_from_path(self, data_path):
        """ä»æ•°æ®è·¯å¾„æ£€æµ‹åœºæ™¯ç±»å‹"""
        path_lower = data_path.lower()

        for scene_type, keywords in self.scene_keywords.items():
            if any(keyword in path_lower for keyword in keywords):
                self.current_scene = scene_type
                print(f"ğŸ¯ Auto-detected scene type: {scene_type} from path: {data_path}")
                return scene_type

        print(f"ğŸ” Using default scene type: {self.current_scene}")
        return self.current_scene

    def get_current_prompt(self):
        """è·å–å½“å‰åœºæ™¯çš„prompt"""
        scene_info = self.scene_prompts[self.current_scene]
        prompt = ". ".join(scene_info["dynamic_objects"])
        return prompt, scene_info["confidence_threshold"]

    def get_detailed_prompt(self):
        """è·å–è¯¦ç»†çš„promptä¿¡æ¯"""
        scene_info = self.scene_prompts[self.current_scene]
        return {
            "prompt": ". ".join(scene_info["dynamic_objects"]),
            "confidence_threshold": scene_info["confidence_threshold"],
            "description": scene_info["description"],
            "object_classes": scene_info["dynamic_objects"]
        }

    def set_scene(self, scene_type):
        """æ‰‹åŠ¨è®¾ç½®åœºæ™¯ç±»å‹"""
        if scene_type in self.scene_prompts:
            self.current_scene = scene_type
            print(f"ğŸ¬ Scene type set to: {scene_type}")
        else:
            available_scenes = list(self.scene_prompts.keys())
            print(f"âŒ Unknown scene type: {scene_type}. Available: {available_scenes}")

    def add_custom_scene(self, scene_name, dynamic_objects, confidence_threshold=0.2, description=""):
        """æ·»åŠ è‡ªå®šä¹‰åœºæ™¯é…ç½®"""
        self.scene_prompts[scene_name] = {
            "dynamic_objects": dynamic_objects,
            "confidence_threshold": confidence_threshold,
            "description": description
        }
        print(f"âœ… Added custom scene: {scene_name}")


class GroundingDINODetector:
    """ç›´æ¥ä½¿ç”¨æœ¬åœ°.pthæ–‡ä»¶çš„Grounding DINOæ£€æµ‹å™¨"""

    def __init__(self, model_path="/home/zwk/ä¸‹è½½/S3PO-GS-main/groundingdino_swint_ogc.pth", device="cuda"):
        self.device = device
        self.model_path = model_path
        self.model = None
        self.warmup_completed = False

        # ç›´æ¥åŠ è½½æœ¬åœ°æƒé‡
        self._load_local_model()

        # é¢„çƒ­æ¨¡å‹ç¡®ä¿ç¬¬ä¸€å¸§æ­£å¸¸å·¥ä½œ
        if self.model is not None:
            self._warmup_detector()

    def _load_local_model(self):
        """ç›´æ¥åŠ è½½æœ¬åœ°çš„.pthæ–‡ä»¶"""
        try:
            print(f"ğŸ”„ Loading local Grounding DINO model: {self.model_path}")

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(self.model_path):
                print(f"âŒ Model file not found: {self.model_path}")
                self._use_simple_detection()
                return

            # å°è¯•å¯¼å…¥groundingdino
            try:
                from groundingdino.util.inference import load_model, predict
                from groundingdino.util.slconfig import SLConfig
                from groundingdino.models import build_model
                import groundingdino.datasets.transforms as T

                # æŸ¥æ‰¾é…ç½®æ–‡ä»¶
                config_path = self._find_config_file()
                if config_path:
                    self.model = load_model(config_path, self.model_path, device=self.device)
                    self.use_groundingdino = True
                    print(f"âœ… Successfully loaded Grounding DINO from {self.model_path}")
                else:
                    print("âŒ Config file not found")
                    self._use_simple_detection()

            except ImportError:
                print("âŒ groundingdino package not installed")
                print("ğŸ’¡ Install with: pip install groundingdino")
                self._use_simple_detection()

        except Exception as e:
            print(f"âŒ Failed to load model: {e}")
            self._use_simple_detection()

    def _warmup_detector(self):
        """é¢„çƒ­æ£€æµ‹å™¨ï¼Œç¡®ä¿ç¬¬ä¸€å¸§èƒ½æ­£å¸¸å·¥ä½œ"""
        try:
            print("ğŸ”¥ Warming up Grounding DINO detector...")
            # åˆ›å»ºä¸€ä¸ªå°çš„æµ‹è¯•å›¾åƒè¿›è¡Œé¢„çƒ­
            test_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
            test_prompt = "car. person"

            # æ‰§è¡Œä¸€æ¬¡æµ‹è¯•æ£€æµ‹
            boxes, scores, labels = self.detect(test_image, test_prompt, confidence_threshold=0.5)

            self.warmup_completed = True
            print(f"âœ… Detector warmup completed - ready for first frame")

            # æµ‹è¯•æ˜¯å¦èƒ½æ­£å¸¸å·¥ä½œ
            if len(boxes) == 0:
                print("ğŸ“ Warmup note: No detections in random image (expected)")
            else:
                print(f"ğŸ“ Warmup note: Got {len(boxes)} detections in random image")

            return True

        except Exception as e:
            print(f"âš ï¸  Detector warmup failed: {e}")
            self.warmup_completed = False
            return False

    def _find_config_file(self):
        """æŸ¥æ‰¾é…ç½®æ–‡ä»¶"""
        possible_paths = [
            "/home/zwk/ä¸‹è½½/S3PO-GS-main/GroundingDINO-main/groundingdino/config/GroundingDINO_SwinT_OGC.py",
            os.path.join(os.path.dirname(self.model_path), "GroundingDINO_SwinT_OGC.cfg.py"),
            "./GroundingDINO_SwinT_OGC.cfg.py",
        ]

        for path in possible_paths:
            if os.path.exists(path):
                print(f"âœ… Found config file: {path}")
                return path

        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•ä¸‹è½½
        print("âš ï¸  Config file not found, attempting to download...")
        try:
            import urllib.request
            config_url = "https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/groundingdino/config/GroundingDINO_SwinT_OGC.cfg.py"
            config_path = os.path.join(os.path.dirname(self.model_path), "GroundingDINO_SwinT_OGC.cfg.py")
            urllib.request.urlretrieve(config_url, config_path)
            print(f"âœ… Downloaded config to: {config_path}")
            return config_path
        except:
            return None

    def _use_simple_detection(self):
        """ä½¿ç”¨ç®€å•çš„æ£€æµ‹æ–¹æ³•ä½œä¸ºå¤‡é€‰"""
        self.use_groundingdino = False
        print("âš ï¸  Using simple detection mode (no real detection)")

    def detect(self, image, text_prompt, confidence_threshold=0.2):
        """æ£€æµ‹æ¥å£ - ä¿®å¤åæ ‡è½¬æ¢é—®é¢˜"""
        if not hasattr(self, 'use_groundingdino') or not self.use_groundingdino:
            # è¿”å›ç©ºç»“æœï¼Œè®©ç³»ç»Ÿç»§ç»­è¿è¡Œ
            return np.array([]), np.array([]), []

        try:
            from groundingdino.util.inference import predict
            import groundingdino.datasets.transforms as T

            # å‡†å¤‡å›¾åƒå¹¶è®°å½•åŸå§‹å°ºå¯¸
            if isinstance(image, np.ndarray):
                if image.dtype != np.uint8:
                    image = (image * 255).astype(np.uint8)
                image_pil = Image.fromarray(image)
                orig_h, orig_w = image.shape[:2]  # åŸå§‹å›¾åƒå°ºå¯¸
            else:
                image_pil = image
                orig_w, orig_h = image_pil.size  # PILæ ¼å¼ï¼š(width, height)

            print(f"ğŸ–¼ï¸  Original image size: {orig_w}x{orig_h}")

            # ä½¿ç”¨ä¸resizeçš„transformï¼Œé¿å…å¤æ‚çš„åæ ‡è½¬æ¢
            transform = T.Compose([
                T.ToTensor(),
                T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
            ])

            # åº”ç”¨å˜æ¢
            image_tensor, _ = transform(image_pil, None)

            # é¢„æµ‹
            boxes, logits, phrases = predict(
                model=self.model,
                image=image_tensor,
                caption=text_prompt,
                box_threshold=confidence_threshold,
                text_threshold=confidence_threshold,
                device=self.device
            )

            if len(boxes) == 0:
                print(f"ğŸ” No objects detected")
                return np.array([]), np.array([]), []

            # å…³é”®ä¿®å¤ï¼šå¤„ç†Grounding DINOè¿”å›çš„åæ ‡æ ¼å¼
            # Grounding DINOè¿”å›çš„æ˜¯å½’ä¸€åŒ–çš„ [cx, cy, w, h] æ ¼å¼
            boxes_np = boxes.cpu().numpy()
            scores_np = logits.cpu().numpy()

            # è°ƒè¯•ï¼šæŸ¥çœ‹åŸå§‹boxesæ ¼å¼
            if len(boxes_np) > 0:
                print(f"ğŸ“¦ Raw box format (first box): {boxes_np[0]} (shape: {boxes_np.shape})")

            # è½¬æ¢ [cx, cy, w, h] åˆ° [x1, y1, x2, y2]
            # boxesæ˜¯å½’ä¸€åŒ–åæ ‡ (0-1ä¹‹é—´)
            cx = boxes_np[:, 0] * orig_w  # ä¸­å¿ƒç‚¹xåæ ‡
            cy = boxes_np[:, 1] * orig_h  # ä¸­å¿ƒç‚¹yåæ ‡
            w = boxes_np[:, 2] * orig_w  # å®½åº¦
            h = boxes_np[:, 3] * orig_h  # é«˜åº¦

            # è½¬æ¢ä¸ºå·¦ä¸Šè§’å’Œå³ä¸‹è§’åæ ‡
            x1 = cx - w / 2
            y1 = cy - h / 2
            x2 = cx + w / 2
            y2 = cy + h / 2

            # ç»„åˆæˆæ–°çš„boxesæ•°ç»„ [x1, y1, x2, y2]
            boxes_xyxy = np.stack([x1, y1, x2, y2], axis=1)

            # ç¡®ä¿åæ ‡åœ¨æœ‰æ•ˆèŒƒå›´å†…
            boxes_xyxy[:, 0] = np.clip(boxes_xyxy[:, 0], 0, orig_w)  # x1
            boxes_xyxy[:, 1] = np.clip(boxes_xyxy[:, 1], 0, orig_h)  # y1
            boxes_xyxy[:, 2] = np.clip(boxes_xyxy[:, 2], 0, orig_w)  # x2
            boxes_xyxy[:, 3] = np.clip(boxes_xyxy[:, 3], 0, orig_h)  # y2

            labels = []
            for phrase in phrases:
                label = phrase.split('(')[0].strip()
                labels.append(label)

            print(f"ğŸ¯ Detected {len(boxes_xyxy)} objects (converted from cxcywh to xyxy)")
            for i, (box, score, label) in enumerate(zip(boxes_xyxy, scores_np, labels)):
                x1, y1, x2, y2 = box
                width = x2 - x1
                height = y2 - y1
                print(f"  {i + 1}. {label}: [{x1:.1f}, {y1:.1f}, {x2:.1f}, {y2:.1f}] "
                      f"(w={width:.1f}, h={height:.1f}) conf={score:.3f}")

            return boxes_xyxy, scores_np, labels

        except Exception as e:
            print(f"âŒ Detection failed: {e}")
            import traceback
            traceback.print_exc()

            # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨resizeæ–¹æ³•ä½œä¸ºå¤‡é€‰
            print("ğŸ”„ Trying fallback detection with resize...")
            return self._detect_with_resize(image, text_prompt, confidence_threshold)

    def _detect_with_resize(self, image, text_prompt, confidence_threshold):
        """å¤‡é€‰æ–¹æ³•ï¼šä½¿ç”¨resizeçš„æ£€æµ‹"""
        try:
            from groundingdino.util.inference import predict
            import groundingdino.datasets.transforms as T

            if isinstance(image, np.ndarray):
                if image.dtype != np.uint8:
                    image = (image * 255).astype(np.uint8)
                image_pil = Image.fromarray(image)
                orig_h, orig_w = image.shape[:2]
            else:
                image_pil = image
                orig_w, orig_h = image_pil.size

            print(f"ğŸ”„ Fallback: Using resize detection for {orig_w}x{orig_h} image")

            # ä½¿ç”¨resizeçš„transform
            transform = T.Compose([
                T.RandomResize([800], max_size=1333),
                T.ToTensor(),
                T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
            ])

            image_tensor, _ = transform(image_pil, None)

            # è·å–å®é™…çš„tensorå°ºå¯¸
            _, tensor_h, tensor_w = image_tensor.shape
            print(f"ğŸ“ Resized tensor dimensions: {tensor_w}x{tensor_h}")

            boxes, logits, phrases = predict(
                model=self.model,
                image=image_tensor,
                caption=text_prompt,
                box_threshold=confidence_threshold,
                text_threshold=confidence_threshold,
                device=self.device
            )

            if len(boxes) == 0:
                return np.array([]), np.array([]), []

            # å¤„ç†boxesæ ¼å¼è½¬æ¢
            boxes_np = boxes.cpu().numpy()
            scores_np = logits.cpu().numpy()

            # è°ƒè¯•ï¼šæŸ¥çœ‹åŸå§‹boxesæ ¼å¼
            if len(boxes_np) > 0:
                print(f"ğŸ“¦ Raw box format in resize (first box): {boxes_np[0]}")

            # Grounding DINO è¿”å›çš„æ˜¯å½’ä¸€åŒ–çš„ [cx, cy, w, h] æ ¼å¼
            # éœ€è¦è½¬æ¢ä¸º [x1, y1, x2, y2] æ ¼å¼
            cx = boxes_np[:, 0] * orig_w
            cy = boxes_np[:, 1] * orig_h
            w = boxes_np[:, 2] * orig_w
            h = boxes_np[:, 3] * orig_h

            # è½¬æ¢ä¸º [x1, y1, x2, y2]
            x1 = cx - w / 2
            y1 = cy - h / 2
            x2 = cx + w / 2
            y2 = cy + h / 2

            # ç»„åˆæˆæ–°çš„boxesæ•°ç»„
            boxes_xyxy = np.stack([x1, y1, x2, y2], axis=1)

            # ç¡®ä¿åæ ‡åœ¨æœ‰æ•ˆèŒƒå›´å†…
            boxes_xyxy[:, 0] = np.clip(boxes_xyxy[:, 0], 0, orig_w)
            boxes_xyxy[:, 1] = np.clip(boxes_xyxy[:, 1], 0, orig_h)
            boxes_xyxy[:, 2] = np.clip(boxes_xyxy[:, 2], 0, orig_w)
            boxes_xyxy[:, 3] = np.clip(boxes_xyxy[:, 3], 0, orig_h)

            labels = []
            for phrase in phrases:
                label = phrase.split('(')[0].strip()
                labels.append(label)

            print(f"ğŸ¯ Resize fallback detected {len(boxes_xyxy)} objects")
            for i, (box, score, label) in enumerate(zip(boxes_xyxy, scores_np, labels)):
                x1, y1, x2, y2 = box
                print(f"  {i + 1}. {label}: [{x1:.1f}, {y1:.1f}, {x2:.1f}, {y2:.1f}] "
                      f"(w={x2 - x1:.1f}, h={y2 - y1:.1f}) conf={score:.3f}")

            return boxes_xyxy, scores_np, labels

        except Exception as e:
            print(f"âŒ Resize detection also failed: {e}")
            import traceback
            traceback.print_exc()
            return np.array([]), np.array([]), []


class EnhancedDynamicObjectMasker:
    def __init__(self, device="cuda", use_sam=True,
                 sam_checkpoint="/home/zwk/ä¸‹è½½/S3PO-GS-main/utils/sam_vit_b_01ec64.pth",
                 save_dir=None, save_images=True, scene_type="outdoor_street"):
        """
        ä½¿ç”¨æœ¬åœ°Grounding DINOæƒé‡çš„åŠ¨æ€ç‰©ä½“é®ç½©å™¨ï¼ˆç§»é™¤åœ°é¢åŠŸèƒ½ï¼‰
        """
        self.device = device
        self.initialization_success = True
        self.first_frame_processed = False  # æ·»åŠ ç¬¬ä¸€å¸§æ ‡è®°

        # åœºæ™¯å’ŒPromptç®¡ç†
        try:
            self.prompt_manager = ScenePromptManager(default_scene=scene_type)
            print(f"âœ… Scene prompt manager initialized")
        except Exception as e:
            print(f"âŒ Failed to initialize scene manager: {e}")
            self.initialization_success = False

        # ç›´æ¥ä½¿ç”¨æœ¬åœ°çš„Grounding DINOæƒé‡
        print(f"ğŸ”„ Initializing Grounding DINO detector with local weights...")
        try:
            self.grounding_detector = GroundingDINODetector(
                model_path="/home/zwk/ä¸‹è½½/S3PO-GS-main/groundingdino_swint_ogc.pth",
                device=device
            )
            print(f"âœ… Grounding DINO detector initialized")
        except Exception as e:
            print(f"âŒ Failed to initialize Grounding DINO: {e}")
            self.grounding_detector = None
            self.initialization_success = False

        # SAMåˆ†å‰²å™¨ï¼ˆä¿æŒä¸å˜ï¼‰
        self.use_sam = use_sam
        if use_sam and SAM_AVAILABLE:
            try:
                if os.path.exists(sam_checkpoint):
                    sam = sam_model_registry["vit_b"](checkpoint=sam_checkpoint)
                    sam.to(device=device)
                    self.sam_predictor = SamPredictor(sam)
                    print("âœ… SAM model loaded successfully")
                else:
                    print(f"âš ï¸  SAM checkpoint not found at {sam_checkpoint}")
                    self.use_sam = False
            except Exception as e:
                print(f"âš ï¸  Warning: SAM model failed to load ({e})")
                self.use_sam = False

        # å…¶ä»–åˆå§‹åŒ–ä¿æŒä¸å˜...
        self.prev_frame = None
        self.prev_mask = None
        self.motion_threshold = 3.0
        self.mask_history = []
        self.history_length = 5
        self.save_images = save_images
        self.save_dir = save_dir if save_dir else "./masked_images"

        if self.save_images:
            try:
                self._create_save_directories()
            except Exception as e:
                print(f"âš ï¸  Failed to create save directories: {e}")
                self.save_images = False

        # æ‰“å°é…ç½®ä¿¡æ¯
        print(f"ğŸ¯ Dynamic Object Masker Configuration:")
        print(f"  - Using local model: /home/zwk/ä¸‹è½½/S3PO-GS-main/groundingdino_swint_ogc.pth")
        print(f"  - SAM enabled: {self.use_sam}")
        print(f"  - Save images: {self.save_images}")

    def _create_conservative_first_frame_mask(self, image):
        """ä¸ºç¬¬ä¸€å¸§åˆ›å»ºä¿å®ˆçš„åŠ¨æ€ç‰©ä½“mask"""
        h, w = image.shape[:2]

        # åŸºäºé¢œè‰²å’Œä½ç½®çš„å¯å‘å¼æ£€æµ‹
        # 1. æ£€æµ‹è½¦è¾†å¸¸è§é¢œè‰²
        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)

        # è½¦è¾†å¸¸è§é¢œè‰²èŒƒå›´
        vehicle_color_ranges = [
            # ç™½è‰²è½¦è¾†
            ([0, 0, 180], [180, 30, 255]),
            # é»‘è‰²è½¦è¾†
            ([0, 0, 0], [180, 255, 50]),
            # ç°è‰²è½¦è¾†
            ([0, 0, 50], [180, 50, 150]),
            # çº¢è‰²è½¦è¾†
            ([0, 100, 100], [10, 255, 255]),
            ([170, 100, 100], [180, 255, 255]),
            # è“è‰²è½¦è¾†
            ([100, 100, 100], [130, 255, 255]),
        ]

        color_mask = np.zeros((h, w), dtype=np.uint8)
        for lower, upper in vehicle_color_ranges:
            lower = np.array(lower)
            upper = np.array(upper)
            mask = cv2.inRange(hsv, lower, upper)
            color_mask = cv2.bitwise_or(color_mask, mask)

        # 2. å½¢æ€å­¦æ“ä½œ
        kernel = np.ones((5, 5), np.uint8)
        color_mask = cv2.morphologyEx(color_mask, cv2.MORPH_CLOSE, kernel)
        color_mask = cv2.morphologyEx(color_mask, cv2.MORPH_OPEN, kernel)

        # 3. åªä¿ç•™è¾ƒå¤§çš„è¿é€šåŒºåŸŸï¼ˆå¯èƒ½æ˜¯è½¦è¾†ï¼‰
        contours, _ = cv2.findContours(color_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        filtered_mask = np.zeros((h, w), dtype=np.uint8)

        min_area = (h * w) * 0.01  # è‡³å°‘å å›¾åƒ1%çš„åŒºåŸŸ
        for contour in contours:
            area = cv2.contourArea(contour)
            if area > min_area:
                cv2.drawContours(filtered_mask, [contour], -1, 1, -1)

        # 4. åœ¨å›¾åƒä¸‹åŠéƒ¨åˆ†æ›´ä¿å®ˆï¼ˆè½¦è¾†æ›´å¯èƒ½å‡ºç°åœ¨è¿™é‡Œï¼‰
        conservative_mask = filtered_mask.copy()
        bottom_half_start = int(h * 0.4)

        # å¯¹ä¸‹åŠéƒ¨åˆ†åº”ç”¨æ›´å®½æ¾çš„æ£€æµ‹
        bottom_region = image[bottom_half_start:, :, :]
        bottom_gray = cv2.cvtColor(bottom_region, cv2.COLOR_RGB2GRAY)

        # æ£€æµ‹è¾¹ç¼˜è¾ƒå¼ºçš„åŒºåŸŸï¼ˆè½¦è¾†è½®å»“ï¼‰
        edges = cv2.Canny(bottom_gray, 50, 150)
        dilated_edges = cv2.dilate(edges, np.ones((3, 3), np.uint8), iterations=2)

        conservative_mask[bottom_half_start:, :] = np.maximum(
            conservative_mask[bottom_half_start:, :],
            (dilated_edges > 0).astype(np.uint8)
        )

        print(f"ğŸ›¡ï¸  Created conservative first-frame mask: {np.sum(conservative_mask)} pixels")
        return conservative_mask

    def _fallback_detection(self, image, frame_idx=None):
        """æ”¹è¿›çš„fallbackæ£€æµ‹ï¼Œä¸ºç¬¬ä¸€å¸§æä¾›ä¿å®ˆçš„åŠ¨æ€ç‰©ä½“æ£€æµ‹"""
        print(f"ğŸ”„ Using improved fallback detection for frame {frame_idx}")
        h, w = image.shape[:2]

        # å¦‚æœæ˜¯ç¬¬ä¸€å¸§æˆ–å‰å‡ å¸§ï¼Œä½¿ç”¨ä¿å®ˆçš„å¯å‘å¼æ£€æµ‹
        if frame_idx is not None and frame_idx < 5:
            print(f"ğŸ”° Applying conservative detection for early frame {frame_idx}")
            conservative_mask = self._create_conservative_first_frame_mask(image)

            # æ‰©å±•maskä»¥ç¡®ä¿å®‰å…¨
            kernel = np.ones((9, 9), np.uint8)
            expanded_mask = cv2.dilate(conservative_mask, kernel, iterations=2)

            return expanded_mask, 0.5, image.copy()

        # å¯¹äºåç»­å¸§ï¼Œå¦‚æœæœ‰å†å²maskï¼Œå¯ä»¥ä½¿ç”¨è¿åŠ¨æ£€æµ‹
        if hasattr(self, 'prev_frame') and self.prev_frame is not None:
            current_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)

            try:
                # è®¡ç®—å…‰æµ
                flow = cv2.calcOpticalFlowFarneback(
                    self.prev_frame, current_gray, None,
                    pyr_scale=0.5, levels=3, winsize=15,
                    iterations=3, poly_n=5, poly_sigma=1.2, flags=0
                )

                magnitude = np.sqrt(flow[..., 0] ** 2 + flow[..., 1] ** 2)
                motion_mask = (magnitude > self.motion_threshold).astype(np.uint8)

                # æ‰©å±•è¿åŠ¨åŒºåŸŸ
                kernel = np.ones((7, 7), np.uint8)
                expanded_motion = cv2.dilate(motion_mask, kernel, iterations=1)

                print(f"ğŸƒ Motion-based fallback detection: {np.sum(expanded_motion)} pixels")
                return expanded_motion, 0.3, image.copy()

            except Exception as e:
                print(f"âŒ Motion detection failed: {e}")

        # æœ€åçš„ä¿åº•æ–¹æ¡ˆï¼šè¿”å›ç©ºmaskï¼Œä½†è®°å½•è­¦å‘Š
        print(f"âš ï¸  WARNING: No dynamic object detection for frame {frame_idx}")
        return np.zeros((h, w), dtype=np.uint8), 0.0, image.copy()

    def set_scene_from_config(self, config):
        """ä»é…ç½®ä¸­è®¾ç½®åœºæ™¯ç±»å‹"""
        scene_hint = config.get("dynamic_filtering", {}).get("scene_type", None)
        data_path = config.get("Dataset", {}).get("dataset_path", "")

        # ä¼˜å…ˆä½¿ç”¨é…ç½®ä¸­çš„åœºæ™¯ç±»å‹
        if scene_hint:
            self.prompt_manager.set_scene(scene_hint)
        # å…¶æ¬¡å°è¯•ä»æ•°æ®è·¯å¾„æ¨æ–­
        elif data_path:
            self.prompt_manager.detect_scene_from_path(data_path)

        # æ›´æ–°æ£€æµ‹é˜ˆå€¼
        prompt_info = self.prompt_manager.get_detailed_prompt()
        print(f"ğŸ¬ Scene configuration updated:")
        print(f"  - Active scene: {self.prompt_manager.current_scene}")
        print(f"  - Confidence threshold: {prompt_info['confidence_threshold']}")

    def _create_save_directories(self):
        """åˆ›å»ºä¿å­˜å›¾åƒçš„ç›®å½•ç»“æ„"""
        directories = [
            self.save_dir,
            os.path.join(self.save_dir, "original"),
            os.path.join(self.save_dir, "grounding_dino_detections"),  # Grounding DINOæ£€æµ‹æ¡†
            os.path.join(self.save_dir, "grounding_dino_masks"),  # Grounding DINOç”Ÿæˆçš„mask
            os.path.join(self.save_dir, "sam_masks"),  # SAMç²¾ç¡®åˆ†å‰²çš„mask
            os.path.join(self.save_dir, "motion_masks"),  # è¿åŠ¨æ£€æµ‹mask
            os.path.join(self.save_dir, "final_masks"),  # æœ€ç»ˆç»„åˆmask
            os.path.join(self.save_dir, "masked_overlay"),  # å åŠ æ˜¾ç¤º
            os.path.join(self.save_dir, "static_only"),  # åªæ˜¾ç¤ºé™æ€åŒºåŸŸ
            os.path.join(self.save_dir, "keyframes"),  # å…³é”®å¸§ç‰¹æ®Šä¿å­˜
            os.path.join(self.save_dir, "detection_analysis"),  # æ£€æµ‹åˆ†æç»“æœ
        ]

        for directory in directories:
            os.makedirs(directory, exist_ok=True)

        print(f"ğŸ“ Created mask image directories in: {self.save_dir}")

    def save_detection_results(self, image, frame_idx, grounding_dino_mask=None, sam_mask=None,
                               motion_mask=None, final_mask=None, boxes=None, labels=None, scores=None):
        """ä¿å­˜Grounding DINOæ£€æµ‹å’Œåˆ†å‰²çš„å„ç§ç»“æœ"""
        if not self.save_images:
            return

        try:
            # ç¡®ä¿å›¾åƒæ˜¯æ­£ç¡®çš„æ ¼å¼
            if isinstance(image, np.ndarray):
                if image.dtype != np.uint8:
                    image = (image * 255).astype(np.uint8) if image.max() <= 1.0 else image.astype(np.uint8)
                img_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) if len(image.shape) == 3 else image
            else:
                return

            # 1. ä¿å­˜åŸå§‹å›¾åƒ
            original_path = os.path.join(self.save_dir, "original", f"frame_{frame_idx:06d}.jpg")
            cv2.imwrite(original_path, img_bgr)

            # 2. ä¿å­˜Grounding DINOæ£€æµ‹æ¡†å’Œæ ‡ç­¾
            if boxes is not None and len(boxes) > 0:
                detection_img = img_bgr.copy()
                print(
                    f"ğŸ–¼ï¸  Drawing {len(boxes)} detection boxes on {detection_img.shape[1]}x{detection_img.shape[0]} image")

                for i, box in enumerate(boxes):
                    x1, y1, x2, y2 = map(int, box)

                    # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
                    x1 = max(0, min(x1, detection_img.shape[1] - 1))
                    y1 = max(0, min(y1, detection_img.shape[0] - 1))
                    x2 = max(0, min(x2, detection_img.shape[1] - 1))
                    y2 = max(0, min(y2, detection_img.shape[0] - 1))

                    print(f"  Box {i + 1}: [{x1}, {y1}, {x2}, {y2}]")

                    # ç»˜åˆ¶æ£€æµ‹æ¡†
                    cv2.rectangle(detection_img, (x1, y1), (x2, y2), (0, 255, 0), 3)

                    # æ·»åŠ æ ‡ç­¾å’Œç½®ä¿¡åº¦
                    if labels and i < len(labels):
                        label_text = labels[i]
                        if scores is not None and i < len(scores):
                            label_text += f" {scores[i]:.2f}"

                        # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
                        (text_width, text_height), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)
                        cv2.rectangle(detection_img, (x1, y1 - text_height - 10), (x1 + text_width + 5, y1),
                                      (0, 255, 0), -1)
                        cv2.putText(detection_img, label_text, (x1 + 2, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.7,
                                    (0, 0, 0), 2)

                detection_path = os.path.join(self.save_dir, "grounding_dino_detections", f"frame_{frame_idx:06d}.jpg")
                cv2.imwrite(detection_path, detection_img)
                print(f"âœ… Saved detection visualization to {detection_path}")

            # 3. ä¿å­˜å„ç§mask
            if grounding_dino_mask is not None:
                grounding_mask_img = (grounding_dino_mask * 255).astype(np.uint8)
                grounding_path = os.path.join(self.save_dir, "grounding_dino_masks", f"frame_{frame_idx:06d}.jpg")
                cv2.imwrite(grounding_path, grounding_mask_img)

            if sam_mask is not None:
                sam_mask_img = (sam_mask * 255).astype(np.uint8)
                sam_path = os.path.join(self.save_dir, "sam_masks", f"frame_{frame_idx:06d}.jpg")
                cv2.imwrite(sam_path, sam_mask_img)

            if motion_mask is not None:
                motion_mask_img = (motion_mask * 255).astype(np.uint8)
                motion_path = os.path.join(self.save_dir, "motion_masks", f"frame_{frame_idx:06d}.jpg")
                cv2.imwrite(motion_path, motion_mask_img)

            # 4. ä¿å­˜æœ€ç»ˆmaskå’Œç»„åˆæ˜¾ç¤º
            if final_mask is not None:
                final_mask_img = (final_mask * 255).astype(np.uint8)
                final_path = os.path.join(self.save_dir, "final_masks", f"frame_{frame_idx:06d}.jpg")
                cv2.imwrite(final_path, final_mask_img)

                # å åŠ æ˜¾ç¤ºï¼ˆåŠ¨æ€åŒºåŸŸç”¨çº¢è‰²ï¼‰
                overlay_img = img_bgr.copy()
                overlay_img[final_mask > 0] = [0, 0, 255]  # åŠ¨æ€ç‰©ä½“ç”¨çº¢è‰²
                overlay_path = os.path.join(self.save_dir, "masked_overlay", f"frame_{frame_idx:06d}.jpg")
                cv2.imwrite(overlay_path, overlay_img)

                # é™æ€åŒºåŸŸå›¾åƒ
                static_img = img_bgr.copy()
                static_img[final_mask > 0] = [0, 0, 0]
                static_path = os.path.join(self.save_dir, "static_only", f"frame_{frame_idx:06d}.jpg")
                cv2.imwrite(static_path, static_img)

            # 5. ä¿å­˜æ£€æµ‹åˆ†æç»“æœ
            if boxes is not None and labels is not None:
                analysis_path = os.path.join(self.save_dir, "detection_analysis", f"frame_{frame_idx:06d}.txt")
                with open(analysis_path, 'w') as f:
                    f.write(f"Frame {frame_idx} Detection Analysis\n")
                    f.write(f"Scene Type: {self.prompt_manager.current_scene}\n")
                    f.write(f"Prompt Used: {self.prompt_manager.get_current_prompt()[0]}\n")
                    f.write(f"Total Detections: {len(boxes)}\n\n")

                    for i, (box, label) in enumerate(zip(boxes, labels)):
                        score = scores[i] if scores is not None and i < len(scores) else 0.0
                        f.write(f"Detection {i + 1}:\n")
                        f.write(f"  Label: {label}\n")
                        f.write(f"  Confidence: {score:.3f}\n")
                        f.write(f"  Box: [{box[0]:.1f}, {box[1]:.1f}, {box[2]:.1f}, {box[3]:.1f}]\n\n")

            print(f"ğŸ’¾ Saved all Grounding DINO detection results for frame {frame_idx}")

        except Exception as e:
            print(f"âŒ Warning: Failed to save detection results for frame {frame_idx}: {e}")
            import traceback
            traceback.print_exc()

    def detect_and_segment(self, image, frame_idx=None):
        """
        æ”¹è¿›çš„æ£€æµ‹æ–¹æ³•ï¼Œç‰¹åˆ«ä¼˜åŒ–ç¬¬ä¸€å¸§çš„å¤„ç†å’Œåæ ‡ä¿®å¤ï¼ˆç§»é™¤åœ°é¢åŠŸèƒ½ï¼‰
        """
        h, w = image.shape[:2]
        final_mask = np.zeros((h, w), dtype=np.uint8)
        grounding_dino_mask = np.zeros((h, w), dtype=np.uint8)
        sam_mask = None
        motion_mask = None
        max_confidence = 0.0

        # ç¬¬ä¸€å¸§ç‰¹æ®Šå¤„ç†æ ‡è®°
        is_first_frame = (frame_idx == 0) or not self.first_frame_processed

        if is_first_frame:
            print(f"ğŸ”° Processing FIRST FRAME {frame_idx} with enhanced detection")

        # 1. è·å–å½“å‰åœºæ™¯çš„promptå’Œé˜ˆå€¼
        try:
            text_prompt, confidence_threshold = self.prompt_manager.get_current_prompt()

            # ç¬¬ä¸€å¸§ä½¿ç”¨æ›´ä½çš„ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œæ›´ä¿å®ˆçš„æ£€æµ‹
            if is_first_frame:
                confidence_threshold = max(0.15, confidence_threshold * 0.7)
                print(f"ğŸ”° First frame: lowered confidence threshold to {confidence_threshold}")

            print(f"ğŸ¯ Using prompt: '{text_prompt[:100]}...' (confidence: {confidence_threshold})")
        except Exception as e:
            print(f"âŒ Failed to get prompt: {e}")
            text_prompt = "car. truck. person. bicycle"
            confidence_threshold = 0.15 if is_first_frame else 0.2

        # 2. Grounding DINOæ£€æµ‹ - å¤šæ¬¡å°è¯•ç¡®ä¿ç¬¬ä¸€å¸§æˆåŠŸ
        detection_success = False
        max_attempts = 3 if is_first_frame else 1

        for attempt in range(max_attempts):
            try:
                if is_first_frame and attempt > 0:
                    print(f"ğŸ”„ First frame detection attempt {attempt + 1}/{max_attempts}")

                boxes, scores, labels = self.grounding_detector.detect(
                    image, text_prompt, confidence_threshold
                )

                if len(boxes) > 0 or not is_first_frame:
                    detection_success = True
                    break

            except Exception as e:
                print(f"âŒ Grounding DINO detection attempt {attempt + 1} failed: {e}")
                if attempt == max_attempts - 1:
                    print(f"âŒ All detection attempts failed for frame {frame_idx}")

        # å¦‚æœæ£€æµ‹å®Œå…¨å¤±è´¥ï¼Œä½¿ç”¨fallback
        if not detection_success or len(boxes) == 0:
            print(f"ğŸ†˜ Using fallback detection for frame {frame_idx}")
            fallback_mask, fallback_conf, fallback_image = self._fallback_detection(image, frame_idx)

            # ä¿å­˜fallbackç»“æœ
            if frame_idx is not None and self.save_images:
                try:
                    self.save_detection_results(
                        image, frame_idx,
                        grounding_dino_mask=fallback_mask,
                        final_mask=fallback_mask,
                        boxes=[], labels=['fallback'], scores=[fallback_conf]
                    )
                except Exception as e:
                    print(f"âš ï¸  Failed to save fallback results: {e}")

            self.first_frame_processed = True
            return fallback_mask, fallback_conf, fallback_image

        # 3. å¤„ç†æ£€æµ‹ç»“æœï¼Œåˆ›å»ºåŸºç¡€mask
        vehicle_detected = False

        try:
            for i, (box, score, label) in enumerate(zip(boxes, scores, labels)):
                x1, y1, x2, y2 = box.astype(int)
                max_confidence = max(max_confidence, score)

                # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
                x1 = max(0, min(x1, w - 1))
                y1 = max(0, min(y1, h - 1))
                x2 = max(0, min(x2, w - 1))
                y2 = max(0, min(y2, h - 1))

                # ç¡®ä¿x2 > x1, y2 > y1
                if x2 <= x1 or y2 <= y1:
                    print(f"âš ï¸  Invalid box coordinates for '{label}': [{x1}, {y1}, {x2}, {y2}]")
                    continue

                # æ£€æŸ¥æ˜¯å¦æ£€æµ‹åˆ°è½¦è¾†ç±»åˆ«
                vehicle_keywords = ["car", "truck", "bus", "vehicle", "van", "suv", "motorcycle", "bike"]
                if any(keyword in label.lower() for keyword in vehicle_keywords):
                    vehicle_detected = True

                    # å¯¹è½¦è¾†ç±»åˆ«æ‰©å±•è¾¹ç•Œæ¡† - ç¬¬ä¸€å¸§æ›´ä¿å®ˆ
                    width = x2 - x1
                    height = y2 - y1
                    expand_ratio = 0.15 if is_first_frame else 0.1
                    expand_w = int(width * expand_ratio)
                    expand_h = int(height * expand_ratio)

                    x1 = max(0, x1 - expand_w)
                    y1 = max(0, y1 - expand_h)
                    x2 = min(w, x2 + expand_w)
                    y2 = min(h, y2 + expand_h)

                    print(f"ğŸš— {'[FIRST FRAME] ' if is_first_frame else ''}Expanded vehicle '{label}' "
                          f"(conf={score:.3f}): expanded by {expand_w}x{expand_h}")
                else:
                    print(f"ğŸ‘¤ Non-vehicle detection '{label}' (conf={score:.3f})")

                # åˆ›å»ºåŸºç¡€mask
                grounding_dino_mask[y1:y2, x1:x2] = 1

            final_mask = grounding_dino_mask.copy()
        except Exception as e:
            print(f"âŒ Failed to process detection results: {e}")

        # 4. SAMç²¾ç¡®åˆ†å‰²ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        use_sam_result = False  # æ ‡è®°æ˜¯å¦ä½¿ç”¨SAMç»“æœ
        if self.use_sam and len(boxes) > 0:
            try:
                sam_combined_mask = np.zeros((h, w), dtype=np.uint8)
                self.sam_predictor.set_image(image)

                successful_sam_count = 0
                for i, (box, score, label) in enumerate(zip(boxes, scores, labels)):
                    try:
                        x1, y1, x2, y2 = box.astype(int)

                        # ç¡®ä¿åæ ‡æœ‰æ•ˆ
                        x1 = max(0, min(x1, w - 1))
                        y1 = max(0, min(y1, h - 1))
                        x2 = max(0, min(x2, w - 1))
                        y2 = max(0, min(y2, h - 1))

                        if x2 <= x1 or y2 <= y1:
                            continue

                        # å¯¹è½¦è¾†è¿›è¡Œæ‰©å±•ï¼ˆä¸ä¸Šé¢ä¿æŒä¸€è‡´ï¼‰
                        vehicle_keywords = ["car", "truck", "bus", "vehicle", "van", "suv", "motorcycle", "bike"]
                        if any(keyword in label.lower() for keyword in vehicle_keywords):
                            width = x2 - x1
                            height = y2 - y1
                            expand_ratio = 0.15 if is_first_frame else 0.1
                            expand_w = int(width * expand_ratio)
                            expand_h = int(height * expand_ratio)
                            x1 = max(0, x1 - expand_w)
                            y1 = max(0, y1 - expand_h)
                            x2 = min(w, x2 + expand_w)
                            y2 = min(h, y2 + expand_h)

                        input_box = np.array([x1, y1, x2, y2])

                        masks, sam_scores, _ = self.sam_predictor.predict(
                            point_coords=None,
                            point_labels=None,
                            box=input_box[None, :],
                            multimask_output=False,
                        )

                        if len(masks) > 0:
                            best_mask = masks[0].astype(np.uint8)
                            sam_combined_mask = np.logical_or(sam_combined_mask, best_mask).astype(np.uint8)
                            successful_sam_count += 1
                            print(f"âœ… SAM refined '{label}' detection")
                    except Exception as e:
                        print(f"âŒ SAM failed for '{label}': {e}")

                if sam_combined_mask.sum() > 0:
                    final_mask = sam_combined_mask
                    sam_mask = sam_combined_mask
                    use_sam_result = True  # æ ‡è®°ä½¿ç”¨äº†SAMç»“æœ
                    print(f"ğŸ¯ SAM successfully refined {successful_sam_count}/{len(boxes)} detections")
                    print(
                        f"ğŸ“Š SAM mask pixels: {sam_mask.sum()}, Grounding DINO mask pixels: {grounding_dino_mask.sum()}")
            except Exception as e:
                print(f"âŒ SAM processing failed: {e}")

        # 5. è¿åŠ¨æ£€æµ‹å¢å¼º - ä¿®å¤ï¼šä¸è¦è¦†ç›–SAM maskï¼Œè€Œæ˜¯ç»„åˆ
        if not is_first_frame:  # ç¬¬ä¸€å¸§ä¸ä½¿ç”¨è¿åŠ¨æ£€æµ‹
            try:
                motion_refined_mask = self._refine_with_motion(image, final_mask)
                if motion_refined_mask is not None:
                    motion_mask = motion_refined_mask

                    # ä¿®å¤ï¼šå¦‚æœä½¿ç”¨äº†SAMï¼Œå°†è¿åŠ¨æ£€æµ‹ä½œä¸ºè¡¥å……è€Œä¸æ˜¯æ›¿æ¢
                    if use_sam_result:
                        # åªåœ¨SAMæ²¡æœ‰æ£€æµ‹åˆ°çš„åŒºåŸŸä½¿ç”¨è¿åŠ¨æ£€æµ‹
                        # æˆ–è€…æ‰©å±•SAMæ£€æµ‹åˆ°çš„åŒºåŸŸ
                        final_mask = np.logical_or(final_mask, motion_refined_mask).astype(np.uint8)
                        print(f"ğŸƒ Motion detection added to SAM mask (combined)")
                    else:
                        # å¦‚æœæ²¡æœ‰SAMï¼Œå¯ä»¥ä½¿ç”¨è¿åŠ¨æ£€æµ‹ç»“æœ
                        final_mask = motion_refined_mask
                        print(f"ğŸƒ Motion detection used as primary mask")
            except Exception as e:
                print(f"âŒ Motion detection failed: {e}")

        # 6. æ—¶é—´ä¸€è‡´æ€§æ»¤æ³¢ - ä¿®å¤ï¼šå¯¹äºç¬¬ä¸€å¸§æˆ–ä½¿ç”¨SAMçš„æƒ…å†µè¦æ›´ä¿å®ˆ
        if not is_first_frame and not use_sam_result:
            # åªæœ‰åœ¨æ²¡æœ‰ä½¿ç”¨SAMä¸”ä¸æ˜¯ç¬¬ä¸€å¸§æ—¶æ‰ä½¿ç”¨æ—¶é—´ä¸€è‡´æ€§
            try:
                final_mask = self._temporal_consistency(final_mask)
                print(f"â±ï¸  Applied temporal consistency filter")
            except Exception as e:
                print(f"âŒ Temporal consistency failed: {e}")
        elif use_sam_result:
            print(f"â­ï¸  Skipping temporal consistency (SAM result is more reliable)")

        # 7. è½¦è¾†ç‰¹æ®Šå¤„ç† - ç¬¬ä¸€å¸§æ›´ä¿å®ˆ
        if vehicle_detected and final_mask.sum() > 0:
            try:
                kernel_size = 7 if is_first_frame else 5
                kernel = np.ones((kernel_size, kernel_size), np.uint8)
                final_mask = cv2.dilate(final_mask, kernel, iterations=1)
                print(
                    f"ğŸ›¡ï¸  {'[FIRST FRAME] ' if is_first_frame else ''}Applied additional dilation for vehicle detection")
            except Exception as e:
                print(f"âŒ Dilation failed: {e}")

        # è°ƒè¯•ä¿¡æ¯ï¼šæ¯”è¾ƒå„ä¸ªmaskçš„å·®å¼‚
        if sam_mask is not None:
            print(f"ğŸ” Mask comparison:")
            print(f"  - Grounding DINO mask: {grounding_dino_mask.sum()} pixels")
            print(f"  - SAM mask: {sam_mask.sum()} pixels")
            print(f"  - Final mask: {final_mask.sum()} pixels")
            if motion_mask is not None:
                print(f"  - Motion mask: {motion_mask.sum()} pixels")

        # 8. ä¿å­˜æ‰€æœ‰ç»“æœ
        if frame_idx is not None and self.save_images:
            try:
                self.save_detection_results(
                    image, frame_idx,
                    grounding_dino_mask=grounding_dino_mask,
                    sam_mask=sam_mask,
                    motion_mask=motion_mask,
                    final_mask=final_mask,
                    boxes=boxes,
                    labels=labels,
                    scores=scores
                )
            except Exception as e:
                print(f"âš ï¸  Failed to save detection results: {e}")

        # 9. è¾“å‡ºæ£€æµ‹ç»Ÿè®¡
        try:
            detection_stats = {}
            if labels:
                for label in labels:
                    detection_stats[label] = detection_stats.get(label, 0) + 1

            print(f"ğŸ“Š {'[FIRST FRAME] ' if is_first_frame else ''}Frame {frame_idx} summary:")
            print(f"  - Scene: {getattr(self.prompt_manager, 'current_scene', 'unknown')}")
            print(f"  - Detections: {len(boxes)} objects")
            print(f"  - Vehicle detected: {vehicle_detected}")
            print(f"  - Max confidence: {max_confidence:.3f}")
            print(f"  - Final mask pixels: {final_mask.sum()}")
            print(f"  - Detection breakdown: {detection_stats}")
        except Exception as e:
            print(f"âŒ Failed to print statistics: {e}")

        # æ ‡è®°ç¬¬ä¸€å¸§å·²å¤„ç†
        if is_first_frame:
            self.first_frame_processed = True
            print(f"âœ… First frame {frame_idx} processing completed")

        return final_mask, max_confidence, image.copy()

    def _refine_with_motion(self, current_frame, detection_mask):
        """ä½¿ç”¨å…‰æµè¿åŠ¨ä¿¡æ¯ä¼˜åŒ–mask - ä¿®å¤ç‰ˆæœ¬"""
        if self.prev_frame is None:
            self.prev_frame = cv2.cvtColor(current_frame, cv2.COLOR_RGB2GRAY)
            return None  # ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶è¿”å›Noneï¼Œä¸ä¿®æ”¹mask

        current_gray = cv2.cvtColor(current_frame, cv2.COLOR_RGB2GRAY)

        try:
            # è®¡ç®—å…‰æµ
            flow = cv2.calcOpticalFlowFarneback(
                self.prev_frame, current_gray, None,
                pyr_scale=0.5, levels=3, winsize=15,
                iterations=3, poly_n=5, poly_sigma=1.2, flags=0
            )

            # è®¡ç®—è¿åŠ¨å¹…åº¦
            magnitude = np.sqrt(flow[..., 0] ** 2 + flow[..., 1] ** 2)

            # è¿åŠ¨mask
            motion_mask = (magnitude > self.motion_threshold).astype(np.uint8)

            # ä¿®å¤ï¼šæ­£ç¡®å¤„ç†è¿åŠ¨å’Œé™æ­¢åŒºåŸŸçš„ç»„åˆ
            # ç§»åŠ¨çš„ç‰©ä½“ä¿ç•™
            moving_objects = np.logical_and(detection_mask, motion_mask).astype(np.uint8)

            # å¯¹é™æ­¢çš„æ£€æµ‹ç‰©ä½“ï¼Œå¦‚æœå®ƒä»¬åœ¨æ£€æµ‹maskä¸­ï¼Œä¹Ÿè¦ä¿ç•™ä¸€éƒ¨åˆ†
            # ä¿®å¤ï¼šé¿å…ä½¿ç”¨0.5çš„æµ®ç‚¹æ•°
            static_detection = np.logical_and(detection_mask, ~motion_mask).astype(np.uint8)

            # ç»„åˆï¼šç§»åŠ¨ç‰©ä½“å®Œå…¨ä¿ç•™ï¼Œé™æ­¢æ£€æµ‹ç‰©ä½“ä¹Ÿä¿ç•™ï¼ˆå¯èƒ½æ˜¯æš‚æ—¶é™æ­¢çš„è½¦ï¼‰
            refined_mask = np.logical_or(moving_objects, static_detection).astype(np.uint8)

            self.prev_frame = current_gray

            print(
                f"ğŸƒ Motion refinement: moving={moving_objects.sum()}, static={static_detection.sum()}, refined={refined_mask.sum()}")

            return refined_mask

        except Exception as e:
            print(f"âŒ Motion detection failed: {e}")
            self.prev_frame = current_gray
            return None  # å¤±è´¥æ—¶è¿”å›Noneï¼Œä¸ä¿®æ”¹åŸmask

    def get_static_mask_for_gaussian_init(self, image, frame_idx=None):
        """
        ä¸ºé«˜æ–¯ä½“åˆå§‹åŒ–è·å–é™æ€åŒºåŸŸmaskï¼ˆç§»é™¤åœ°é¢ä¿®å¤åŠŸèƒ½ï¼‰
        è¿™ä¸ªå‡½æ•°ä¼šè¿”å›å¯ä»¥ç”¨äºåˆå§‹åŒ–é«˜æ–¯ä½“çš„åŒºåŸŸï¼ˆæ’é™¤åŠ¨æ€ç‰©ä½“ï¼‰

        Returns:
            static_mask: é™æ€åŒºåŸŸmask
            image: åŸå§‹å›¾åƒï¼ˆæ— ä¿®å¤ï¼‰
        """
        # è·å–åŠ¨æ€ç‰©ä½“mask
        dynamic_mask, confidence, original_image = self.detect_and_segment(image, frame_idx)

        # é™æ€maskæ˜¯åŠ¨æ€maskçš„åå‘
        static_mask = (1 - dynamic_mask).astype(np.uint8)

        return static_mask, original_image

    def _temporal_consistency(self, current_mask):
        """æ—¶é—´ä¸€è‡´æ€§æ»¤æ³¢ï¼Œå‡å°‘maskçš„é—ªçƒ"""
        self.mask_history.append(current_mask.copy())

        if len(self.mask_history) > self.history_length:
            self.mask_history.pop(0)

        if len(self.mask_history) < 3:
            return current_mask

        # ä½¿ç”¨å†å²maskçš„ä¸­ä½æ•°æ»¤æ³¢
        mask_stack = np.stack(self.mask_history, axis=0)
        consistent_mask = np.median(mask_stack, axis=0).astype(np.uint8)

        return consistent_mask


class FrontEnd(mp.Process):
    def __init__(self, config, model, save_dir=None):
        super().__init__()
        self.config = config
        self.background = None
        self.pipeline_params = None
        self.frontend_queue = None
        self.backend_queue = None
        self.q_main2vis = None
        self.q_vis2main = None
        self.save_dir = save_dir

        self.initialized = False
        self.kf_indices = []
        self.monocular = config["Training"]["monocular"]
        self.iteration_count = 0
        self.occ_aware_visibility = {}
        self.current_window = []

        self.reset = True
        self.requested_init = False
        self.requested_keyframe = 0
        self.use_every_n_frames = 1

        self.gaussians = None
        self.cameras = dict()
        self.device = "cuda:0"
        self.pause = False

        self.model = model  # MASt3R Model
        self.theta = 0

        # åˆå§‹åŒ–å¢å¼ºçš„åŠ¨æ€ç‰©ä½“é®ç½©å™¨ï¼ˆç§»é™¤åœ°é¢åŠŸèƒ½ï¼‰
        self.enable_dynamic_filtering = config.get("dynamic_filtering", {}).get("enabled", True)
        self.filter_initialization = config.get("dynamic_filtering", {}).get("filter_initialization", True)
        self.save_masked_images = config.get("dynamic_filtering", {}).get("save_masked_images", True)

        if self.enable_dynamic_filtering and self.filter_initialization:
            # è®¾ç½®ä¿å­˜ç›®å½•
            mask_save_dir = config.get("dynamic_filtering", {}).get("save_dir", "./masked_images")
            scene_type = config.get("dynamic_filtering", {}).get("scene_type", "outdoor_street")

            # åˆ›å»ºåŠ¨æ€é®ç½©å™¨ï¼ˆç§»é™¤åœ°é¢ç›¸å…³å‚æ•°ï¼‰
            self.dynamic_masker = EnhancedDynamicObjectMasker(
                device=self.device,
                use_sam=config.get("dynamic_filtering", {}).get("use_sam", True),
                sam_checkpoint="/home/zwk/ä¸‹è½½/S3PO-GS-main/utils/sam_vit_b_01ec64.pth",
                save_dir=mask_save_dir,
                save_images=self.save_masked_images,
                scene_type=scene_type
            )

            # ä»é…ç½®è®¾ç½®åœºæ™¯
            self.dynamic_masker.set_scene_from_config(config)

            print(f"ğŸ¯ Enhanced Dynamic Filtering with local Grounding DINO:")
            print(f"  - Model: /home/zwk/ä¸‹è½½/S3PO-GS-main/groundingdino_swint_ogc.pth")
            print(f"  - Enabled: {self.enable_dynamic_filtering}")
            print(f"  - Filter initialization: {self.filter_initialization}")
            print(f"  - SAM: {config.get('dynamic_filtering', {}).get('use_sam', True)}")
            print(f"  - Save images: {self.save_masked_images}")
        else:
            print("âŒ Dynamic filtering is DISABLED - dynamic objects will appear in reconstruction")

    def set_hyperparams(self):
        self.save_dir = self.config["Results"]["save_dir"]
        self.save_results = self.config["Results"]["save_results"]
        self.save_trj = self.config["Results"]["save_trj"]
        self.save_trj_kf_intv = self.config["Results"]["save_trj_kf_intv"]

        self.tracking_itr_num = self.config["Training"]["tracking_itr_num"]
        self.kf_interval = self.config["Training"]["kf_interval"]
        self.window_size = self.config["Training"]["window_size"]
        self.single_thread = self.config["Training"]["single_thread"]

    def _expand_dynamic_mask(self, dynamic_mask, kernel_size=5):
        """æ‰©å±•åŠ¨æ€ç‰©ä½“maskï¼Œé¿å…è¾¹ç•Œå¤„çš„é«˜æ–¯ä½“ç”Ÿæˆ"""
        mask_np = dynamic_mask.cpu().numpy().astype(np.uint8)
        kernel = np.ones((kernel_size, kernel_size), np.uint8)
        expanded_mask_np = cv2.dilate(mask_np, kernel, iterations=1)
        expanded_mask = torch.from_numpy(expanded_mask_np).to(dynamic_mask.device).bool()
        return expanded_mask

    def add_new_keyframe(self, cur_frame_idx, depth=None, opacity=None, init=False):
        """æ”¹è¿›çš„å…³é”®å¸§æ·»åŠ ï¼Œç‰¹åˆ«å¤„ç†ç¬¬ä¸€å¸§ï¼ˆç§»é™¤åœ°é¢åŠŸèƒ½ï¼‰"""
        rgb_boundary_threshold = self.config["Training"]["rgb_boundary_threshold"]
        if len(self.kf_indices) > 0:
            last_kf = self.kf_indices[-1]
            viewpoint_last = self.cameras[last_kf]
            R_last = viewpoint_last.R

        self.kf_indices.append(cur_frame_idx)
        viewpoint = self.cameras[cur_frame_idx]

        # è®¡ç®—è§’åº¦å·®
        R_now = viewpoint.R
        if len(self.kf_indices) > 1:
            R_now = R_now.to(torch.float32)
            R_last = R_last.to(torch.float32)
            R_diff = torch.matmul(R_last.T, R_now)
            trace_R_diff = torch.trace(R_diff)
            theta_rad = torch.acos((trace_R_diff - 1) / 2)
            theta_deg = torch.rad2deg(theta_rad)
            self.theta = theta_deg

        gt_img = viewpoint.original_image.cuda()
        valid_rgb = (gt_img.sum(dim=0) > rgb_boundary_threshold)[None]

        # ===== æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨Grounding DINOç”ŸæˆåŠ¨æ€ç‰©ä½“é®ç½©ï¼ˆç§»é™¤åœ°é¢åŠŸèƒ½ï¼‰ =====
        dynamic_mask = None
        static_mask = None
        is_first_frame = (cur_frame_idx == 0)

        if self.enable_dynamic_filtering and (not init or self.filter_initialization):
            # è½¬æ¢å›¾åƒæ ¼å¼ç”¨äºGrounding DINO
            img_np = gt_img.permute(1, 2, 0).cpu().numpy()  # [H, W, 3]
            img_np = (img_np * 255).astype(np.uint8)

            if is_first_frame:
                print(f"ğŸ”° Processing FIRST keyframe {cur_frame_idx} with enhanced detection")
                # ç»™ç¬¬ä¸€å¸§ä¸€äº›é¢å¤–æ—¶é—´ç¡®ä¿æ¨¡å‹å®Œå…¨åŠ è½½
                time.sleep(0.2)

            # ä½¿ç”¨æ–°çš„Grounding DINOæ–¹æ³•è·å–é™æ€maskï¼ˆæ— åœ°é¢ä¿®å¤ï¼‰
            static_mask_np, original_image_np = self.dynamic_masker.get_static_mask_for_gaussian_init(
                img_np, frame_idx=cur_frame_idx
            )

            dynamic_mask = torch.from_numpy(1 - static_mask_np).to(self.device).bool()
            static_mask = torch.from_numpy(static_mask_np).to(self.device).bool()

            # å…³é”®æ”¹åŠ¨ï¼šä»æœ‰æ•ˆåŒºåŸŸä¸­å®Œå…¨æ’é™¤åŠ¨æ€ç‰©ä½“
            # æ‰©å±•åŠ¨æ€maskè¾¹ç•Œï¼Œç¡®ä¿è¾¹ç•Œå¤„ä¹Ÿä¸ç”Ÿæˆé«˜æ–¯ä½“ - ç¬¬ä¸€å¸§æ›´ä¿å®ˆ
            kernel_size = 9 if is_first_frame else 7
            expanded_dynamic_mask = self._expand_dynamic_mask(dynamic_mask, kernel_size=kernel_size)
            expanded_static_mask = ~expanded_dynamic_mask

            # åº”ç”¨æ‰©å±•åçš„é™æ€mask
            valid_rgb = valid_rgb & expanded_static_mask[None]

            # å­˜å‚¨maskä¿¡æ¯
            viewpoint.dynamic_mask = dynamic_mask
            viewpoint.expanded_dynamic_mask = expanded_dynamic_mask
            viewpoint.static_mask = static_mask
            viewpoint.expanded_static_mask = expanded_static_mask

            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            static_ratio = static_mask.float().mean().item()
            expanded_static_ratio = expanded_static_mask.float().mean().item()

            print(f"ğŸ”§ {'[FIRST FRAME] ' if is_first_frame else ''}Frame {cur_frame_idx} processing:")
            print(f"  ğŸ“Š Original static ratio: {static_ratio:.1%}")
            print(f"  ğŸ“Š Expanded static ratio: {expanded_static_ratio:.1%}")
            print(f"  ğŸ›¡ï¸  Safety margin: {(static_ratio - expanded_static_ratio) * 100:.1f}%")

            # ç¬¬ä¸€å¸§æ£€æŸ¥
            if is_first_frame and expanded_static_ratio < 0.15:
                print(f"ğŸš¨ CRITICAL: First frame static region only {expanded_static_ratio:.1%}!")
                print("    This will severely affect initialization!")
                print("    Consider:")
                print("    1. Checking if scene type is correct")
                print("    2. Reducing confidence threshold")
                print("    3. Reducing mask expansion")

            # å¦‚æœé™æ€åŒºåŸŸå¤ªå°‘ï¼Œå‘å‡ºè­¦å‘Š
            if expanded_static_ratio < 0.2:
                print(f"âš ï¸  WARNING: Expanded static region only {expanded_static_ratio:.1%}!")
                print("    This may severely affect reconstruction quality.")
                print("    Consider reducing mask expansion or adjusting scene prompts.")
            elif expanded_static_ratio < 0.4:
                print(f"âš ï¸  CAUTION: Limited static region {expanded_static_ratio:.1%}")
                print("    Tracking may be less stable.")
        # ============================================================

        if self.monocular:
            if depth is None:
                initial_depth = torch.from_numpy(viewpoint.mono_depth).unsqueeze(0)
                print(f"Initial depth map stats for frame {cur_frame_idx}:",
                      f"Max: {torch.max(initial_depth).item():.3f}",
                      f"Min: {torch.min(initial_depth).item():.3f}",
                      f"Mean: {torch.mean(initial_depth).item():.3f}")

                # å°†æ— æ•ˆåŒºåŸŸï¼ˆåŒ…æ‹¬åŠ¨æ€åŒºåŸŸï¼‰æ·±åº¦è®¾ä¸º0
                # ç”±äºvalid_rgbå·²ç»æ’é™¤äº†åŠ¨æ€åŒºåŸŸï¼Œè¿™é‡Œä¼šè‡ªåŠ¨å¤„ç†
                initial_depth[~valid_rgb.cpu()] = 0

                # é¢å¤–çš„ç»Ÿè®¡ä¿¡æ¯
                if dynamic_mask is not None:
                    total_pixels = initial_depth[0].numel()
                    valid_pixels = (initial_depth[0] > 0).sum().item()
                    print(
                        f"  Valid depth pixels: {valid_pixels}/{total_pixels} ({valid_pixels / total_pixels * 100:.1f}%)")

                return initial_depth[0].numpy()
            else:
                depth = depth.detach().clone()
                opacity = opacity.detach()
                initial_depth = depth

                # æ·±åº¦å¤„ç†
                render_depth = initial_depth.cpu().numpy()[0]
                initial_depth, scale_factor, error_mask, num_accurate_pixels = process_depth(
                    render_depth, viewpoint.mono_depth,
                    last_depth=viewpoint_last.mono_depth,
                    im1=viewpoint_last.original_image,
                    im2=viewpoint.original_image,
                    model=self.model,
                    patch_size=self.config["depth"]["patch_size"],
                    mean_threshold=self.config["depth"]["mean_threshold"],
                    std_threshold=self.config["depth"]["std_threshold"],
                    error_threshold=self.config["depth"]["error_threshold"],
                    final_error_threshold=self.config["depth"]["final_error_threshold"],
                    min_accurate_pixels_ratio=self.config["depth"]["min_accurate_pixels_ratio"]
                )

                viewpoint.mono_depth = viewpoint.mono_depth * scale_factor

                # åº”ç”¨å®Œæ•´çš„æ©ç ï¼ˆåŒ…æ‹¬RGBè¾¹ç•Œå’ŒåŠ¨æ€ç‰©ä½“ï¼‰
                valid_rgb_np = valid_rgb.cpu().numpy() if isinstance(valid_rgb, torch.Tensor) else valid_rgb
                if initial_depth.shape == valid_rgb_np.shape[1:]:
                    initial_depth[~valid_rgb_np[0]] = 0

            return initial_depth

        # ä½¿ç”¨ground truthæ·±åº¦
        initial_depth = torch.from_numpy(viewpoint.depth).unsqueeze(0)
        # åº”ç”¨æ©ç ï¼ˆvalid_rgbå·²ç»åŒ…å«äº†åŠ¨æ€ç‰©ä½“æ’é™¤ï¼‰
        initial_depth[~valid_rgb.cpu()] = 0

        return initial_depth[0].numpy()

    def tracking(self, cur_frame_idx, viewpoint):
        """è·Ÿè¸ªå‡½æ•°ï¼ŒåŒ…å«Grounding DINOåŠ¨æ€ç‰©ä½“è¿‡æ»¤ï¼ˆç§»é™¤åœ°é¢åŠŸèƒ½ï¼‰"""
        # ç”ŸæˆåŠ¨æ€ç‰©ä½“é®ç½©ï¼ˆä¸»è¦ç”¨äºç»Ÿè®¡å’Œå¯è§†åŒ–ï¼‰
        if self.enable_dynamic_filtering:
            gt_img = viewpoint.original_image
            img_np = gt_img.permute(1, 2, 0).cpu().numpy()
            img_np = (img_np * 255).astype(np.uint8)

            # ä½¿ç”¨Grounding DINOæ£€æµ‹æ–¹æ³•ï¼ˆæ— åœ°é¢ä¿®å¤ï¼‰
            static_mask_np, original_image_np = self.dynamic_masker.get_static_mask_for_gaussian_init(
                img_np, frame_idx=cur_frame_idx
            )

            dynamic_mask = torch.from_numpy(1 - static_mask_np).to(self.device).bool()
            static_mask = torch.from_numpy(static_mask_np).to(self.device).bool()

            viewpoint.dynamic_mask = dynamic_mask
            viewpoint.static_mask = static_mask

            static_ratio = viewpoint.static_mask.float().mean().item()
            print(f"ğŸ¬ Tracking frame {cur_frame_idx}: Static ratio={static_ratio:.1%} (Grounding DINO)")

        # åŸæœ‰çš„è·Ÿè¸ªé€»è¾‘
        prev = self.cameras[cur_frame_idx - self.use_every_n_frames]
        pose_prev = getWorld2View2(prev.R, prev.T)

        last_keyframe_idx = self.current_window[0]
        last_kf = self.cameras[last_keyframe_idx]
        pose_last_kf = getWorld2View2(last_kf.R, last_kf.T)
        img1 = last_kf.original_image

        img2 = viewpoint.original_image
        rel_pose, render_depth = get_pose(
            img1=img1, img2=img2, model=self.model,
            dist_coeffs=self.dataset.dist_coeffs,
            viewpoint=last_kf, gaussians=self.gaussians,
            pipeline_params=self.pipeline_params, background=self.background
        )

        viewpoint.mono_depth = get_depth(img2, img2, self.model)

        identity_matrix = torch.eye(4, device=self.device)
        rel_pose = torch.from_numpy(rel_pose).to(self.device).float()

        if torch.allclose(rel_pose, identity_matrix, atol=1e-6):
            pose_init = rel_pose @ pose_last_kf
            viewpoint.update_RT(prev.R, prev.T)
        else:
            pose_init = rel_pose @ pose_last_kf
            viewpoint.update_RT(pose_init[:3, :3], pose_init[:3, 3])

        # ä¼˜åŒ–å‚æ•°
        opt_params = []
        opt_params.append({
            "params": [viewpoint.cam_rot_delta],
            "lr": self.config["Training"]["lr"]["cam_rot_delta"],
            "name": "rot_{}".format(viewpoint.uid),
        })
        opt_params.append({
            "params": [viewpoint.cam_trans_delta],
            "lr": self.config["Training"]["lr"]["cam_trans_delta"],
            "name": "trans_{}".format(viewpoint.uid),
        })
        opt_params.append({
            "params": [viewpoint.exposure_a],
            "lr": 0.01,
            "name": "exposure_a_{}".format(viewpoint.uid),
        })
        opt_params.append({
            "params": [viewpoint.exposure_b],
            "lr": 0.01,
            "name": "exposure_b_{}".format(viewpoint.uid),
        })

        pose_optimizer = torch.optim.Adam(opt_params)

        for tracking_itr in range(self.tracking_itr_num):
            render_pkg = render(
                viewpoint, self.gaussians, self.pipeline_params, self.background
            )
            image, depth, opacity = (
                render_pkg["render"],
                render_pkg["depth"],
                render_pkg["opacity"],
            )

            pose_optimizer.zero_grad()

            # ===== åŠ¨æ€æ„ŸçŸ¥çš„æŸå¤±è®¡ç®— =====
            # æŸå¤±å‡½æ•°ç°åœ¨ä¼šè‡ªåŠ¨æ’é™¤åŠ¨æ€åŒºåŸŸï¼Œä¸é«˜æ–¯ä½“ç”Ÿæˆä¿æŒä¸€è‡´
            loss_tracking = get_loss_tracking(self.config, image, depth, opacity, viewpoint)

            # è°ƒè¯•ä¿¡æ¯ï¼šæ¯50æ¬¡è¿­ä»£è¾“å‡ºä¸€æ¬¡maskè¦†ç›–æƒ…å†µ
            if tracking_itr == 0 and self.enable_dynamic_filtering:
                try:
                    from utils.slam_utils import debug_loss_mask_coverage
                    debug_loss_mask_coverage(self.config, viewpoint, verbose=False)
                except:
                    pass
            # ===================================

            loss_tracking.backward()

            with torch.no_grad():
                pose_optimizer.step()
                converged = update_pose(viewpoint)

            if tracking_itr % 10 == 0:
                self.q_main2vis.put(
                    gui_utils.GaussianPacket(
                        current_frame=viewpoint,
                        gtcolor=viewpoint.original_image,
                        gtdepth=viewpoint.depth if not self.monocular
                        else np.zeros((viewpoint.image_height, viewpoint.image_width)),
                    )
                )
            if converged:
                break

        self.median_depth = get_median_depth(depth, opacity)
        return render_pkg

    def save_keyframe_mask(self, viewpoint, cur_frame_idx):
        """ä¸ºå…³é”®å¸§ä¿å­˜ç‰¹æ®Šæ ‡è®°çš„æ©ç å›¾åƒï¼ˆç§»é™¤åœ°é¢åŠŸèƒ½ï¼‰"""
        if (not self.enable_dynamic_filtering or
                not self.save_masked_images or
                not hasattr(viewpoint, 'dynamic_mask')):
            return

        try:
            # åˆ›å»ºå…³é”®å¸§ç›®å½•
            kf_dir = os.path.join(self.dynamic_masker.save_dir, "keyframes")
            os.makedirs(kf_dir, exist_ok=True)

            # è·å–åŸå§‹å›¾åƒ
            gt_image = viewpoint.original_image  # [3, H, W] tensor
            img_np = gt_image.permute(1, 2, 0).cpu().numpy()
            img_np = (img_np * 255).astype(np.uint8)
            img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)

            # è·å–åŠ¨æ€maskå’Œæ‰©å±•mask
            dynamic_mask = viewpoint.dynamic_mask.cpu().numpy().astype(np.uint8)
            if hasattr(viewpoint, 'expanded_dynamic_mask'):
                expanded_mask = viewpoint.expanded_dynamic_mask.cpu().numpy().astype(np.uint8)
            else:
                expanded_mask = dynamic_mask

            # åˆ›å»ºå…³é”®å¸§ç‰¹æ®Šæ ‡è®°å›¾åƒ
            # åŸå§‹åŠ¨æ€åŒºåŸŸç”¨çº¢è‰²ï¼Œæ‰©å±•åŒºåŸŸç”¨é»„è‰²
            kf_img = img_bgr.copy()
            kf_img[dynamic_mask > 0] = [0, 0, 255]  # çº¢è‰²ï¼šåŸå§‹åŠ¨æ€åŒºåŸŸ
            kf_img[(expanded_mask > 0) & (dynamic_mask == 0)] = [0, 255, 255]  # é»„è‰²ï¼šæ‰©å±•åŒºåŸŸ

            # ä¿å­˜å…³é”®å¸§å›¾åƒ
            kf_path = os.path.join(kf_dir, f"keyframe_{cur_frame_idx:06d}_grounding_dino.jpg")
            cv2.imwrite(kf_path, kf_img)

            print(
                f"ğŸ’¾ Saved Grounding DINO keyframe mask for frame {cur_frame_idx} (Red=Dynamic, Yellow=Expanded)")

        except Exception as e:
            print(f"Warning: Failed to save keyframe mask for frame {cur_frame_idx}: {e}")

    def is_keyframe(self, cur_frame_idx, last_keyframe_idx, cur_frame_visibility_filter, occ_aware_visibility):
        """è€ƒè™‘Grounding DINOåŠ¨æ€ç‰©ä½“æ£€æµ‹çš„å…³é”®å¸§é€‰æ‹©ç­–ç•¥ï¼ˆç§»é™¤åœ°é¢åŠŸèƒ½ï¼‰"""
        kf_translation = self.config["Training"]["kf_translation"]
        kf_min_translation = self.config["Training"]["kf_min_translation"]
        kf_overlap = self.config["Training"]["kf_overlap"]

        curr_frame = self.cameras[cur_frame_idx]
        last_kf = self.cameras[last_keyframe_idx]

        # è®¡ç®—ç›¸æœºè¿åŠ¨
        pose_CW = getWorld2View2(curr_frame.R, curr_frame.T)
        last_kf_CW = getWorld2View2(last_kf.R, last_kf.T)
        last_kf_WC = torch.linalg.inv(last_kf_CW)
        dist = torch.norm((pose_CW @ last_kf_WC)[0:3, 3])

        dist_check = dist > kf_translation * self.median_depth
        dist_check2 = dist > kf_min_translation * self.median_depth

        # è®¡ç®—é‡å åº¦
        union = torch.logical_or(
            cur_frame_visibility_filter, occ_aware_visibility[last_keyframe_idx]
        ).count_nonzero()
        intersection = torch.logical_and(
            cur_frame_visibility_filter, occ_aware_visibility[last_keyframe_idx]
        ).count_nonzero()

        # ===== åŠ¨æ€åœºæ™¯çš„å…³é”®å¸§ç­–ç•¥è°ƒæ•´ =====
        adjusted_overlap = kf_overlap

        if hasattr(curr_frame, 'expanded_static_mask'):
            # æ£€æŸ¥æ‰©å±•åçš„é™æ€åŒºåŸŸæ¯”ä¾‹
            static_ratio = curr_frame.expanded_static_mask.float().mean().item()
            if static_ratio < 0.3:
                # é™æ€åŒºåŸŸå¤ªå°‘ï¼Œæ›´ç§¯æåˆ›å»ºå…³é”®å¸§
                adjusted_overlap = kf_overlap * 0.7
                print(
                    f"ğŸ”„ Limited static region ({static_ratio:.1%}) after Grounding DINO, adjusted overlap: {adjusted_overlap:.3f}")
        # ==========================================

        point_ratio_2 = intersection / union
        return (point_ratio_2 < adjusted_overlap and dist_check2) or dist_check

    def add_to_window(self, cur_frame_idx, cur_frame_visibility_filter, occ_aware_visibility, window):
        # åŸæœ‰å®ç°ä¿æŒä¸å˜
        N_dont_touch = 2
        window = [cur_frame_idx] + window
        curr_frame = self.cameras[cur_frame_idx]
        to_remove = []
        removed_frame = None

        for i in range(N_dont_touch, len(window)):
            kf_idx = window[i]
            intersection = torch.logical_and(
                cur_frame_visibility_filter, occ_aware_visibility[kf_idx]
            ).count_nonzero()
            denom = min(
                cur_frame_visibility_filter.count_nonzero(),
                occ_aware_visibility[kf_idx].count_nonzero(),
            )
            point_ratio_2 = intersection / denom
            cut_off = self.config["Training"].get("kf_cutoff", 0.4)
            if not self.initialized:
                cut_off = 0.4
            if (point_ratio_2 <= cut_off) and (len(window) > self.config["Training"]["window_size"]):
                to_remove.append(kf_idx)

        if to_remove:
            window.remove(to_remove[-1])
            removed_frame = to_remove[-1]

        kf_0_WC = torch.linalg.inv(getWorld2View2(curr_frame.R, curr_frame.T))

        if len(window) > self.config["Training"]["window_size"]:
            inv_dist = []
            for i in range(N_dont_touch, len(window)):
                inv_dists = []
                kf_i_idx = window[i]
                kf_i = self.cameras[kf_i_idx]
                kf_i_CW = getWorld2View2(kf_i.R, kf_i.T)
                for j in range(N_dont_touch, len(window)):
                    if i == j:
                        continue
                    kf_j_idx = window[j]
                    kf_j = self.cameras[kf_j_idx]
                    kf_j_WC = torch.linalg.inv(getWorld2View2(kf_j.R, kf_j.T))
                    T_CiCj = kf_i_CW @ kf_j_WC
                    inv_dists.append(1.0 / (torch.norm(T_CiCj[0:3, 3]) + 1e-6).item())
                T_CiC0 = kf_i_CW @ kf_0_WC
                k = torch.sqrt(torch.norm(T_CiC0[0:3, 3])).item()
                inv_dist.append(k * sum(inv_dists))

            idx = np.argmax(inv_dist)
            removed_frame = window[N_dont_touch + idx]
            window.remove(removed_frame)

        return window, removed_frame

    def request_keyframe(self, cur_frame_idx, viewpoint, current_window, depthmap):
        msg = ["keyframe", cur_frame_idx, viewpoint, current_window, depthmap, self.theta]
        self.backend_queue.put(msg)
        self.requested_keyframe += 1

    def reqeust_mapping(self, cur_frame_idx, viewpoint):
        msg = ["map", cur_frame_idx, viewpoint]
        self.backend_queue.put(msg)

    def request_init(self, cur_frame_idx, viewpoint, depth_map):
        msg = ["init", cur_frame_idx, viewpoint, depth_map]
        self.backend_queue.put(msg)
        self.requested_init = True

    def sync_backend(self, data):
        self.gaussians = data[1]
        occ_aware_visibility = data[2]
        keyframes = data[3]
        self.occ_aware_visibility = occ_aware_visibility

        for kf_id, kf_R, kf_T in keyframes:
            self.cameras[kf_id].update_RT(kf_R.clone(), kf_T.clone())

    def cleanup(self, cur_frame_idx):
        self.cameras[cur_frame_idx].clean()
        if cur_frame_idx % 10 == 0:
            torch.cuda.empty_cache()

    def initialize(self, cur_frame_idx, viewpoint):
        """æ”¹è¿›çš„åˆå§‹åŒ–æ–¹æ³•ï¼Œç¡®ä¿ç¬¬ä¸€å¸§æ­£ç¡®å¤„ç†ï¼ˆç§»é™¤åœ°é¢åŠŸèƒ½ï¼‰"""
        self.initialized = not self.monocular
        self.kf_indices = []
        self.iteration_count = 0
        self.occ_aware_visibility = {}
        self.current_window = []

        while not self.backend_queue.empty():
            self.backend_queue.get()

        viewpoint.update_RT(viewpoint.R_gt, viewpoint.T_gt)

        img = viewpoint.original_image
        viewpoint.mono_depth = get_depth(img, img, self.model)

        # åœ¨åˆå§‹åŒ–é˜¶æ®µå°±åº”ç”¨Grounding DINOåŠ¨æ€è¿‡æ»¤ï¼ˆç§»é™¤åœ°é¢åŠŸèƒ½ï¼‰
        print(f"ğŸ”„ INITIALIZING with frame {cur_frame_idx}")
        if self.enable_dynamic_filtering and self.filter_initialization:
            print("  âœ… Enhanced first-frame Grounding DINO filtering ENABLED")
            print("  ğŸ”° Using conservative detection for initialization")
            print(f"  ğŸ¯ Scene type: {self.dynamic_masker.prompt_manager.current_scene}")

            # ç»™åŠ¨æ€é®ç½©å™¨ä¸€äº›æ—¶é—´ç¡®ä¿æ¨¡å‹å®Œå…¨åŠ è½½
            time.sleep(0.1)

        elif self.enable_dynamic_filtering and not self.filter_initialization:
            print("  âš ï¸  Grounding DINO filtering enabled but SKIPPING initialization frame")
        else:
            print("  âŒ Dynamic filtering DISABLED - cars may appear as ghosts!")

        self.kf_indices = []
        depth_map = self.add_new_keyframe(cur_frame_idx, init=True)
        self.request_init(cur_frame_idx, viewpoint, depth_map)
        self.reset = False

    def run(self):
        # ä¸»æ‰§è¡Œå¾ªç¯é›†æˆäº†Grounding DINOåŠ¨æ€ç‰©ä½“è¿‡æ»¤ï¼ˆç§»é™¤åœ°é¢åŠŸèƒ½ï¼‰
        cur_frame_idx = 0
        projection_matrix = getProjectionMatrix2(
            znear=0.01, zfar=100.0,
            fx=self.dataset.fx, fy=self.dataset.fy,
            cx=self.dataset.cx, cy=self.dataset.cy,
            W=self.dataset.width, H=self.dataset.height,
        ).transpose(0, 1)
        projection_matrix = projection_matrix.to(device=self.device)

        tic = torch.cuda.Event(enable_timing=True)
        toc = torch.cuda.Event(enable_timing=True)

        while True:
            # å¤„ç†GUIæ¶ˆæ¯
            if self.q_vis2main.empty():
                if self.pause:
                    continue
            else:
                data_vis2main = self.q_vis2main.get()
                self.pause = data_vis2main.flag_pause
                if self.pause:
                    self.backend_queue.put(["pause"])
                    continue
                else:
                    self.backend_queue.put(["unpause"])

            if self.frontend_queue.empty():
                tic.record()

                # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                if cur_frame_idx >= len(self.dataset):
                    if self.save_results:
                        eval_ate(self.cameras, self.kf_indices, self.save_dir, 0,
                                 final=True, monocular=self.monocular)
                        save_gaussians(self.gaussians, self.save_dir, "final", final=True)
                    break

                # ç­‰å¾…åˆå§‹åŒ–
                if self.requested_init:
                    time.sleep(0.01)
                    continue

                if self.single_thread and self.requested_keyframe > 0:
                    time.sleep(0.01)
                    continue

                if not self.initialized and self.requested_keyframe > 0:
                    time.sleep(0.01)
                    continue

                # å¤„ç†å½“å‰å¸§
                viewpoint = Camera.init_from_dataset(
                    self.dataset, cur_frame_idx, projection_matrix
                )
                viewpoint.compute_grad_mask(self.config)
                self.cameras[cur_frame_idx] = viewpoint

                # åˆå§‹åŒ–
                if self.reset:
                    self.initialize(cur_frame_idx, viewpoint)
                    self.current_window.append(cur_frame_idx)
                    cur_frame_idx += 1
                    continue

                self.initialized = self.initialized or (len(self.current_window) == self.window_size)

                # è·Ÿè¸ª
                render_pkg = self.tracking(cur_frame_idx, viewpoint)
                current_window_dict = {}
                current_window_dict[self.current_window[0]] = self.current_window[1:]
                keyframes = [self.cameras[kf_idx] for kf_idx in self.current_window]

                self.q_main2vis.put(
                    gui_utils.GaussianPacket(
                        gaussians=clone_obj(self.gaussians),
                        current_frame=viewpoint,
                        keyframes=keyframes,
                        kf_window=current_window_dict,
                    )
                )

                if self.requested_keyframe > 0:
                    self.cleanup(cur_frame_idx)
                    cur_frame_idx += 1
                    continue

                # å…³é”®å¸§é€‰æ‹©
                last_keyframe_idx = self.current_window[0]
                check_time = (cur_frame_idx - last_keyframe_idx) >= self.kf_interval
                curr_visibility = (render_pkg["n_touched"] > 0).long()

                create_kf = self.is_keyframe(
                    cur_frame_idx, last_keyframe_idx,
                    curr_visibility, self.occ_aware_visibility,
                )

                if len(self.current_window) < self.window_size:
                    union = torch.logical_or(
                        curr_visibility, self.occ_aware_visibility[last_keyframe_idx]
                    ).count_nonzero()
                    intersection = torch.logical_and(
                        curr_visibility, self.occ_aware_visibility[last_keyframe_idx]
                    ).count_nonzero()
                    point_ratio = intersection / union
                    create_kf = (check_time and
                                 point_ratio < self.config["Training"]["kf_overlap"])

                if self.single_thread:
                    create_kf = check_time and create_kf

                if create_kf:
                    self.current_window, removed = self.add_to_window(
                        cur_frame_idx, curr_visibility,
                        self.occ_aware_visibility, self.current_window,
                    )
                    depth_map = self.add_new_keyframe(
                        cur_frame_idx,
                        depth=render_pkg["depth"],
                        opacity=render_pkg["opacity"],
                        init=False,
                    )
                    self.request_keyframe(
                        cur_frame_idx, viewpoint, self.current_window, depth_map
                    )

                    # ä¸ºå…³é”®å¸§ä¿å­˜ç‰¹æ®Šæ ‡è®°çš„æ©ç å›¾åƒ
                    self.save_keyframe_mask(viewpoint, cur_frame_idx)
                else:
                    self.cleanup(cur_frame_idx)

                cur_frame_idx += 1

                # è¯„ä¼°
                if (self.save_results and self.save_trj and create_kf and
                        len(self.kf_indices) % self.save_trj_kf_intv == 0):
                    Log("Evaluating ATE at frame: ", cur_frame_idx)
                    eval_ate(self.cameras, self.kf_indices, self.save_dir,
                             cur_frame_idx, monocular=self.monocular)

                toc.record()
                torch.cuda.synchronize()
                if create_kf:
                    duration = tic.elapsed_time(toc)
                    time.sleep(max(0.01, 1.0 / 3.0 - duration / 1000))

            else:
                # å¤„ç†åç«¯æ¶ˆæ¯
                data = self.frontend_queue.get()
                if data[0] == "sync_backend":
                    self.sync_backend(data)
                elif data[0] == "keyframe":
                    self.sync_backend(data)
                    self.requested_keyframe -= 1
                elif data[0] == "init":
                    self.sync_backend(data)
                    self.requested_init = False
                elif data[0] == "stop":
                    Log("Frontend Stopped.")
                    break