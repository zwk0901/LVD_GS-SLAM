import time
import numpy as np
import torch
import torch.multiprocessing as mp
import os
import cv2
from PIL import Image
import torch.nn.functional as F
import colorsys
from typing import Dict, List, Tuple, Optional
from groundingdino.util import box_ops
# Grounding DINO imports with fallback handling
try:
    from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection

    GROUNDING_DINO_AVAILABLE = True
except ImportError:
    print("âš ï¸  Warning: transformers not available. Grounding DINO will be disabled.")
    GROUNDING_DINO_AVAILABLE = False

# SAM imports with fallback handling
try:
    from segment_anything import SamPredictor, sam_model_registry

    SAM_AVAILABLE = True
except ImportError:
    print("âš ï¸  Warning: segment_anything not available. SAM will be disabled.")
    SAM_AVAILABLE = False

from gaussian_splatting.gaussian_renderer import render
from gaussian_splatting.utils.graphics_utils import getProjectionMatrix2, getWorld2View2
from gui import gui_utils
from utils.camera_utils import Camera
from utils.eval_utils import eval_ate, save_gaussians
from utils.logging_utils import Log
from utils.multiprocessing_utils import clone_obj
from utils.pose_utils import update_pose
from utils.slam_utils import get_loss_tracking, get_median_depth
from utils.init_pose import get_pose, get_depth
from utils.depth_utils import process_depth

try:
    from groundingdino.util.inference import load_model, load_image, predict, annotate
    import groundingdino.datasets.transforms as T
    from groundingdino.models import build_model
    from groundingdino.util.slconfig import SLConfig
    from groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap

    GROUNDING_DINO_ORIGINAL = True
    print("âœ… Original Grounding DINO package available")
except ImportError:
    print("âš ï¸  Original Grounding DINO not installed")
    print("   Install with: pip install groundingdino")

    # å°è¯•transformersä½œä¸ºå¤‡é€‰
    try:
        from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection

        GROUNDING_DINO_AVAILABLE = True
        print("âœ… Transformers available as fallback")
    except ImportError:
        print("âš ï¸  Transformers not available either")


class ColorfulSegmentationVisualizer:
    """å½©è‰²ç±»åˆ«åˆ†å‰²å¯è§†åŒ–å™¨ - å®Œå…¨ä¿®å¤ç‰ˆï¼Œè§£å†³è¾¹ç•Œæ¡†åæ ‡é—®é¢˜"""

    def __init__(self):
        # é¢„å®šä¹‰åŠ¨æ€å¯¹è±¡é¢œè‰²ï¼ˆæš–è‰²è°ƒï¼Œè¡¨ç¤ºè¿åŠ¨ï¼‰
        self.dynamic_colors = {
            'person': [0, 0, 255],  # çº¢è‰²
            'people': [0, 0, 255],
            'pedestrian': [0, 0, 255],
            'pedestrians': [0, 0, 255],
            'human': [0, 0, 255],

            'car': [0, 165, 255],  # æ©™è‰²
            'cars': [0, 165, 255],
            'vehicle': [0, 165, 255],
            'vehicles': [0, 165, 255],

            'truck': [0, 255, 255],  # é»„è‰²
            'trucks': [0, 255, 255],

            'bus': [255, 0, 255],  # å“çº¢è‰²
            'buses': [255, 0, 255],

            'bicycle': [128, 0, 255],  # ç´«è‰²
            'bike': [128, 0, 255],
            'bicycles': [128, 0, 255],

            'motorcycle': [255, 0, 128],  # ç²‰çº¢è‰²
            'motorcycles': [255, 0, 128],
            'motorbike': [255, 0, 128],

            'scooter': [0, 128, 255],  # æµ…è“è‰²
            'e-scooter': [0, 128, 255],
            'skateboard': [64, 0, 255],  # æ·±ç´«è‰²
        }

        # é¢„å®šä¹‰é™æ€å¯¹è±¡é¢œè‰²ï¼ˆå†·è‰²è°ƒï¼Œè¡¨ç¤ºé™æ­¢ï¼‰
        self.static_colors = {
            'building': [128, 128, 64],  # æ©„æ¦„è‰²
            'wall': [96, 96, 96],  # ç°è‰²
            'road': [64, 64, 64],  # æ·±ç°è‰²
            'street': [64, 64, 64],
            'pavement': [128, 128, 128],  # æµ…ç°è‰²
            'sidewalk': [160, 160, 160],
            'crosswalk': [192, 192, 192],

            'traffic light': [0, 255, 0],  # ç»¿è‰²
            'stop sign': [0, 100, 200],  # æ·±æ©™è‰²
            'street sign': [0, 150, 150],  # é’è‰²
            'road sign': [0, 150, 150],
            'traffic sign': [0, 150, 150],
            'traffic _ sign': [0, 150, 150],
            'traffic * sign': [0, 150, 150],

            'lamp post': [100, 50, 0],  # æ£•è‰²
            'street lamp': [100, 50, 0],
            'lamp': [100, 50, 0],
            'pole': [100, 50, 0],
            'traffic cone': [0, 200, 255],  # äº®æ©™è‰²

            'tree': [0, 100, 0],  # æ·±ç»¿è‰²
            'fence': [150, 150, 0],  # æ·±é»„è‰²
            'barrier': [100, 100, 0],
            'guardrail': [100, 100, 0],

            'fire hydrant': [255, 100, 0],  # æ©™çº¢è‰²
            'firent': [255, 100, 0],
            'mailbox': [200, 0, 100],  # æ·±çº¢è‰²
            'bench': [150, 100, 50],  # æ£•è¤è‰²
            'curb': [200, 200, 200],  # ç™½è‰²
            'parking meter': [100, 100, 100],  # ä¸­ç°è‰²
            'bollard': [150, 150, 100],  # æš—é»„è‰²
            'street furniture': [120, 120, 120],
            'street _ furniture': [120, 120, 120],
            'furniture': [120, 120, 120],
            'manhole cover': [50, 50, 50],  # å¾ˆæ·±çš„ç°è‰²
            'guard': [80, 80, 80],
        }

        self.color_cache = {}
        self.class_count = 0

    def get_color_for_class(self, class_name: str, is_dynamic: bool = True) -> List[int]:
        """ä¸ºç±»åˆ«è·å–é¢œè‰²ï¼ˆBGRæ ¼å¼ï¼‰"""
        if class_name is None:
            class_name = f"{'dynamic' if is_dynamic else 'static'}_unknown_{self.class_count}"

        class_name = str(class_name).lower().strip()
        class_name_clean = class_name.replace('_', ' ').replace('*', ' ').replace('  ', ' ')

        predefined_colors = self.dynamic_colors if is_dynamic else self.static_colors

        # ç²¾ç¡®åŒ¹é…
        if class_name in predefined_colors:
            return predefined_colors[class_name]

        if class_name_clean in predefined_colors:
            return predefined_colors[class_name_clean]

        # åŒ…å«åŒ¹é…
        for key, color in predefined_colors.items():
            if key in class_name or class_name in key:
                return color
            if key in class_name_clean or class_name_clean in key:
                return color

        # è¯è¯­åŒ¹é…
        class_words = class_name_clean.split()
        for key, color in predefined_colors.items():
            key_words = key.split()
            if any(word in class_words for word in key_words):
                return color
            if any(class_word in key_words for class_word in class_words):
                return color

        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{class_name}_{is_dynamic}"
        if cache_key in self.color_cache:
            return self.color_cache[cache_key]

        # ç”Ÿæˆæ–°é¢œè‰²
        try:
            if is_dynamic:
                hue_base = 0  # çº¢è‰²åŸºç¡€
                hue_range = 60  # åˆ°é»„è‰²çš„èŒƒå›´
            else:
                hue_base = 180  # é’è‰²åŸºç¡€
                hue_range = 120  # åˆ°ç»¿è‰²çš„èŒƒå›´

            hue = (hue_base + (self.class_count * 137.5) % hue_range) % 360
            saturation = 0.6 + (self.class_count % 4) * 0.1
            value = 0.7 + (self.class_count % 3) * 0.15

            rgb = colorsys.hsv_to_rgb(hue / 360, saturation, value)
            bgr = [int(rgb[2] * 255), int(rgb[1] * 255), int(rgb[0] * 255)]

            self.color_cache[cache_key] = bgr
            self.class_count += 1

            return bgr

        except Exception as e:
            print(f"âš ï¸  ç”Ÿæˆé¢œè‰²æ—¶å‡ºé”™: {e}ï¼Œä½¿ç”¨é»˜è®¤é¢œè‰²")
            return [0, 0, 255] if is_dynamic else [0, 255, 0]

    def _convert_box_coordinates(self, box, w, h):
        """ğŸ”§ æ ¸å¿ƒä¿®å¤ï¼šæ™ºèƒ½è½¬æ¢è¾¹ç•Œæ¡†åæ ‡æ ¼å¼"""
        try:
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            if hasattr(box, 'cpu'):
                box_np = box.cpu().numpy()
            elif hasattr(box, 'numpy'):
                box_np = box.numpy()
            else:
                box_np = np.array(box)

            print(f"ğŸ” åŸå§‹boxåæ ‡: {box_np}, å›¾åƒå°ºå¯¸: {w}x{h}")

            # æ£€æŸ¥åæ ‡æ ¼å¼å’ŒèŒƒå›´
            if len(box_np) != 4:
                raise ValueError(f"æ— æ•ˆçš„boxæ ¼å¼ï¼Œåº”è¯¥æœ‰4ä¸ªå€¼ï¼Œå®é™…: {len(box_np)}")

            # åˆ¤æ–­åæ ‡æ ¼å¼
            if np.all(box_np <= 1.0) and np.all(box_np >= 0.0):
                # å½’ä¸€åŒ–åæ ‡ (0-1èŒƒå›´)
                print(f"  ğŸ“Œ æ£€æµ‹åˆ°å½’ä¸€åŒ–åæ ‡")

                # æ£€æŸ¥æ˜¯å¦æ˜¯ cxcywh æ ¼å¼ (center_x, center_y, width, height)
                cx, cy, bw, bh = box_np

                # è½¬æ¢ä¸ºåƒç´ åæ ‡çš„ xyxy æ ¼å¼
                x1 = int((cx - bw / 2) * w)
                y1 = int((cy - bh / 2) * h)
                x2 = int((cx + bw / 2) * w)
                y2 = int((cy + bh / 2) * h)

                print(f"  ğŸ“Œ cxcywh -> xyxy: ({cx:.3f},{cy:.3f},{bw:.3f},{bh:.3f}) -> ({x1},{y1},{x2},{y2})")

            else:
                # åƒç´ åæ ‡
                # print(f"  ğŸ“Œ æ£€æµ‹åˆ°åƒç´ åæ ‡")

                # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯ xyxy æ ¼å¼
                if (box_np[2] > box_np[0] and box_np[3] > box_np[1] and
                        box_np[0] >= 0 and box_np[1] >= 0):
                    # å·²ç»æ˜¯ xyxy æ ¼å¼
                    x1, y1, x2, y2 = box_np.astype(int)
                    # print(f"  ğŸ“Œ å·²æ˜¯xyxyæ ¼å¼: ({x1},{y1},{x2},{y2})")
                else:
                    # å¯èƒ½æ˜¯ cxcywh åƒç´ æ ¼å¼
                    cx, cy, bw, bh = box_np
                    x1 = int(cx - bw / 2)
                    y1 = int(cy - bh / 2)
                    x2 = int(cx + bw / 2)
                    y2 = int(cy + bh / 2)
                    # print(f"  ğŸ“Œ åƒç´ cxcywh -> xyxy: ({cx},{cy},{bw},{bh}) -> ({x1},{y1},{x2},{y2})")

            # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
            x1 = max(0, min(x1, w - 1))
            y1 = max(0, min(y1, h - 1))
            x2 = max(x1 + 1, min(x2, w))
            y2 = max(y1 + 1, min(y2, h))

            # print(f"  âœ… æœ€ç»ˆåæ ‡: ({x1},{y1},{x2},{y2})")

            return x1, y1, x2, y2

        except Exception as e:
            print(f"âŒ åæ ‡è½¬æ¢å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤çš„å°åŒºåŸŸ
            return 0, 0, min(50, w), min(50, h)

    def debug_detection_results(self, dynamic_boxes, dynamic_labels, static_boxes, static_labels, image_shape):
        """ğŸ” è°ƒè¯•æ£€æµ‹ç»“æœçš„åæ ‡ä¿¡æ¯"""
        h, w = image_shape[:2]
        print(f"\nğŸ” === è°ƒè¯•æ£€æµ‹ç»“æœ ===")
        print(f"å›¾åƒå°ºå¯¸: {w} x {h}")

        print(f"\nğŸ¯ åŠ¨æ€å¯¹è±¡ ({len(dynamic_boxes)} ä¸ª):")
        for i, (box, label) in enumerate(zip(dynamic_boxes[:3], dynamic_labels[:3])):  # åªæ˜¾ç¤ºå‰3ä¸ª
            try:
                x1, y1, x2, y2 = self._convert_box_coordinates(box, w, h)
                area = (x2 - x1) * (y2 - y1)
                print(f"  {i + 1}. {label}: ({x1},{y1},{x2},{y2}) é¢ç§¯={area}")
            except Exception as e:
                print(f"  {i + 1}. {label}: åæ ‡è½¬æ¢å¤±è´¥ - {e}")

        print(f"\nğŸ—ï¸ é™æ€å¯¹è±¡ ({len(static_boxes)} ä¸ª):")
        for i, (box, label) in enumerate(zip(static_boxes[:3], static_labels[:3])):  # åªæ˜¾ç¤ºå‰3ä¸ª
            try:
                x1, y1, x2, y2 = self._convert_box_coordinates(box, w, h)
                area = (x2 - x1) * (y2 - y1)
                print(f"  {i + 1}. {label}: ({x1},{y1},{x2},{y2}) é¢ç§¯={area}")
            except Exception as e:
                print(f"  {i + 1}. {label}: åæ ‡è½¬æ¢å¤±è´¥ - {e}")

        print(f"=========================\n")

    def create_combined_segmentation_mask(self, image_shape: Tuple[int, int],
                                          dynamic_boxes: np.ndarray, dynamic_labels: List[str],
                                          dynamic_scores: np.ndarray,
                                          static_boxes: np.ndarray, static_labels: List[str],
                                          static_scores: np.ndarray,
                                          dynamic_sam_masks: Optional[List[np.ndarray]] = None,
                                          static_sam_masks: Optional[List[np.ndarray]] = None) -> Tuple[
        np.ndarray, Dict, Dict, np.ndarray]:
        """åˆ›å»ºåŠ¨æ€å’Œé™æ€å¯¹è±¡çš„ç»„åˆå½©è‰²åˆ†å‰²mask"""
        h, w = image_shape
        colored_mask = np.zeros((h, w, 3), dtype=np.uint8)
        class_mask = np.zeros((h, w), dtype=np.int32)
        dynamic_color_map = {}
        static_color_map = {}

        class_id = 1

        # å¤„ç†åŠ¨æ€å¯¹è±¡
        try:
            if len(dynamic_boxes) > 0:
                print(f"ğŸ¯ å¤„ç† {len(dynamic_boxes)} ä¸ªåŠ¨æ€å¯¹è±¡")
                for i, (box, label, score) in enumerate(zip(dynamic_boxes, dynamic_labels, dynamic_scores)):
                    try:
                        if label is None:
                            label = f"dynamic_object_{i}"

                        color = self.get_color_for_class(label, is_dynamic=True)
                        dynamic_color_map[str(label)] = color

                        # æ£€æŸ¥SAM mask
                        use_sam_mask = False
                        if (dynamic_sam_masks is not None and
                                len(dynamic_sam_masks) > i and
                                dynamic_sam_masks[i] is not None):

                            try:
                                mask = dynamic_sam_masks[i]
                                if hasattr(mask, 'shape') and len(mask.shape) >= 2:
                                    if mask.shape[0] == h and mask.shape[1] == w:
                                        mask_bool = mask.astype(bool)
                                        colored_mask[mask_bool] = color
                                        class_mask[mask_bool] = class_id
                                        use_sam_mask = True
                                        print(f"  âœ… ä½¿ç”¨SAM mask for {label}")
                                    else:
                                        print(f"âš ï¸  åŠ¨æ€SAM mask {i} å°ºå¯¸ä¸åŒ¹é…: {mask.shape} vs ({h}, {w})")
                            except Exception as e:
                                print(f"âš ï¸  å¤„ç†åŠ¨æ€SAM mask {i} æ—¶å‡ºé”™: {e}")

                        # å¦‚æœSAM maskä¸å¯ç”¨ï¼Œä½¿ç”¨è¾¹ç•Œæ¡†
                        if not use_sam_mask:
                            try:
                                x1, y1, x2, y2 = self._convert_box_coordinates(box, w, h)

                                if x2 > x1 and y2 > y1:
                                    colored_mask[y1:y2, x1:x2] = color
                                    class_mask[y1:y2, x1:x2] = class_id
                                    print(f"  âœ… ä½¿ç”¨è¾¹ç•Œæ¡† for {label}: ({x1},{y1},{x2},{y2})")
                                else:
                                    print(f"  âŒ æ— æ•ˆè¾¹ç•Œæ¡† for {label}")

                            except Exception as e:
                                print(f"âŒ å¤„ç†åŠ¨æ€è¾¹ç•Œæ¡† {i} æ—¶å‡ºé”™: {e}")
                                continue

                        class_id += 1

                    except Exception as e:
                        print(f"âŒ å¤„ç†åŠ¨æ€å¯¹è±¡ {i} æ—¶å‡ºé”™: {e}")
                        continue
        except Exception as e:
            print(f"âŒ å¤„ç†åŠ¨æ€å¯¹è±¡åˆ—è¡¨æ—¶å‡ºé”™: {e}")

        # å¤„ç†é™æ€å¯¹è±¡
        try:
            if len(static_boxes) > 0:
                print(f"ğŸ—ï¸ å¤„ç† {len(static_boxes)} ä¸ªé™æ€å¯¹è±¡")
                for i, (box, label, score) in enumerate(zip(static_boxes, static_labels, static_scores)):
                    try:
                        if label is None:
                            label = f"static_object_{i}"

                        color = self.get_color_for_class(label, is_dynamic=False)
                        static_color_map[str(label)] = color

                        # æ£€æŸ¥SAM mask
                        use_sam_mask = False
                        if (static_sam_masks is not None and
                                len(static_sam_masks) > i and
                                static_sam_masks[i] is not None):

                            try:
                                mask = static_sam_masks[i]
                                if hasattr(mask, 'shape') and len(mask.shape) >= 2:
                                    if mask.shape[0] == h and mask.shape[1] == w:
                                        mask_bool = mask.astype(bool)
                                        colored_mask[mask_bool] = color
                                        class_mask[mask_bool] = class_id
                                        use_sam_mask = True
                                        print(f"  âœ… ä½¿ç”¨SAM mask for {label}")
                                    else:
                                        print(f"âš ï¸  é™æ€SAM mask {i} å°ºå¯¸ä¸åŒ¹é…: {mask.shape} vs ({h}, {w})")
                            except Exception as e:
                                print(f"âš ï¸  å¤„ç†é™æ€SAM mask {i} æ—¶å‡ºé”™: {e}")

                        # å¦‚æœSAM maskä¸å¯ç”¨ï¼Œä½¿ç”¨è¾¹ç•Œæ¡†
                        if not use_sam_mask:
                            try:
                                x1, y1, x2, y2 = self._convert_box_coordinates(box, w, h)

                                if x2 > x1 and y2 > y1:
                                    colored_mask[y1:y2, x1:x2] = color
                                    class_mask[y1:y2, x1:x2] = class_id
                                    print(f"  âœ… ä½¿ç”¨è¾¹ç•Œæ¡† for {label}: ({x1},{y1},{x2},{y2})")
                                else:
                                    print(f"  âŒ æ— æ•ˆè¾¹ç•Œæ¡† for {label}")

                            except Exception as e:
                                print(f"âŒ å¤„ç†é™æ€è¾¹ç•Œæ¡† {i} æ—¶å‡ºé”™: {e}")
                                continue

                        class_id += 1

                    except Exception as e:
                        print(f"âŒ å¤„ç†é™æ€å¯¹è±¡ {i} æ—¶å‡ºé”™: {e}")
                        continue
        except Exception as e:
            print(f"âŒ å¤„ç†é™æ€å¯¹è±¡åˆ—è¡¨æ—¶å‡ºé”™: {e}")

        return colored_mask, dynamic_color_map, static_color_map, class_mask

    def create_overlay_visualization(self, original_image: np.ndarray,
                                     colored_mask: np.ndarray,
                                     alpha: float = 0.6) -> np.ndarray:
        """åˆ›å»ºå åŠ å¯è§†åŒ–"""
        try:
            if len(original_image.shape) == 3:
                original_bgr = cv2.cvtColor(original_image, cv2.COLOR_RGB2BGR)
            else:
                original_bgr = original_image

            mask_area = (colored_mask.sum(axis=2) > 0)
            overlay = original_bgr.copy()

            if np.any(mask_area):
                overlay[mask_area] = cv2.addWeighted(
                    original_bgr[mask_area], 1 - alpha,
                    colored_mask[mask_area], alpha, 0
                )

            return overlay
        except Exception as e:
            print(f"âŒ åˆ›å»ºå åŠ å¯è§†åŒ–æ—¶å‡ºé”™: {e}")
            return original_image if original_image is not None else np.zeros((480, 640, 3), dtype=np.uint8)

    def add_combined_legend(self, image: np.ndarray,
                            dynamic_color_map: Dict[str, List[int]],
                            static_color_map: Dict[str, List[int]],
                            dynamic_scores: Optional[np.ndarray] = None,
                            dynamic_labels: Optional[List[str]] = None,
                            static_scores: Optional[np.ndarray] = None,
                            static_labels: Optional[List[str]] = None) -> np.ndarray:
        """åœ¨å›¾åƒä¸Šæ·»åŠ åŠ¨æ€å’Œé™æ€å¯¹è±¡çš„ç»„åˆå›¾ä¾‹"""
        try:
            legend_image = image.copy()
            h, w = image.shape[:2]

            total_classes = len(dynamic_color_map) + len(static_color_map)
            if total_classes == 0:
                return legend_image

            legend_width = min(350, w // 3)
            legend_height = min(h - 20, total_classes * 25 + 100)
            legend_x = max(0, w - legend_width - 10)
            legend_y = 10

            # ç»˜åˆ¶å›¾ä¾‹èƒŒæ™¯
            cv2.rectangle(legend_image,
                          (legend_x, legend_y),
                          (legend_x + legend_width, legend_y + legend_height),
                          (255, 255, 255), -1)
            cv2.rectangle(legend_image,
                          (legend_x, legend_y),
                          (legend_x + legend_width, legend_y + legend_height),
                          (0, 0, 0), 2)

            y_offset = legend_y + 20

            # åŠ¨æ€å¯¹è±¡æ ‡é¢˜
            if dynamic_color_map:
                cv2.putText(legend_image, "Dynamic Objects",
                            (legend_x + 10, y_offset),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)
                y_offset += 25

                # ç»˜åˆ¶åŠ¨æ€å¯¹è±¡
                for class_name, color in dynamic_color_map.items():
                    try:
                        if y_offset > legend_y + legend_height - 30:
                            break

                        cv2.rectangle(legend_image,
                                      (legend_x + 10, y_offset - 8),
                                      (legend_x + 25, y_offset + 8),
                                      color, -1)

                        display_name = str(class_name)
                        if len(display_name) > 25:
                            display_name = display_name[:22] + "..."

                        text = display_name
                        if dynamic_scores is not None and dynamic_labels is not None:
                            try:
                                for j, label in enumerate(dynamic_labels):
                                    if str(label).lower() == str(class_name).lower():
                                        text += f" ({dynamic_scores[j]:.2f})"
                                        break
                            except Exception as e:
                                print(f"âš ï¸  å¤„ç†åŠ¨æ€å¯¹è±¡ç½®ä¿¡åº¦æ—¶å‡ºé”™: {e}")

                        cv2.putText(legend_image, text,
                                    (legend_x + 30, y_offset + 5),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 0), 1)
                        y_offset += 22
                    except Exception as e:
                        print(f"âŒ ç»˜åˆ¶åŠ¨æ€å›¾ä¾‹é¡¹æ—¶å‡ºé”™: {e}")
                        continue

            # é™æ€å¯¹è±¡æ ‡é¢˜
            if static_color_map:
                y_offset += 5
                if y_offset < legend_y + legend_height - 50:
                    cv2.putText(legend_image, "Static Objects",
                                (legend_x + 10, y_offset),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 100, 0), 2)
                    y_offset += 25

                    # ç»˜åˆ¶é™æ€å¯¹è±¡
                    for class_name, color in static_color_map.items():
                        try:
                            if y_offset > legend_y + legend_height - 30:
                                break

                            cv2.rectangle(legend_image,
                                          (legend_x + 10, y_offset - 8),
                                          (legend_x + 25, y_offset + 8),
                                          color, -1)

                            display_name = str(class_name)
                            if len(display_name) > 25:
                                display_name = display_name[:22] + "..."

                            text = display_name
                            if static_scores is not None and static_labels is not None:
                                try:
                                    for j, label in enumerate(static_labels):
                                        if str(label).lower() == str(class_name).lower():
                                            text += f" ({static_scores[j]:.2f})"
                                            break
                                except Exception as e:
                                    print(f"âš ï¸  å¤„ç†é™æ€å¯¹è±¡ç½®ä¿¡åº¦æ—¶å‡ºé”™: {e}")

                            cv2.putText(legend_image, text,
                                        (legend_x + 30, y_offset + 5),
                                        cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 0), 1)
                            y_offset += 22
                        except Exception as e:
                            print(f"âŒ ç»˜åˆ¶é™æ€å›¾ä¾‹é¡¹æ—¶å‡ºé”™: {e}")
                            continue

            return legend_image

        except Exception as e:
            print(f"âŒ æ·»åŠ å›¾ä¾‹æ—¶å‡ºé”™: {e}")
            return image if image is not None else np.zeros((480, 640, 3), dtype=np.uint8)

    def create_separate_visualizations(self, image_shape: Tuple[int, int],
                                       dynamic_boxes: np.ndarray, dynamic_labels: List[str],
                                       dynamic_scores: np.ndarray,
                                       static_boxes: np.ndarray, static_labels: List[str],
                                       static_scores: np.ndarray,
                                       dynamic_sam_masks: Optional[List[np.ndarray]] = None,
                                       static_sam_masks: Optional[List[np.ndarray]] = None) -> Tuple[
        np.ndarray, np.ndarray]:
        """åˆ›å»ºåˆ†ç¦»çš„åŠ¨æ€å’Œé™æ€å¯¹è±¡å¯è§†åŒ–"""
        try:
            h, w = image_shape

            # åŠ¨æ€å¯¹è±¡å¯è§†åŒ–
            dynamic_mask = np.zeros((h, w, 3), dtype=np.uint8)
            try:
                if len(dynamic_boxes) > 0:
                    for i, (box, label, score) in enumerate(zip(dynamic_boxes, dynamic_labels, dynamic_scores)):
                        try:
                            if label is None:
                                label = f"dynamic_{i}"

                            color = self.get_color_for_class(label, is_dynamic=True)

                            # å¤„ç†SAM mask
                            if (dynamic_sam_masks is not None and
                                    len(dynamic_sam_masks) > i and
                                    dynamic_sam_masks[i] is not None):
                                try:
                                    mask = dynamic_sam_masks[i]
                                    if hasattr(mask, 'shape') and mask.shape[0] == h and mask.shape[1] == w:
                                        mask_bool = mask.astype(bool)
                                        dynamic_mask[mask_bool] = color
                                    else:
                                        self._draw_box_mask(dynamic_mask, box, color, w, h)
                                except Exception as e:
                                    print(f"âš ï¸  å¤„ç†åŠ¨æ€SAM mask {i}: {e}")
                                    self._draw_box_mask(dynamic_mask, box, color, w, h)
                            else:
                                self._draw_box_mask(dynamic_mask, box, color, w, h)
                        except Exception as e:
                            print(f"âŒ å¤„ç†åŠ¨æ€å¯¹è±¡ {i}: {e}")
                            continue
            except Exception as e:
                print(f"âŒ å¤„ç†åŠ¨æ€å¯¹è±¡åˆ—è¡¨: {e}")

            # é™æ€å¯¹è±¡å¯è§†åŒ–
            static_mask = np.zeros((h, w, 3), dtype=np.uint8)
            try:
                if len(static_boxes) > 0:
                    for i, (box, label, score) in enumerate(zip(static_boxes, static_labels, static_scores)):
                        try:
                            if label is None:
                                label = f"static_{i}"

                            color = self.get_color_for_class(label, is_dynamic=False)

                            # å¤„ç†SAM mask
                            if (static_sam_masks is not None and
                                    len(static_sam_masks) > i and
                                    static_sam_masks[i] is not None):
                                try:
                                    mask = static_sam_masks[i]
                                    if hasattr(mask, 'shape') and mask.shape[0] == h and mask.shape[1] == w:
                                        mask_bool = mask.astype(bool)
                                        static_mask[mask_bool] = color
                                    else:
                                        self._draw_box_mask(static_mask, box, color, w, h)
                                except Exception as e:
                                    print(f"âš ï¸  å¤„ç†é™æ€SAM mask {i}: {e}")
                                    self._draw_box_mask(static_mask, box, color, w, h)
                            else:
                                self._draw_box_mask(static_mask, box, color, w, h)
                        except Exception as e:
                            print(f"âŒ å¤„ç†é™æ€å¯¹è±¡ {i}: {e}")
                            continue
            except Exception as e:
                print(f"âŒ å¤„ç†é™æ€å¯¹è±¡åˆ—è¡¨: {e}")

            return dynamic_mask, static_mask

        except Exception as e:
            print(f"âŒ åˆ›å»ºåˆ†ç¦»å¯è§†åŒ–æ—¶å‡ºé”™: {e}")
            h, w = image_shape
            return np.zeros((h, w, 3), dtype=np.uint8), np.zeros((h, w, 3), dtype=np.uint8)

    def _draw_box_mask(self, mask, box, color, w, h):
        """å®‰å…¨åœ°ç»˜åˆ¶è¾¹ç•Œæ¡†mask"""
        try:
            x1, y1, x2, y2 = self._convert_box_coordinates(box, w, h)

            if x2 > x1 and y2 > y1:
                mask[y1:y2, x1:x2] = color
            else:
                print(f"âŒ æ— æ•ˆçš„è¾¹ç•Œæ¡†å°ºå¯¸: ({x1},{y1},{x2},{y2})")

        except Exception as e:
            print(f"âŒ ç»˜åˆ¶è¾¹ç•Œæ¡†æ—¶å‡ºé”™: {e}")


class ScenePromptManager:
    """åœºæ™¯æ£€æµ‹å’ŒPromptç®¡ç†å™¨"""

    def __init__(self, default_scene="outdoor_street"):
        self.current_scene = default_scene
        self.scene_prompts = {

            "outdoor_street": {
                "dynamic_objects": [
                    "car", "cars", "vehicle", "vehicles", "truck", "trucks",
                    "bus", "buses", "motorcycle", "motorcycles",
                    "bicycle", "bicycles", "person", "people", "pedestrian",
                    "pedestrians"
                ],
                "static_objects": [
                    "road", "sidewalk", "building", "traffic_sign", "traffic_light",
                    "pole", "tree", "street_furniture", "barrier", "guardrail",
                    "traffic light", "stop sign", "street sign", "road sign",
                    "lamp post", "street lamp", "pole", "traffic cone",
                    "fire hydrant", "mailbox", "bench", "tree", "curb",
                    "bollard"
                ],
                "confidence_threshold": 0.20,
                "description": "Complete urban street scene with all dynamic and static elements"
            },

            "parking_lot": {
                "dynamic_objects": [
                    "car", "cars", "parked car", "moving car",
                    "truck", "trucks", "van", "vans",
                    "suv", "sedan", "hatchback",
                    "person", "people", "pedestrian", "walking person",
                    "shopping cart", "trolley",
                    "motorcycle", "bike"
                ],
                "static_objects": [
                    "parking sign", "parking meter", "light pole",
                    "building", "wall", "fence", "barrier",
                    "parking line", "curb", "pavement"
                ],
                "confidence_threshold": 0.2,
                "description": "Parking lot with stationary and moving vehicles"
            },

            "highway": {
                "dynamic_objects": [
                    "car", "cars", "vehicle", "vehicles",
                    "truck", "trucks", "semi truck", "trailer",
                    "bus", "coach", "van", "suv",
                    "motorcycle", "motorbike"
                ],
                "static_objects": [
                    "road", "highway", "guardrail", "barrier",
                    "highway sign", "road sign", "light pole",
                    "bridge", "overpass"
                ],
                "confidence_threshold": 0.25,
                "description": "Highway scene with fast-moving vehicles"
            },

            "residential": {
                "dynamic_objects": [
                    "car", "cars", "parked car",
                    "person", "people", "child", "children", "adult",
                    "bicycle", "bike", "scooter", "skateboard",
                    "dog", "cat", "pet", "animal",
                    "stroller", "wheelchair"
                ],
                "static_objects": [
                    "house", "building", "fence", "gate",
                    "mailbox", "tree", "garden", "lawn",
                    "sidewalk", "driveway", "street lamp"
                ],
                "confidence_threshold": 0.18,
                "description": "Residential area with people and pets"
            },

            "indoor": {
                "dynamic_objects": [
                    "person", "people", "human", "visitor",
                    "chair", "rolling chair", "office chair",
                    "robot", "cleaning robot", "vacuum robot",
                    "cart", "trolley", "wheelchair",
                    "door", "opening door", "moving door"
                ],
                "static_objects": [
                    "wall", "desk", "table", "cabinet",
                    "window", "door", "ceiling", "floor",
                    "computer", "monitor", "lamp"
                ],
                "confidence_threshold": 0.3,
                "description": "Indoor environment with people and movable objects"
            },

            "construction": {
                "dynamic_objects": [
                    "construction vehicle", "excavator", "bulldozer",
                    "dump truck", "crane", "forklift",
                    "worker", "construction worker", "person",
                    "vehicle", "truck", "van"
                ],
                "static_objects": [
                    "building", "construction site", "scaffold",
                    "barrier", "fence", "sign", "pole",
                    "container", "material pile"
                ],
                "confidence_threshold": 0.2,
                "description": "Construction site with heavy machinery"
            },

            "campus": {
                "dynamic_objects": [
                    "person", "people", "student", "students",
                    "bicycle", "bike", "scooter", "skateboard",
                    "car", "vehicle", "bus", "shuttle bus",
                    "delivery robot", "robot", "cart"
                ],
                "static_objects": [
                    "building", "library", "classroom", "tree",
                    "bench", "sign", "lamp post", "statue",
                    "fountain", "walkway", "lawn"
                ],
                "confidence_threshold": 0.2,
                "description": "University campus with students and vehicles"
            }
        }

        # åœºæ™¯è‡ªåŠ¨æ£€æµ‹å…³é”®è¯
        self.scene_keywords = {
            "highway": ["highway", "freeway", "motorway", "interstate"],
            "parking_lot": ["parking", "garage", "lot"],
            "residential": ["residential", "neighborhood", "suburb"],
            "indoor": ["indoor", "inside", "interior", "office", "building"],
            "construction": ["construction", "building", "work", "site"],
            "campus": ["campus", "university", "college", "school"]
        }

    def detect_scene_from_config(self, config_scene_hint=None):
        """ä»é…ç½®æˆ–å…¶ä»–ä¿¡æ¯æ£€æµ‹åœºæ™¯ç±»å‹"""
        if config_scene_hint and config_scene_hint in self.scene_prompts:
            self.current_scene = config_scene_hint
            return self.current_scene

        # å¯ä»¥æ‰©å±•ä¸ºåŸºäºå›¾åƒå†…å®¹çš„åœºæ™¯æ£€æµ‹
        return self.current_scene

    def detect_scene_from_path(self, data_path):
        """ä»æ•°æ®è·¯å¾„æ£€æµ‹åœºæ™¯ç±»å‹"""
        path_lower = data_path.lower()

        for scene_type, keywords in self.scene_keywords.items():
            if any(keyword in path_lower for keyword in keywords):
                self.current_scene = scene_type
                print(f"ğŸ¯ Auto-detected scene type: {scene_type} from path: {data_path}")
                return scene_type

        print(f"ğŸ” Using default scene type: {self.current_scene}")
        return self.current_scene

    def get_current_prompt(self):
        """è·å–å½“å‰åœºæ™¯çš„åŠ¨æ€å¯¹è±¡prompt"""
        scene_info = self.scene_prompts[self.current_scene]
        prompt = ". ".join(scene_info["dynamic_objects"])
        return prompt, scene_info["confidence_threshold"]

    def get_static_prompt(self):
        """è·å–å½“å‰åœºæ™¯çš„é™æ€å¯¹è±¡prompt"""
        scene_info = self.scene_prompts[self.current_scene]
        if "static_objects" in scene_info:
            prompt = ". ".join(scene_info["static_objects"])
            return prompt, scene_info["confidence_threshold"]
        return "", scene_info["confidence_threshold"]

    def get_combined_prompt(self):
        """è·å–åŠ¨æ€å’Œé™æ€å¯¹è±¡çš„ç»„åˆprompt"""
        scene_info = self.scene_prompts[self.current_scene]
        dynamic_prompt = ". ".join(scene_info["dynamic_objects"])
        static_prompt = ". ".join(scene_info.get("static_objects", []))
        combined_prompt = dynamic_prompt
        if static_prompt:
            combined_prompt += ". " + static_prompt
        return combined_prompt, scene_info["confidence_threshold"]

    def get_detailed_prompt(self):
        """è·å–è¯¦ç»†çš„promptä¿¡æ¯"""
        scene_info = self.scene_prompts[self.current_scene]
        return {
            "dynamic_prompt": ". ".join(scene_info["dynamic_objects"]),
            "static_prompt": ". ".join(scene_info.get("static_objects", [])),
            "confidence_threshold": scene_info["confidence_threshold"],
            "description": scene_info["description"],
            "dynamic_classes": scene_info["dynamic_objects"],
            "static_classes": scene_info.get("static_objects", [])
        }

    def set_scene(self, scene_type):
        """æ‰‹åŠ¨è®¾ç½®åœºæ™¯ç±»å‹"""
        if scene_type in self.scene_prompts:
            self.current_scene = scene_type
            print(f"ğŸ¬ Scene type set to: {scene_type}")
        else:
            available_scenes = list(self.scene_prompts.keys())
            print(f"âŒ Unknown scene type: {scene_type}. Available: {available_scenes}")

    def add_custom_scene(self, scene_name, dynamic_objects, static_objects=None, confidence_threshold=0.2,
                         description=""):
        """æ·»åŠ è‡ªå®šä¹‰åœºæ™¯é…ç½®"""
        self.scene_prompts[scene_name] = {
            "dynamic_objects": dynamic_objects,
            "static_objects": static_objects or [],
            "confidence_threshold": confidence_threshold,
            "description": description
        }
        print(f"âœ… Added custom scene: {scene_name}")


class GroundingDINODetector:
    """Grounding DINO æ£€æµ‹å™¨å°è£…ï¼Œæ”¯æŒæœ¬åœ°.pthæ–‡ä»¶"""

    def __init__(self, model_path=None, config_path=None, device="cuda"):
        self.device = device
        self.model = None
        self.processor = None
        self.use_original = False
        self.use_transformers = False
        self.use_yolo = True

        # å¦‚æœæä¾›äº†æœ¬åœ°.pthæ–‡ä»¶è·¯å¾„
        if model_path and model_path.endswith('.pth') and os.path.exists(model_path):
            self._load_original_grounding_dino(model_path, config_path)
        else:
            self._load_transformers_model(model_path)

    def _load_original_grounding_dino(self, model_path, config_path=None):
        """åŠ è½½åŸå§‹çš„Grounding DINOæ¨¡å‹ï¼ˆ.pthæ–‡ä»¶ï¼‰"""
        if not GROUNDING_DINO_ORIGINAL:
            print("âŒ Original Grounding DINO package not installed")
            print("ğŸ’¡ Install with: pip install groundingdino")
            self._try_load_yolo()
            return

        try:
            print(f"ğŸ”„ Loading Grounding DINO from .pth file: {model_path}")

            # å¦‚æœæ²¡æœ‰æä¾›configè·¯å¾„ï¼Œå°è¯•æŸ¥æ‰¾
            if config_path is None:
                # å°è¯•å‡ ä¸ªå¸¸è§çš„é…ç½®æ–‡ä»¶ä½ç½®
                possible_configs = [
                    "/home/zwk/ä¸‹è½½/S3PO-GS-main/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py",
                ]

                for cfg in possible_configs:
                    if os.path.exists(cfg):
                        config_path = cfg
                        print(f"âœ… Found config file: {config_path}")
                        break

                if config_path is None:
                    print("âŒ Config file not found. Please provide config_path")
                    print("ğŸ’¡ Download from: https://github.com/IDEA-Research/GroundingDINO")
                    self._try_load_yolo()
                    return

            # åŠ è½½æ¨¡å‹
            self.model = load_model(config_path, model_path, device=self.device)
            self.use_original = True
            print("âœ… Successfully loaded Grounding DINO from .pth file")

        except Exception as e:
            print(f"âŒ Failed to load Grounding DINO .pth: {e}")
            self._try_load_yolo()

    def _load_transformers_model(self, model_id):
        """åŠ è½½Hugging Face transformersç‰ˆæœ¬çš„æ¨¡å‹"""
        if not GROUNDING_DINO_AVAILABLE:
            print("âŒ Transformers not available")
            self._try_load_yolo()
            return

        if model_id is None:
            model_id = "IDEA-Research/grounding-dino-tiny"

        try:
            print(f"ğŸ”„ Loading Grounding DINO from Hugging Face: {model_id}")
            self.processor = AutoProcessor.from_pretrained(model_id)
            self.model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(self.device)
            self.use_transformers = True
            print("âœ… Successfully loaded Grounding DINO from Hugging Face")
        except Exception as e:
            print(f"âŒ Failed to load from Hugging Face: {e}")
            self._try_load_yolo()

    def _try_load_yolo(self):
        """å°è¯•åŠ è½½YOLOä½œä¸ºå¤‡é€‰æ£€æµ‹å™¨"""
        try:
            import ultralytics
            from ultralytics import YOLO

            print("ğŸ”„ Trying to load YOLOv8 as fallback...")
            self.yolo_model = YOLO('/home/zwk/ä¸‹è½½/S3PO-GS-main/yolo11x.pt')
            self.use_yolo = True
            print("âœ… YOLOv8 loaded as fallback detector")
        except Exception as e:
            print(f"âŒ Failed to load YOLO: {e}")
            print("âš ï¸  No detector available! Detection will be disabled.")

    def detect(self, image, text_prompt, confidence_threshold=0.2):
        """ç»Ÿä¸€çš„æ£€æµ‹æ¥å£"""
        if self.use_original:
            return self._detect_original(image, text_prompt, confidence_threshold)
        elif self.use_transformers:
            return self._detect_transformers(image, text_prompt, confidence_threshold)
        elif self.use_yolo:
            return self._detect_yolo(image, text_prompt, confidence_threshold)
        else:
            return np.array([]), np.array([]), []

    def _detect_original(self, image, text_prompt, confidence_threshold):
        """ä½¿ç”¨åŸå§‹Grounding DINOè¿›è¡Œæ£€æµ‹"""
        try:
            # ç¡®ä¿è¾“å…¥æ ¼å¼æ­£ç¡®
            if isinstance(image, np.ndarray):
                if image.dtype != np.uint8:
                    image = (image * 255).astype(np.uint8)
                # åŸå§‹Grounding DINOéœ€è¦PIL Image
                image_pil = Image.fromarray(image)
            else:
                image_pil = image

            # å›¾åƒé¢„å¤„ç†
            transform = T.Compose([
                T.RandomResize([800], max_size=1333),
                T.ToTensor(),
                T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
            ])
            image_tensor, _ = transform(image_pil, None)

            # é¢„æµ‹
            boxes, logits, phrases = predict(
                model=self.model,
                image=image_tensor,
                caption=text_prompt,
                box_threshold=confidence_threshold,
                text_threshold=confidence_threshold,
                device=self.device
            )

            # è½¬æ¢è¾“å‡ºæ ¼å¼
            h, w = image.shape[:2] if isinstance(image, np.ndarray) else image_pil.size[::-1]
            boxes_scaled = boxes * torch.tensor([w, h, w, h], dtype=torch.float32)
            boxes_np = boxes_scaled.cpu().numpy()
            scores_np = logits.cpu().numpy()

            # è§£ææ ‡ç­¾
            labels = []
            for phrase in phrases:
                # phraseæ ¼å¼å¯èƒ½æ˜¯ "object(0.95)" è¿™æ ·çš„
                label = phrase.split('(')[0].strip()
                labels.append(label)

            print(f"ğŸ¯ Grounding DINO detected {len(boxes_np)} objects")

            return boxes_np, scores_np, labels

        except Exception as e:
            print(f"âŒ Original Grounding DINO detection failed: {e}")
            return np.array([]), np.array([]), []

    def _detect_transformers(self, image, text_prompt, confidence_threshold):
        """ä½¿ç”¨transformersç‰ˆæœ¬è¿›è¡Œæ£€æµ‹"""
        try:
            if isinstance(image, np.ndarray):
                if image.dtype != np.uint8:
                    image = (image * 255).astype(np.uint8)
                image_pil = Image.fromarray(image)
            else:
                image_pil = image

            inputs = self.processor(images=image_pil, text=text_prompt, return_tensors="pt").to(self.device)

            with torch.no_grad():
                outputs = self.model(**inputs)

            results = self.processor.post_process_grounded_object_detection(
                outputs,
                inputs.input_ids,
                box_threshold=confidence_threshold,
                text_threshold=confidence_threshold,
                target_sizes=[image_pil.size[::-1]]
            )[0]

            boxes = results["boxes"].cpu().numpy() if len(results["boxes"]) > 0 else np.array([])
            scores = results["scores"].cpu().numpy() if len(results["scores"]) > 0 else np.array([])
            labels = results["labels"] if "labels" in results else []

            print(f"ğŸ¯ Grounding DINO detected {len(boxes)} objects")
            return boxes, scores, labels

        except Exception as e:
            print(f"âŒ Transformers detection failed: {e}")
            return np.array([]), np.array([]), []

    def _detect_yolo(self, image, text_prompt, confidence_threshold):
        """ä½¿ç”¨YOLOè¿›è¡Œæ£€æµ‹"""
        if not hasattr(self, 'yolo_model'):
            return np.array([]), np.array([]), []

        try:
            # YOLOç±»åˆ«æ˜ å°„
            target_classes = {
                'person': 0, 'bicycle': 1, 'car': 2, 'motorcycle': 3,
                'bus': 5, 'truck': 7, 'traffic light': 9,
                'stop sign': 11, 'bench': 13, 'bird': 14,
                'cat': 15, 'dog': 16, 'horse': 17
            }

            # è§£æprompt
            prompt_words = text_prompt.lower().replace('.', '').split()
            relevant_classes = []
            for word in prompt_words:
                if word in target_classes:
                    relevant_classes.append(target_classes[word])
                elif word.rstrip('s') in target_classes:
                    relevant_classes.append(target_classes[word.rstrip('s')])

            # è¿è¡Œæ£€æµ‹
            results = self.yolo_model(image, conf=confidence_threshold)

            boxes = []
            scores = []
            labels = []

            for r in results:
                if r.boxes is not None:
                    for box in r.boxes:
                        class_id = int(box.cls)
                        if not relevant_classes or class_id in relevant_classes:
                            xyxy = box.xyxy[0].cpu().numpy()
                            boxes.append(xyxy)
                            scores.append(float(box.conf))
                            labels.append(self.yolo_model.names[class_id])

            boxes = np.array(boxes) if boxes else np.array([])
            scores = np.array(scores) if scores else np.array([])

            print(f"ğŸ¯ YOLO detected {len(boxes)} objects")
            return boxes, scores, labels

        except Exception as e:
            print(f"âŒ YOLO detection failed: {e}")
            return np.array([]), np.array([]), []


class EnhancedDynamicObjectMasker:
    """å¢å¼ºçš„åŠ¨æ€ç‰©ä½“é®ç½©å™¨ï¼Œæ”¯æŒåŠ¨æ€å’Œé™æ€å¯¹è±¡çš„å®Œæ•´å¯è§†åŒ–"""

    def __init__(self, device="cuda", use_sam=True,
                 sam_checkpoint="/home/zwk/ä¸‹è½½/S3PO-GS-main/sam_vit_h_4b8939.pth",
                 save_dir=None, save_images=True, use_ground_segmentation=True,
                 scene_type="outdoor_street",
                 grounding_dino_model="/home/zwk/ä¸‹è½½/S3PO-GS-main/groundingdino_swint_ogc.pth",
                 grounding_dino_config=None,
                 enable_colorful_vis=True,
                 detect_static_objects=True):
        """
        åˆå§‹åŒ–å¢å¼ºçš„åŠ¨æ€ç‰©ä½“é®ç½©å™¨

        Args:
            detect_static_objects: æ˜¯å¦æ£€æµ‹å’Œå¯è§†åŒ–é™æ€å¯¹è±¡
            enable_colorful_vis: æ˜¯å¦å¯ç”¨å½©è‰²åˆ†å‰²å¯è§†åŒ–
        """
        self.device = device
        self.initialization_success = True
        self.enable_colorful_vis = enable_colorful_vis
        self.detect_static_objects = detect_static_objects

        # åœºæ™¯å’ŒPromptç®¡ç†
        try:
            self.prompt_manager = ScenePromptManager(default_scene=scene_type)
            print(f"âœ… Scene prompt manager initialized")
        except Exception as e:
            print(f"âŒ Failed to initialize scene manager: {e}")
            self.initialization_success = False

        # Grounding DINOæ£€æµ‹å™¨ - æ”¯æŒ.pthæ–‡ä»¶
        print(f"ğŸ”„ Initializing Grounding DINO detector...")
        try:
            # å¦‚æœæ˜¯.pthæ–‡ä»¶ï¼Œä½¿ç”¨åŸå§‹åŠ è½½æ–¹å¼
            if False:
                self.grounding_detector = GroundingDINODetector(
                    model_path='/home/zwk/ä¸‹è½½/S3PO-GS-main/groundingdino_swint_ogc.pth',
                    config_path='/home/zwk/ä¸‹è½½/S3PO-GS-main/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py',
                    device=device
                )
            else:
                # å¦åˆ™ä½¿ç”¨Hugging Faceæ¨¡å‹
                self.grounding_detector = GroundingDINODetector(
                    model_path=grounding_dino_model,
                    device=device
                )

            print(f"âœ… Grounding DINO detector initialized")
        except Exception as e:
            print(f"âŒ Failed to initialize Grounding DINO: {e}")
            self.grounding_detector = None
            self.initialization_success = False

        # SAMåˆ†å‰²å™¨
        self.use_sam = use_sam
        if use_sam:
            try:
                if os.path.exists(sam_checkpoint):
                    sam = sam_model_registry["vit_h"](checkpoint=sam_checkpoint)
                    sam.to(device=device)
                    self.sam_predictor = SamPredictor(sam)
                    print("âœ… SAM model loaded successfully")
                else:
                    print(f"âš ï¸  SAM checkpoint not found at {sam_checkpoint}")
                    print("âš ï¸  SAM will be disabled")
                    self.use_sam = False
            except Exception as e:
                print(f"âš ï¸  Warning: SAM model failed to load ({e})")
                print("âš ï¸  Will use Grounding DINO boxes only")
                self.use_sam = False

        # ğŸ¨ å½©è‰²å¯è§†åŒ–å™¨
        if self.enable_colorful_vis:
            try:
                self.colorful_visualizer = ColorfulSegmentationVisualizer()
                print("âœ… Colorful segmentation visualizer initialized")
            except Exception as e:
                print(f"âš ï¸  Colorful visualizer failed to initialize: {e}")
                self.enable_colorful_vis = False

        # åœ°é¢åˆ†å‰²åŠŸèƒ½
        self.use_ground_segmentation = use_ground_segmentation
        if use_ground_segmentation:
            try:
                self._init_ground_segmentation()
            except Exception as e:
                print(f"âš ï¸  Ground segmentation initialization failed: {e}")
                self.use_ground_segmentation = False

        # è¿åŠ¨æ£€æµ‹å‚æ•°
        self.prev_frame = None
        self.prev_mask = None
        self.motion_threshold = 3.0

        # æ—¶é—´ä¸€è‡´æ€§å‚æ•°
        self.mask_history = []
        self.history_length = 5

        # åœ°é¢ä¿®å¤å‚æ•°
        self.inpaint_radius = 3
        self.ground_dilation_kernel = np.ones((7, 7), np.uint8)

        # å›¾åƒä¿å­˜è®¾ç½®
        self.save_images = save_images
        self.save_dir = save_dir if save_dir else "./masked_images"
        if self.save_images:
            try:
                self._create_save_directories()
            except Exception as e:
                print(f"âš ï¸  Failed to create save directories: {e}")
                self.save_images = False

        # æ‰“å°é…ç½®ä¿¡æ¯
        self._print_configuration(grounding_dino_model)
        print(f"ğŸ¯ Mode: Dynamic objects will be MASKED OUT (not reconstructed)")
        print(f"âœ… Mode: Static objects will be PRESERVED (reconstructed)")
        if self.detect_static_objects:
            print(f"ğŸ¨ Mode: Static objects will also be VISUALIZED")

    def _create_save_directories(self):
        """åˆ›å»ºä¿å­˜å›¾åƒçš„ç›®å½•ç»“æ„ï¼ŒåŒ…æ‹¬åŠ¨æ€å’Œé™æ€å¯¹è±¡çš„å¯è§†åŒ–ç›®å½•"""
        directories = [
            self.save_dir,
            os.path.join(self.save_dir, "original"),
            os.path.join(self.save_dir, "grounding_dino_detections"),
            os.path.join(self.save_dir, "grounding_dino_masks"),
            os.path.join(self.save_dir, "sam_masks"),
            os.path.join(self.save_dir, "motion_masks"),
            os.path.join(self.save_dir, "ground_masks"),
            os.path.join(self.save_dir, "shadow_regions"),
            os.path.join(self.save_dir, "inpainted_ground"),
            os.path.join(self.save_dir, "final_masks"),
            os.path.join(self.save_dir, "masked_overlay"),
            os.path.join(self.save_dir, "static_only"),
            os.path.join(self.save_dir, "repaired_images"),
            os.path.join(self.save_dir, "keyframes"),
            os.path.join(self.save_dir, "detection_analysis"),

            # ğŸ¨ å½©è‰²å¯è§†åŒ–ç›®å½•
            os.path.join(self.save_dir, "colorful_combined"),  # åŠ¨æ€+é™æ€ç»„åˆ
            os.path.join(self.save_dir, "colorful_dynamic_only"),  # ä»…åŠ¨æ€å¯¹è±¡
            os.path.join(self.save_dir, "colorful_static_only"),  # ä»…é™æ€å¯¹è±¡
            os.path.join(self.save_dir, "colorful_overlay"),  # å åŠ æ˜¾ç¤º
            os.path.join(self.save_dir, "colorful_legend"),  # å¸¦å›¾ä¾‹
            os.path.join(self.save_dir, "colorful_analysis"),  # åˆ†æç»“æœ
        ]

        for directory in directories:
            os.makedirs(directory, exist_ok=True)

        print(f"ğŸ“ Created all directories including colorful visualization in: {self.save_dir}")

    def _print_configuration(self, grounding_dino_model):
        """æ‰“å°é…ç½®ä¿¡æ¯"""
        try:
            if self.prompt_manager:
                prompt_info = self.prompt_manager.get_detailed_prompt()
                print(f"ğŸ¯ Enhanced Dynamic Object Masker Configuration:")
                print(f"  - Scene type: {self.prompt_manager.current_scene}")
                print(f"  - Scene description: {prompt_info['description']}")
                print(f"  - Dynamic objects: {', '.join(prompt_info['dynamic_classes'][:5])}...")
                if self.detect_static_objects:
                    print(f"  - Static objects: {', '.join(prompt_info['static_classes'][:5])}...")
                print(f"  - Confidence threshold: {prompt_info['confidence_threshold']}")
            else:
                print(f"ğŸ¯ Enhanced Dynamic Object Masker Configuration:")
                print(f"  - Scene manager: FAILED")

            print(f"  - Grounding DINO model: {grounding_dino_model}")
            print(f"  - Grounding DINO status: {'âœ… OK' if self.grounding_detector else 'âš ï¸  FAILED'}")
            print(f"  - SAM enabled: {self.use_sam}")
            print(f"  - Ground segmentation: {self.use_ground_segmentation}")
            print(f"  - Colorful visualization: {self.enable_colorful_vis}")
            print(f"  - Detect static objects: {self.detect_static_objects}")
            print(f"  - Save images: {self.save_images}")

            if not self.initialization_success:
                print(f"âš ï¸  WARNING: Some components failed to initialize!")
                print(f"  - The system will continue with reduced functionality")
                print(f"  - Consider checking dependencies and model files")

        except Exception as e:
            print(f"âŒ Failed to print configuration: {e}")

    def _init_ground_segmentation(self):
        """åˆå§‹åŒ–åœ°é¢åˆ†å‰²æ¨¡å‹"""
        try:
            self.ground_segmentation_method = "traditional"
            print("âœ… Ground segmentation initialized with traditional method")
        except Exception as e:
            print(f"Warning: Ground segmentation failed: {e}")
            self.ground_segmentation_method = "traditional"

    def set_scene_from_config(self, config):
        """ä»é…ç½®ä¸­è®¾ç½®åœºæ™¯ç±»å‹"""
        scene_hint = config.get("dynamic_filtering", {}).get("scene_type", None)
        data_path = config.get("Dataset", {}).get("dataset_path", "")

        # ä¼˜å…ˆä½¿ç”¨é…ç½®ä¸­çš„åœºæ™¯ç±»å‹
        if scene_hint:
            self.prompt_manager.set_scene(scene_hint)
        # å…¶æ¬¡å°è¯•ä»æ•°æ®è·¯å¾„æ¨æ–­
        elif data_path:
            self.prompt_manager.detect_scene_from_path(data_path)

        # æ›´æ–°æ£€æµ‹é˜ˆå€¼
        prompt_info = self.prompt_manager.get_detailed_prompt()
        print(f"ğŸ¬ Scene configuration updated:")
        print(f"  - Active scene: {self.prompt_manager.current_scene}")
        print(f"  - Confidence threshold: {prompt_info['confidence_threshold']}")

    def segment_ground(self, image):
        """åˆ†å‰²å›¾åƒä¸­çš„åœ°é¢åŒºåŸŸ"""
        if self.ground_segmentation_method == "traditional":
            return self._traditional_ground_segmentation(image)
        else:
            return self._ml_ground_segmentation(image)

    def _traditional_ground_segmentation(self, image):
        """åŸºäºä¼ ç»Ÿæ–¹æ³•çš„åœ°é¢åˆ†å‰²"""
        h, w = image.shape[:2]
        ground_mask = np.zeros((h, w), dtype=np.uint8)

        # è½¬æ¢åˆ°HSVç©ºé—´è¿›è¡Œé¢œè‰²åˆ†æ
        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)

        # 1. åŸºäºä½ç½®çš„å…ˆéªŒï¼šåœ°é¢é€šå¸¸åœ¨å›¾åƒä¸‹åŠéƒ¨åˆ†
        ground_region_y_start = int(h * 0.6)

        # 2. åœ¨ä¸‹åŠéƒ¨åˆ†è¿›è¡Œé¢œè‰²èšç±»
        lower_region = image[ground_region_y_start:, :, :]
        lower_gray = gray[ground_region_y_start:, :]

        # 3. åŸºäºé¢œè‰²ä¸€è‡´æ€§æ£€æµ‹åœ°é¢
        kernel_size = 15
        blur_gray = cv2.GaussianBlur(lower_gray, (kernel_size, kernel_size), 0)
        grad_x = cv2.Sobel(blur_gray, cv2.CV_64F, 1, 0, ksize=3)
        grad_y = cv2.Sobel(blur_gray, cv2.CV_64F, 0, 1, ksize=3)
        gradient_magnitude = np.sqrt(grad_x ** 2 + grad_y ** 2)

        # åœ°é¢åŒºåŸŸé€šå¸¸æ¢¯åº¦è¾ƒå°
        texture_threshold = np.percentile(gradient_magnitude, 30)
        smooth_regions = (gradient_magnitude < texture_threshold).astype(np.uint8)

        # 4. åŸºäºé¢œè‰²èšç±»æ£€æµ‹ä¸»è¦åœ°é¢é¢œè‰²
        mean_color = np.mean(lower_region, axis=(0, 1))
        color_distances = np.linalg.norm(lower_region - mean_color, axis=2)
        color_threshold = np.std(color_distances) * 1.5
        color_mask = (color_distances < color_threshold).astype(np.uint8)

        # 5. ç»“åˆçº¹ç†å’Œé¢œè‰²ä¿¡æ¯
        combined_mask = np.logical_and(smooth_regions, color_mask).astype(np.uint8)

        # 6. å½¢æ€å­¦æ“ä½œæ¸…ç†mask
        kernel = np.ones((5, 5), np.uint8)
        combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_CLOSE, kernel)
        combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_OPEN, kernel)

        # 7. å°†ç»“æœæ˜ å°„å›å®Œæ•´å›¾åƒ
        ground_mask[ground_region_y_start:, :] = combined_mask

        # 8. å‘ä¸Šæ‰©å±•åœ°é¢åŒºåŸŸ
        if np.sum(combined_mask) > 0:
            ground_mask = self._extend_ground_upward(image, ground_mask, ground_region_y_start)

        return ground_mask

    def _extend_ground_upward(self, image, initial_ground_mask, start_y):
        """å‘ä¸Šæ‰©å±•åœ°é¢åŒºåŸŸï¼ŒåŸºäºé¢œè‰²ç›¸ä¼¼æ€§"""
        h, w = image.shape[:2]
        extended_mask = initial_ground_mask.copy()

        # è·å–å·²çŸ¥åœ°é¢åŒºåŸŸçš„å¹³å‡é¢œè‰²
        ground_pixels = image[initial_ground_mask > 0]
        if len(ground_pixels) == 0:
            return initial_ground_mask

        mean_ground_color = np.mean(ground_pixels, axis=0)
        color_std = np.std(ground_pixels, axis=0)

        # å‘ä¸Šé€è¡Œæ£€æŸ¥
        for y in range(start_y - 1, max(int(h * 0.3), 0), -1):
            row_colors = image[y, :, :]
            color_distances = np.linalg.norm(row_colors - mean_ground_color, axis=1)

            threshold = np.linalg.norm(color_std) * 2
            similar_pixels = color_distances < threshold

            # åªä¿ç•™ä¸ä¸‹æ–¹åœ°é¢åŒºåŸŸè¿é€šçš„åƒç´ 
            if y < h - 1:
                below_mask = extended_mask[y + 1, :]
                # ç¡®ä¿below_maskæ˜¯ä¸€ç»´çš„ï¼Œå¹¶è¿›è¡Œè†¨èƒ€æ“ä½œ
                below_mask_2d = below_mask.reshape(1, -1)  # è½¬æ¢ä¸º [1, W] å½¢çŠ¶è¿›è¡Œè†¨èƒ€
                dilated_below = cv2.dilate(below_mask_2d.astype(np.uint8), np.ones((1, 3), np.uint8))
                dilated_below_1d = dilated_below.reshape(-1) > 0  # è½¬å›ä¸€ç»´å¹¶è½¬ä¸ºå¸ƒå°”å€¼

                # ç¡®ä¿ç»´åº¦åŒ¹é…
                connected_pixels = np.logical_and(similar_pixels, dilated_below_1d)
                extended_mask[y, :] = connected_pixels.astype(np.uint8)

        return extended_mask

    def _ml_ground_segmentation(self, image):
        """åŸºäºæœºå™¨å­¦ä¹ æ¨¡å‹çš„åœ°é¢åˆ†å‰²ï¼ˆé¢„ç•™æ¥å£ï¼‰"""
        h, w = image.shape[:2]
        return np.zeros((h, w), dtype=np.uint8)

    def repair_ground_shadows(self, image, vehicle_mask, ground_mask):
        """ä¿®å¤è½¦è¾†åœ¨åœ°é¢ä¸Šçš„é˜´å½±/é¬¼å½±"""
        # 1. æ£€æµ‹è½¦è¾†maskä¸åœ°é¢çš„äº¤é›†ï¼ˆæ½œåœ¨é˜´å½±åŒºåŸŸï¼‰
        shadow_regions = np.logical_and(vehicle_mask, ground_mask).astype(np.uint8)

        if np.sum(shadow_regions) == 0:
            return image.copy(), shadow_regions

        # 2. æ‰©å±•é˜´å½±åŒºåŸŸï¼ŒåŒ…å«å¯èƒ½çš„è¾¹ç¼˜æ•ˆåº”
        kernel = self.ground_dilation_kernel
        expanded_shadow = cv2.dilate(shadow_regions, kernel, iterations=1)

        # 3. ç¡®ä¿æ‰©å±•åŒºåŸŸä»åœ¨åœ°é¢å†…
        final_shadow_regions = np.logical_and(expanded_shadow, ground_mask).astype(np.uint8)

        # 4. åˆ›å»ºä¿®å¤mask
        inpaint_mask = final_shadow_regions.astype(np.uint8) * 255

        # 5. ä½¿ç”¨å›¾åƒä¿®å¤ç®—æ³•
        repaired_image = self._inpaint_ground_region(image, inpaint_mask, ground_mask)

        return repaired_image, final_shadow_regions

    def _inpaint_ground_region(self, image, inpaint_mask, ground_mask):
        """å¯¹åœ°é¢åŒºåŸŸè¿›è¡Œå›¾åƒä¿®å¤"""
        try:
            repaired = cv2.inpaint(image, inpaint_mask, self.inpaint_radius, cv2.INPAINT_TELEA)
            return repaired
        except:
            return self._simple_ground_inpaint(image, inpaint_mask, ground_mask)

    def _simple_ground_inpaint(self, image, inpaint_mask, ground_mask):
        """ç®€å•çš„åœ°é¢ä¿®å¤ï¼šç”¨å‘¨å›´åœ°é¢åƒç´ çš„å‡å€¼å¡«å……"""
        repaired = image.copy()
        h, w = image.shape[:2]

        repair_coords = np.where(inpaint_mask > 0)
        if len(repair_coords[0]) == 0:
            return repaired

        for i in range(len(repair_coords[0])):
            y, x = repair_coords[0][i], repair_coords[1][i]

            search_radius = 10
            y1 = max(0, y - search_radius)
            y2 = min(h, y + search_radius + 1)
            x1 = max(0, x - search_radius)
            x2 = min(w, x + search_radius + 1)

            search_region_ground = ground_mask[y1:y2, x1:x2]
            search_region_inpaint = inpaint_mask[y1:y2, x1:x2]

            valid_ground = np.logical_and(search_region_ground, search_region_inpaint == 0)

            if np.sum(valid_ground) > 0:
                search_region_image = image[y1:y2, x1:x2]
                valid_pixels = search_region_image[valid_ground]
                mean_color = np.mean(valid_pixels, axis=0)

                noise = np.random.normal(0, 5, 3)
                final_color = np.clip(mean_color + noise, 0, 255)
                repaired[y, x] = final_color.astype(np.uint8)

        return repaired

    def create_colorful_visualization(self, image, frame_idx,
                                      dynamic_boxes, dynamic_labels, dynamic_scores,
                                      static_boxes, static_labels, static_scores,
                                      dynamic_sam_masks=None, static_sam_masks=None):
        """ğŸ¨ åˆ›å»ºåŠ¨æ€å’Œé™æ€å¯¹è±¡çš„å½©è‰²å¯è§†åŒ– - ä½¿ç”¨ä¿®å¤ç‰ˆ"""
        if not self.save_images or not self.enable_colorful_vis:
            return {}

        try:
            h, w = image.shape[:2]

            # ğŸ” æ·»åŠ è°ƒè¯•ä¿¡æ¯
            print(f"\nğŸ¨ å¼€å§‹åˆ›å»ºå½©è‰²å¯è§†åŒ– - Frame {frame_idx}")
            self.colorful_visualizer.debug_detection_results(
                dynamic_boxes, dynamic_labels, static_boxes, static_labels, (h, w)
            )

            # 1. åˆ›å»ºç»„åˆå½©è‰²åˆ†å‰²maskï¼ˆä½¿ç”¨ä¿®å¤ç‰ˆçš„åæ ‡è½¬æ¢ï¼‰
            combined_mask, dynamic_color_map, static_color_map, class_mask = self.colorful_visualizer.create_combined_segmentation_mask(
                (h, w), dynamic_boxes, dynamic_labels, dynamic_scores,
                static_boxes, static_labels, static_scores,
                dynamic_sam_masks, static_sam_masks
            )

            # 2. éªŒè¯ç”Ÿæˆçš„mask
            dynamic_pixels = np.sum(combined_mask[:, :, 0] > 0) if len(dynamic_color_map) > 0 else 0
            static_pixels = np.sum(combined_mask[:, :, 1] > 0) if len(static_color_map) > 0 else 0
            total_colored_pixels = np.sum(np.any(combined_mask > 0, axis=2))

            print(f"  ğŸ“Š ç”Ÿæˆçš„maskç»Ÿè®¡:")
            print(f"     - æ€»å½©è‰²åƒç´ : {total_colored_pixels}")
            print(f"     - åŠ¨æ€åŒºåŸŸåƒç´ : {dynamic_pixels}")
            print(f"     - é™æ€åŒºåŸŸåƒç´ : {static_pixels}")
            print(f"     - è¦†ç›–ç‡: {total_colored_pixels / (h * w) * 100:.1f}%")

            # 3. åˆ›å»ºåˆ†ç¦»çš„å¯è§†åŒ–
            dynamic_only_mask, static_only_mask = self.colorful_visualizer.create_separate_visualizations(
                (h, w), dynamic_boxes, dynamic_labels, dynamic_scores,
                static_boxes, static_labels, static_scores,
                dynamic_sam_masks, static_sam_masks
            )

            # 4. åˆ›å»ºå åŠ å¯è§†åŒ–
            overlay_image = self.colorful_visualizer.create_overlay_visualization(
                image, combined_mask, alpha=0.6
            )

            # 5. åˆ›å»ºå¸¦å›¾ä¾‹çš„å¯è§†åŒ–
            legend_image = self.colorful_visualizer.add_combined_legend(
                overlay_image, dynamic_color_map, static_color_map,
                dynamic_scores, dynamic_labels, static_scores, static_labels
            )

            # 6. ä¿å­˜æ‰€æœ‰å¯è§†åŒ–ç»“æœ
            self._save_colorful_results(
                frame_idx, image, combined_mask, dynamic_only_mask, static_only_mask,
                overlay_image, legend_image, dynamic_color_map, static_color_map,
                dynamic_labels, dynamic_scores, static_labels, static_scores, class_mask
            )

            print(f"âœ… å½©è‰²å¯è§†åŒ–åˆ›å»ºå®Œæˆ - Frame {frame_idx}")
            print(f"   - åŠ¨æ€ç±»åˆ«: {list(dynamic_color_map.keys())}")
            print(f"   - é™æ€ç±»åˆ«: {list(static_color_map.keys())}")

            return {
                'combined_mask': combined_mask,
                'dynamic_only_mask': dynamic_only_mask,
                'static_only_mask': static_only_mask,
                'overlay_image': overlay_image,
                'legend_image': legend_image,
                'dynamic_color_map': dynamic_color_map,
                'static_color_map': static_color_map,
                'class_mask': class_mask
            }

        except Exception as e:
            print(f"âŒ å½©è‰²å¯è§†åŒ–åˆ›å»ºå¤±è´¥ - Frame {frame_idx}: {e}")
            import traceback
            traceback.print_exc()
            return {}
    def _save_colorful_results(self, frame_idx, original_image, combined_mask,
                               dynamic_only_mask, static_only_mask, overlay_image,
                               legend_image, dynamic_color_map, static_color_map,
                               dynamic_labels, dynamic_scores, static_labels, static_scores, class_mask):
        """ä¿å­˜å½©è‰²å¯è§†åŒ–ç»“æœ"""
        try:
            # ä¿å­˜ç»„åˆå½©è‰²mask
            cv2.imwrite(
                os.path.join(self.save_dir, "colorful_combined", f"frame_{frame_idx:06d}_combined.jpg"),
                combined_mask
            )

            # ä¿å­˜åŠ¨æ€å¯¹è±¡mask
            cv2.imwrite(
                os.path.join(self.save_dir, "colorful_dynamic_only", f"frame_{frame_idx:06d}_dynamic.jpg"),
                dynamic_only_mask
            )

            # ä¿å­˜é™æ€å¯¹è±¡mask
            cv2.imwrite(
                os.path.join(self.save_dir, "colorful_static_only", f"frame_{frame_idx:06d}_static.jpg"),
                static_only_mask
            )

            # ä¿å­˜å åŠ å›¾åƒ
            cv2.imwrite(
                os.path.join(self.save_dir, "colorful_overlay", f"frame_{frame_idx:06d}_overlay.jpg"),
                overlay_image
            )

            # ä¿å­˜å¸¦å›¾ä¾‹çš„å›¾åƒ
            cv2.imwrite(
                os.path.join(self.save_dir, "colorful_legend", f"frame_{frame_idx:06d}_legend.jpg"),
                legend_image
            )

            # ä¿å­˜è¯¦ç»†åˆ†æ
            self._create_detailed_analysis(frame_idx, dynamic_color_map, static_color_map,
                                           dynamic_labels, dynamic_scores, static_labels, static_scores, class_mask)

        except Exception as e:
            print(f"âŒ Failed to save colorful results for frame {frame_idx}: {e}")

    def _create_detailed_analysis(self, frame_idx, dynamic_color_map, static_color_map,
                                  dynamic_labels, dynamic_scores, static_labels, static_scores, class_mask):
        """åˆ›å»ºè¯¦ç»†çš„é¢œè‰²åˆ†ææŠ¥å‘Š"""
        try:
            analysis_path = os.path.join(self.save_dir, "colorful_analysis", f"frame_{frame_idx:06d}_analysis.txt")

            # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„åƒç´ ç»Ÿè®¡
            total_pixels = class_mask.size
            class_stats = {}

            # åŠ¨æ€å¯¹è±¡ç»Ÿè®¡
            for i, label in enumerate(dynamic_labels):
                class_id = i + 1  # ç±»åˆ«IDä»1å¼€å§‹
                pixel_count = np.sum(class_mask == class_id)
                percentage = (pixel_count / total_pixels) * 100
                class_stats[f"dynamic_{label}"] = {
                    'pixel_count': pixel_count,
                    'percentage': percentage,
                    'confidence': dynamic_scores[i],
                    'type': 'dynamic'
                }

            # é™æ€å¯¹è±¡ç»Ÿè®¡
            start_id = len(dynamic_labels) + 1
            for i, label in enumerate(static_labels):
                class_id = start_id + i
                pixel_count = np.sum(class_mask == class_id)
                percentage = (pixel_count / total_pixels) * 100
                class_stats[f"static_{label}"] = {
                    'pixel_count': pixel_count,
                    'percentage': percentage,
                    'confidence': static_scores[i],
                    'type': 'static'
                }

            # ä¿å­˜åˆ†ææŠ¥å‘Š
            with open(analysis_path, 'w') as f:
                f.write(f"Frame {frame_idx} Colorful Segmentation Analysis\n")
                f.write("=" * 60 + "\n\n")
                f.write(f"Scene Type: {self.prompt_manager.current_scene}\n")
                f.write(f"Total Dynamic Classes: {len(dynamic_color_map)}\n")
                f.write(f"Total Static Classes: {len(static_color_map)}\n\n")

                # åŠ¨æ€å¯¹è±¡é¢œè‰²æ˜ å°„
                f.write("Dynamic Objects Color Mapping (BGR):\n")
                f.write("-" * 40 + "\n")
                for class_name, color in dynamic_color_map.items():
                    f.write(f"{class_name:20}: BGR({color[0]:3d}, {color[1]:3d}, {color[2]:3d})\n")

                f.write("\nStatic Objects Color Mapping (BGR):\n")
                f.write("-" * 40 + "\n")
                for class_name, color in static_color_map.items():
                    f.write(f"{class_name:20}: BGR({color[0]:3d}, {color[1]:3d}, {color[2]:3d})\n")

                # åƒç´ è¦†ç›–ç»Ÿè®¡
                f.write("\nPixel Coverage Statistics:\n")
                f.write("-" * 40 + "\n")
                sorted_stats = sorted(class_stats.items(), key=lambda x: x[1]['pixel_count'], reverse=True)

                for rank, (class_name, stats) in enumerate(sorted_stats, 1):
                    type_indicator = "ğŸ”´" if stats['type'] == 'dynamic' else "ğŸ”µ"
                    f.write(f"{rank:2d}. {type_indicator} {class_name:20}: {stats['pixel_count']:6d} pixels "
                            f"({stats['percentage']:5.2f}%) - Conf: {stats['confidence']:.3f}\n")

                # æ€»ä½“ç»Ÿè®¡
                dynamic_pixels = sum(s['pixel_count'] for s in class_stats.values() if s['type'] == 'dynamic')
                static_pixels = sum(s['pixel_count'] for s in class_stats.values() if s['type'] == 'static')

                f.write(f"\nOverall Statistics:\n")
                f.write("-" * 20 + "\n")
                f.write(f"Total Dynamic Pixels: {dynamic_pixels} ({dynamic_pixels / total_pixels * 100:.2f}%)\n")
                f.write(f"Total Static Pixels: {static_pixels} ({static_pixels / total_pixels * 100:.2f}%)\n")
                f.write(
                    f"Total Segmented: {dynamic_pixels + static_pixels} ({(dynamic_pixels + static_pixels) / total_pixels * 100:.2f}%)\n")
                f.write(f"Background Pixels: {total_pixels - dynamic_pixels - static_pixels} "
                        f"({(total_pixels - dynamic_pixels - static_pixels) / total_pixels * 100:.2f}%)\n")

        except Exception as e:
            print(f"âŒ Failed to create detailed analysis for frame {frame_idx}: {e}")

    def detect_and_segment(self, image, frame_idx=None):
        """
        ä½¿ç”¨Grounding DINOæ£€æµ‹åŠ¨æ€å’Œé™æ€ç‰©ä½“å¹¶ç”Ÿæˆç²¾ç¡®åˆ†å‰²maskï¼ŒåŒ…å«å®Œæ•´å½©è‰²å¯è§†åŒ–

        Returns:
            final_mask: åŠ¨æ€ç‰©ä½“mask (1=dynamic, 0=static)
            max_confidence: æœ€é«˜æ£€æµ‹ç½®ä¿¡åº¦
            repaired_image: ä¿®å¤åçš„å›¾åƒ
            colorful_results: å½©è‰²å¯è§†åŒ–ç»“æœå­—å…¸
        """
        h, w = image.shape[:2]
        final_mask = np.zeros((h, w), dtype=np.uint8)
        grounding_dino_mask = np.zeros((h, w), dtype=np.uint8)
        repaired_image = image.copy()
        max_confidence = 0.0
        colorful_results = {}
        self._last_detected_objects = []

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥ä½¿ç”¨fallbackæ¨¡å¼
        if not self.grounding_detector or not self.initialization_success:
            print(f"âš ï¸  Using fallback mode for frame {frame_idx}")
            return final_mask, 0.0, repaired_image, colorful_results

        # 1. åœ°é¢åˆ†å‰²ï¼ˆå¯é€‰ï¼‰
        ground_mask = None
        if self.use_ground_segmentation:
            try:
                ground_mask = self.segment_ground(image)
                print(f"ğŸŒ Ground segmentation: {np.sum(ground_mask)} pixels detected as ground")
            except Exception as e:
                print(f"âŒ Ground segmentation failed: {e}")

        # 2. è·å–å½“å‰åœºæ™¯çš„promptå’Œé˜ˆå€¼
        try:
            dynamic_prompt, confidence_threshold = self.prompt_manager.get_current_prompt()
            static_prompt, _ = self.prompt_manager.get_static_prompt()
            print(f"ğŸ¯ Dynamic prompt: '{dynamic_prompt[:30]}...' (confidence: {confidence_threshold})")
            if self.detect_static_objects and static_prompt:
                print(f"ğŸ—ï¸  Static prompt: '{static_prompt[:30]}...'")
        except Exception as e:
            print(f"âŒ Failed to get prompts: {e}")
            dynamic_prompt = "car. truck. person. bicycle"
            static_prompt = "building. tree. road. wall"
            confidence_threshold = 0.2

        # 3. æ£€æµ‹åŠ¨æ€å¯¹è±¡
        dynamic_boxes, dynamic_scores, dynamic_labels = np.array([]), np.array([]), []
        try:
            dynamic_boxes, dynamic_scores, dynamic_labels = self.grounding_detector.detect(
                image, dynamic_prompt, confidence_threshold
            )

            # ğŸ” æ·»åŠ åæ ‡è°ƒè¯•
            if len(dynamic_boxes) > 0:
                print(f"\nğŸ” åŠ¨æ€å¯¹è±¡æ£€æµ‹ç»“æœè°ƒè¯•:")
                for i, (box, label, score) in enumerate(zip(dynamic_boxes[:3], dynamic_labels[:3], dynamic_scores[:3])):
                    print(f"  {i + 1}. {label} (conf: {score:.3f})")
                    if hasattr(box, 'shape'):
                        print(f"     åŸå§‹box: {box}")
                    else:
                        print(f"     åŸå§‹box: {np.array(box)}")

        except Exception as e:
            print(f"âŒ åŠ¨æ€å¯¹è±¡æ£€æµ‹å¤±è´¥: {e}")

        # 4. æ£€æµ‹é™æ€å¯¹è±¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        static_boxes, static_scores, static_labels = np.array([]), np.array([]), []
        if self.detect_static_objects and static_prompt:
            try:
                static_boxes, static_scores, static_labels = self.grounding_detector.detect(
                    image, static_prompt, confidence_threshold
                )
                print(f"ğŸ—ï¸  æ£€æµ‹åˆ° {len(static_boxes)} ä¸ªé™æ€å¯¹è±¡")

                # è°ƒè¯•é™æ€å¯¹è±¡åæ ‡
                if len(static_boxes) > 0:
                    print(f"ğŸ” é™æ€å¯¹è±¡æ£€æµ‹ç»“æœè°ƒè¯•:")
                    for i, (box, label, score) in enumerate(
                            zip(static_boxes[:3], static_labels[:3], static_scores[:3])):
                        print(f"  {i + 1}. {label} (conf: {score:.3f})")
                        print(f"     åŸå§‹box: {np.array(box) if not hasattr(box, 'shape') else box}")

            except Exception as e:
                print(f"âŒ é™æ€å¯¹è±¡æ£€æµ‹å¤±è´¥: {e}")

        if len(dynamic_boxes) == 0 and len(static_boxes) == 0:
            print(f"â„¹ï¸  No objects detected in frame {frame_idx}")
            if frame_idx is not None and self.save_images:
                self.save_detection_results(
                    image, frame_idx,
                    grounding_dino_mask=grounding_dino_mask,
                    final_mask=final_mask,
                    boxes=[], labels=[], scores=[],
                    ground_mask=ground_mask,
                    repaired_image=repaired_image
                )
            return final_mask, 0.0, repaired_image, colorful_results

        # 5. å¤„ç†åŠ¨æ€å¯¹è±¡æ£€æµ‹ç»“æœï¼Œåˆ›å»ºåŠ¨æ€ç‰©ä½“mask
        vehicle_detected = False
        if len(dynamic_boxes) > 0:
            print(f"ğŸ¯ Detected {len(dynamic_boxes)} dynamic objects:")

            for i, (box, score, label) in enumerate(zip(dynamic_boxes, dynamic_scores, dynamic_labels)):
                box_tensor = torch.tensor(box, dtype=torch.float32)
                box_xyxy = box_ops.box_cxcywh_to_xyxy(box_tensor.unsqueeze(0)) * torch.tensor([w, h, w, h])
                x1, y1, x2, y2 = box_xyxy[0].cpu().numpy().astype(int)
                x1, y1 = max(0, x1), max(0, y1)
                x2, y2 = min(w, x2), min(h, y2)

                max_confidence = max(max_confidence, score)

                self._last_detected_objects.append({
                    'label': label,
                    'box': box,
                    'score': score,
                    'frame': frame_idx,
                    'type': 'dynamic'
                })

                # æ£€æŸ¥æ˜¯å¦æ˜¯è½¦è¾†
                vehicle_keywords = ["car", "truck", "bus", "vehicle", "van", "suv", "motorcycle", "bike"]
                if any(keyword in label.lower() for keyword in vehicle_keywords):
                    vehicle_detected = True
                    # å¯¹è½¦è¾†æ‰©å±•è¾¹ç•Œæ¡†
                    width = x2 - x1
                    height = y2 - y1
                    expand_w = int(width * 0.1)
                    expand_h = int(height * 0.1)

                    x1 = max(0, x1 - expand_w)
                    y1 = max(0, y1 - expand_h)
                    x2 = min(w, x2 + expand_w)
                    y2 = min(h, y2 + expand_h)

                    print(f"  ğŸš— Vehicle '{label}' (conf={score:.3f}) - expanded mask")
                else:
                    print(f"  ğŸ‘¤ Object '{label}' (conf={score:.3f})")

                # æ ‡è®°åŠ¨æ€åŒºåŸŸ
                grounding_dino_mask[y1:y2, x1:x2] = 1

            final_mask = grounding_dino_mask.copy()

        # 6. å¤„ç†é™æ€å¯¹è±¡æ£€æµ‹ç»“æœï¼ˆç”¨äºå¯è§†åŒ–ï¼Œä¸å½±å“final_maskï¼‰
        if len(static_boxes) > 0:
            print(f"ğŸ—ï¸  Processing {len(static_boxes)} static objects for visualization:")
            for i, (box, score, label) in enumerate(zip(static_boxes, static_scores, static_labels)):
                self._last_detected_objects.append({
                    'label': label,
                    'box': box,
                    'score': score,
                    'frame': frame_idx,
                    'type': 'static'
                })
                print(f"  ğŸ¢ Static '{label}' (conf={score:.3f})")

        # 7. SAMç²¾ç¡®åˆ†å‰²ï¼ˆå¤„ç†åŠ¨æ€å’Œé™æ€å¯¹è±¡ï¼‰
        dynamic_sam_masks = []
        static_sam_masks = []

        if self.use_sam:
            self.sam_predictor.set_image(image)

            # SAMå¤„ç†åŠ¨æ€å¯¹è±¡
            if len(dynamic_boxes) > 0:
                try:
                    sam_combined_mask = np.zeros((h, w), dtype=np.uint8)

                    for i, (box, score, label) in enumerate(zip(dynamic_boxes, dynamic_scores, dynamic_labels)):
                        try:
                            box_tensor = torch.tensor(box, dtype=torch.float32)
                            box_xyxy = box_ops.box_cxcywh_to_xyxy(box_tensor.unsqueeze(0)) * torch.tensor([w, h, w, h])
                            x1, y1, x2, y2 = box_xyxy[0].cpu().numpy().astype(int)

                            # è½¦è¾†æ‰©å±•å¤„ç†
                            vehicle_keywords = ["car", "truck", "bus", "vehicle", "van", "suv", "motorcycle", "bike"]
                            if any(keyword in label.lower() for keyword in vehicle_keywords):
                                width = x2 - x1
                                height = y2 - y1
                                expand_w = int(width * 0.1)
                                expand_h = int(height * 0.1)
                                x1 = max(0, x1 - expand_w)
                                y1 = max(0, y1 - expand_h)
                                x2 = min(w, x2 + expand_w)
                                y2 = min(h, y2 + expand_h)

                            input_box = np.array([x1, y1, x2, y2])
                            masks, sam_scores, _ = self.sam_predictor.predict(
                                point_coords=None,
                                point_labels=None,
                                box=input_box[None, :],
                                multimask_output=False,
                            )

                            if len(masks) > 0:
                                best_mask = masks[0].astype(np.uint8)
                                dynamic_sam_masks.append(best_mask)
                                sam_combined_mask = np.logical_or(sam_combined_mask, best_mask).astype(np.uint8)
                                print(f"  âœ… SAM refined dynamic '{label}' mask")
                            else:
                                dynamic_sam_masks.append(None)
                        except Exception as e:
                            print(f"  âŒ SAM failed for dynamic '{label}': {e}")
                            dynamic_sam_masks.append(None)

                    if sam_combined_mask.sum() > 0:
                        final_mask = sam_combined_mask
                        print(f"ğŸ¯ SAM refinement complete for dynamic objects")

                except Exception as e:
                    print(f"âŒ SAM processing failed for dynamic objects: {e}")

            # SAMå¤„ç†é™æ€å¯¹è±¡ï¼ˆä»…ç”¨äºå¯è§†åŒ–ï¼‰
            if len(static_boxes) > 0:
                try:
                    for i, (box, score, label) in enumerate(zip(static_boxes, static_scores, static_labels)):
                        try:
                            box_tensor = torch.tensor(box, dtype=torch.float32)
                            box_xyxy = box_ops.box_cxcywh_to_xyxy(box_tensor.unsqueeze(0)) * torch.tensor([w, h, w, h])
                            x1, y1, x2, y2 = box_xyxy[0].cpu().numpy().astype(int)
                            x1, y1 = max(0, x1), max(0, y1)
                            x2, y2 = min(w, x2), min(h, y2)

                            input_box = np.array([x1, y1, x2, y2])
                            masks, sam_scores, _ = self.sam_predictor.predict(
                                point_coords=None,
                                point_labels=None,
                                box=input_box[None, :],
                                multimask_output=False,
                            )

                            if len(masks) > 0:
                                best_mask = masks[0].astype(np.uint8)
                                static_sam_masks.append(best_mask)
                                print(f"  âœ… SAM refined static '{label}' mask")
                            else:
                                static_sam_masks.append(None)
                        except Exception as e:
                            print(f"  âŒ SAM failed for static '{label}': {e}")
                            static_sam_masks.append(None)

                except Exception as e:
                    print(f"âŒ SAM processing failed for static objects: {e}")

        # 8. ğŸ¨ åˆ›å»ºå½©è‰²å¯è§†åŒ–ï¼ˆåŒ…å«åŠ¨æ€å’Œé™æ€å¯¹è±¡ï¼‰
        if self.enable_colorful_vis:
            try:
                colorful_results = self.create_colorful_visualization(
                    image, frame_idx,
                    dynamic_boxes, dynamic_labels, dynamic_scores,
                    static_boxes, static_labels, static_scores,
                    dynamic_sam_masks if dynamic_sam_masks else None,
                    static_sam_masks if static_sam_masks else None
                )
            except Exception as e:
                print(f"âŒ å½©è‰²å¯è§†åŒ–å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()

        # 9. åœ°é¢ä¿®å¤ï¼ˆå¦‚æœæ£€æµ‹åˆ°è½¦è¾†ï¼‰
        shadow_regions = None
        if self.use_ground_segmentation and ground_mask is not None and vehicle_detected:
            try:
                repaired_image, shadow_regions = self.repair_ground_shadows(image, final_mask, ground_mask)
                print(f"ğŸ”§ Ground repair: {np.sum(shadow_regions) if shadow_regions is not None else 0} pixels")
            except Exception as e:
                print(f"âŒ Ground repair failed: {e}")

        # 10. æ—¶é—´ä¸€è‡´æ€§æ»¤æ³¢
        try:
            final_mask = self._temporal_consistency(final_mask)
        except Exception as e:
            print(f"âŒ Temporal consistency failed: {e}")

        # 11. æœ€ç»ˆçš„è†¨èƒ€å¤„ç†
        if final_mask.sum() > 0:
            kernel = np.ones((5, 5), np.uint8)
            final_mask = cv2.dilate(final_mask, kernel, iterations=1)
            print(f"ğŸ›¡ï¸  Applied final dilation to dynamic mask")

        # 12. ä¿å­˜ç»“æœ
        if frame_idx is not None and self.save_images:
            self.save_detection_results(
                image, frame_idx,
                grounding_dino_mask=grounding_dino_mask,
                final_mask=final_mask,
                boxes=dynamic_boxes,  # ä¿å­˜åŠ¨æ€æ£€æµ‹æ¡†
                labels=dynamic_labels,
                scores=dynamic_scores,
                ground_mask=ground_mask,
                shadow_regions=shadow_regions,
                repaired_image=repaired_image
            )

        # è¾“å‡ºç»Ÿè®¡
        print(f"ğŸ“Š Frame {frame_idx} final statistics:")
        print(f"  - Dynamic objects: {len(dynamic_boxes)} detected")
        print(f"  - Static objects: {len(static_boxes)} detected")
        print(f"  - Total dynamic pixels: {final_mask.sum()} ({np.mean(final_mask) * 100:.1f}% of image)")
        print(f"  - Max confidence: {max_confidence:.3f}")
        if self.enable_colorful_vis and colorful_results:
            total_classes = len(colorful_results.get('dynamic_color_map', {})) + len(
                colorful_results.get('static_color_map', {}))
            print(f"  - Colorful visualization: âœ… Created with {total_classes} classes")

        return final_mask, max_confidence, repaired_image, colorful_results

    def debug_coordinate_conversion():
        """è°ƒè¯•åæ ‡è½¬æ¢çš„ç‹¬ç«‹æµ‹è¯•å‡½æ•°"""
        visualizer = ColorfulSegmentationVisualizer()

        # æµ‹è¯•å„ç§åæ ‡æ ¼å¼
        test_cases = [
            # å½’ä¸€åŒ– cxcywh
            ([0.5, 0.5, 0.2, 0.3], "å½’ä¸€åŒ– cxcywh"),
            # åƒç´  xyxy
            ([100, 100, 200, 200], "åƒç´  xyxy"),
            # åƒç´  cxcywh
            ([150, 150, 100, 100], "åƒç´  cxcywh"),
            # torch tensor
            (torch.tensor([0.3, 0.4, 0.1, 0.2]), "torchå½’ä¸€åŒ–"),
        ]

        w, h = 640, 480
        print(f"ğŸ” æµ‹è¯•å›¾åƒå°ºå¯¸: {w} x {h}")

        for box, desc in test_cases:
            try:
                x1, y1, x2, y2 = visualizer._convert_box_coordinates(box, w, h)
                print(f"âœ… {desc}: {box} -> ({x1},{y1},{x2},{y2})")
            except Exception as e:
                print(f"âŒ {desc}: {box} -> è½¬æ¢å¤±è´¥: {e}")
    def get_static_mask_for_gaussian_init(self, image, frame_idx=None):
        """
        ä¸ºé«˜æ–¯ä½“åˆå§‹åŒ–è·å–é™æ€åŒºåŸŸmaskï¼Œç°åœ¨åŒ…å«å®Œæ•´çš„åŠ¨é™æ€å¯è§†åŒ–
        è¿”å›çš„static_maskä¸­ï¼š
        - 1 è¡¨ç¤ºé™æ€åŒºåŸŸï¼ˆåº”è¯¥è¢«é‡å»ºï¼‰
        - 0 è¡¨ç¤ºåŠ¨æ€åŒºåŸŸï¼ˆåº”è¯¥è¢«maskæ‰ï¼‰

        Returns:
            static_mask: é™æ€åŒºåŸŸmask (1=static, 0=dynamic)
            repaired_image: ç”¨äºåˆå§‹åŒ–çš„ä¿®å¤å›¾åƒ
            detected_objects: æ£€æµ‹åˆ°çš„æ‰€æœ‰ç‰©ä½“åˆ—è¡¨ï¼ˆåŠ¨æ€+é™æ€ï¼‰
            colorful_results: å½©è‰²å¯è§†åŒ–ç»“æœ
        """
        # è·å–åŠ¨æ€ç‰©ä½“maskå’Œä¿®å¤å›¾åƒï¼Œä»¥åŠå®Œæ•´çš„å¯è§†åŒ–ç»“æœ
        dynamic_mask, confidence, repaired_image, colorful_results = self.detect_and_segment(image, frame_idx)

        # é™æ€maskæ˜¯åŠ¨æ€maskçš„åå‘
        # dynamic_maskä¸­ 1=åŠ¨æ€ç‰©ä½“ï¼Œ0=é™æ€èƒŒæ™¯
        # static_maskä¸­ 1=é™æ€èƒŒæ™¯ï¼Œ0=åŠ¨æ€ç‰©ä½“
        static_mask = (1 - dynamic_mask).astype(np.uint8)

        # è·å–æ£€æµ‹åˆ°çš„ç‰©ä½“ä¿¡æ¯ï¼ˆåŒ…å«åŠ¨æ€å’Œé™æ€ï¼‰
        detected_objects = []
        if hasattr(self, '_last_detected_objects'):
            detected_objects = self._last_detected_objects

        print(f"ğŸ“Š Frame {frame_idx} mask summary:")
        print(f"  - Dynamic pixels (masked out): {np.sum(dynamic_mask > 0)} ({np.mean(dynamic_mask) * 100:.1f}%)")
        print(f"  - Static pixels (reconstructed): {np.sum(static_mask > 0)} ({np.mean(static_mask) * 100:.1f}%)")

        # ç»Ÿè®¡åŠ¨æ€å’Œé™æ€å¯¹è±¡
        dynamic_count = len([obj for obj in detected_objects if obj.get('type') == 'dynamic'])
        static_count = len([obj for obj in detected_objects if obj.get('type') == 'static'])
        print(f"  - Total objects detected: {len(detected_objects)} (Dynamic: {dynamic_count}, Static: {static_count})")

        return static_mask, repaired_image, detected_objects, colorful_results

    def save_detection_results(self, image, frame_idx, grounding_dino_mask=None, sam_mask=None,
                               motion_mask=None, final_mask=None, boxes=None, labels=None, scores=None,
                               ground_mask=None, shadow_regions=None, repaired_image=None):
        """ä¿å­˜Grounding DINOæ£€æµ‹å’Œåˆ†å‰²çš„å„ç§ç»“æœ"""
        if not self.save_images:
            return

        try:
            # ç¡®ä¿å›¾åƒæ˜¯æ­£ç¡®çš„æ ¼å¼
            if isinstance(image, np.ndarray):
                if image.dtype != np.uint8:
                    image = (image * 255).astype(np.uint8) if image.max() <= 1.0 else image.astype(np.uint8)
                img_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) if len(image.shape) == 3 else image
            else:
                return

            # 1. ä¿å­˜åŸå§‹å›¾åƒ
            original_path = os.path.join(self.save_dir, "original", f"frame_{frame_idx:06d}.jpg")
            cv2.imwrite(original_path, img_bgr)

            # 2. ä¿å­˜Grounding DINOæ£€æµ‹æ¡†å’Œæ ‡ç­¾
            if boxes is not None and len(boxes) > 0:
                detection_img = img_bgr.copy()
                for i, box in enumerate(boxes):
                    x1, y1, x2, y2 = map(int, box)
                    # ç»˜åˆ¶æ£€æµ‹æ¡†
                    cv2.rectangle(detection_img, (x1, y1), (x2, y2), (0, 255, 0), 2)

                    # æ·»åŠ æ ‡ç­¾å’Œç½®ä¿¡åº¦
                    if labels and i < len(labels):
                        label_text = labels[i]
                        if scores is not None and i < len(scores):
                            label_text += f" {scores[i]:.2f}"

                        # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
                        (text_width, text_height), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
                        cv2.rectangle(detection_img, (x1, y1 - text_height - 5), (x1 + text_width, y1), (0, 255, 0), -1)
                        cv2.putText(detection_img, label_text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0),
                                    1)

                detection_path = os.path.join(self.save_dir, "grounding_dino_detections", f"frame_{frame_idx:06d}.jpg")
                cv2.imwrite(detection_path, detection_img)

            # 3. ä¿å­˜å„ç§mask
            if grounding_dino_mask is not None:
                grounding_mask_img = (grounding_dino_mask * 255).astype(np.uint8)
                grounding_path = os.path.join(self.save_dir, "grounding_dino_masks", f"frame_{frame_idx:06d}.jpg")
                cv2.imwrite(grounding_path, grounding_mask_img)

            if sam_mask is not None:
                sam_mask_img = (sam_mask * 255).astype(np.uint8)
                sam_path = os.path.join(self.save_dir, "sam_masks", f"frame_{frame_idx:06d}.jpg")
                cv2.imwrite(sam_path, sam_mask_img)

            if motion_mask is not None:
                motion_mask_img = (motion_mask * 255).astype(np.uint8)
                motion_path = os.path.join(self.save_dir, "motion_masks", f"frame_{frame_idx:06d}.jpg")
                cv2.imwrite(motion_path, motion_mask_img)

            # 4. ä¿å­˜åœ°é¢ç›¸å…³ç»“æœ
            if ground_mask is not None:
                ground_mask_img = (ground_mask * 255).astype(np.uint8)
                ground_path = os.path.join(self.save_dir, "ground_masks", f"frame_{frame_idx:06d}.jpg")
                cv2.imwrite(ground_path, ground_mask_img)

            if shadow_regions is not None:
                shadow_mask_img = (shadow_regions * 255).astype(np.uint8)
                shadow_path = os.path.join(self.save_dir, "shadow_regions", f"frame_{frame_idx:06d}.jpg")
                cv2.imwrite(shadow_path, shadow_mask_img)

            if repaired_image is not None:
                repaired_bgr = cv2.cvtColor(repaired_image, cv2.COLOR_RGB2BGR)
                repaired_path = os.path.join(self.save_dir, "repaired_images", f"frame_{frame_idx:06d}.jpg")
                cv2.imwrite(repaired_path, repaired_bgr)

            # 5. ä¿å­˜æœ€ç»ˆmaskå’Œç»„åˆæ˜¾ç¤º
            if final_mask is not None:
                final_mask_img = (final_mask * 255).astype(np.uint8)
                final_path = os.path.join(self.save_dir, "final_masks", f"frame_{frame_idx:06d}.jpg")
                cv2.imwrite(final_path, final_mask_img)

                # å åŠ æ˜¾ç¤ºï¼ˆåŠ¨æ€åŒºåŸŸç”¨çº¢è‰²ï¼Œåœ°é¢ç”¨ç»¿è‰²ï¼‰
                overlay_img = img_bgr.copy()
                if ground_mask is not None:
                    overlay_img[ground_mask > 0] = [0, 255, 0]  # åœ°é¢ç”¨ç»¿è‰²
                overlay_img[final_mask > 0] = [0, 0, 255]  # åŠ¨æ€ç‰©ä½“ç”¨çº¢è‰²
                overlay_path = os.path.join(self.save_dir, "masked_overlay", f"frame_{frame_idx:06d}.jpg")
                cv2.imwrite(overlay_path, overlay_img)

                # é™æ€åŒºåŸŸå›¾åƒ
                static_img = repaired_bgr.copy() if repaired_image is not None else img_bgr.copy()
                static_img[final_mask > 0] = [0, 0, 0]
                static_path = os.path.join(self.save_dir, "static_only", f"frame_{frame_idx:06d}.jpg")
                cv2.imwrite(static_path, static_img)

            # 6. ä¿å­˜æ£€æµ‹åˆ†æç»“æœ
            if boxes is not None and labels is not None:
                analysis_path = os.path.join(self.save_dir, "detection_analysis", f"frame_{frame_idx:06d}.txt")
                with open(analysis_path, 'w') as f:
                    f.write(f"Frame {frame_idx} Detection Analysis\n")
                    f.write(f"Scene Type: {self.prompt_manager.current_scene}\n")
                    f.write(f"Dynamic Prompt Used: {self.prompt_manager.get_current_prompt()[0]}\n")
                    f.write(f"Total Detections: {len(boxes)}\n\n")

                    for i, (box, label) in enumerate(zip(boxes, labels)):
                        score = scores[i] if scores is not None and i < len(scores) else 0.0
                        f.write(f"Detection {i + 1}:\n")
                        f.write(f"  Label: {label}\n")
                        f.write(f"  Confidence: {score:.3f}\n")
                        f.write(f"  Box: [{box[0]:.1f}, {box[1]:.1f}, {box[2]:.1f}, {box[3]:.1f}]\n\n")

            print(f"ğŸ’¾ Saved all detection results for frame {frame_idx}")

        except Exception as e:
            print(f"âŒ Warning: Failed to save detection results for frame {frame_idx}: {e}")

    def _temporal_consistency(self, current_mask):
        """æ—¶é—´ä¸€è‡´æ€§æ»¤æ³¢ï¼Œå‡å°‘maskçš„é—ªçƒ"""
        self.mask_history.append(current_mask.copy())

        if len(self.mask_history) > self.history_length:
            self.mask_history.pop(0)

        if len(self.mask_history) < 3:
            return current_mask

        # ä½¿ç”¨å†å²maskçš„ä¸­ä½æ•°æ»¤æ³¢
        mask_stack = np.stack(self.mask_history, axis=0)
        consistent_mask = np.median(mask_stack, axis=0).astype(np.uint8)

        return consistent_mask


# ä¿®æ”¹FrontEndç±»ä»¥ä½¿ç”¨æ–°çš„æ£€æµ‹å™¨
class FrontEnd(mp.Process):
    def __init__(self, config, model, save_dir=None):
        super().__init__()
        self.config = config
        self.background = None
        self.pipeline_params = None
        self.frontend_queue = None
        self.backend_queue = None
        self.q_main2vis = None
        self.q_vis2main = None
        self.save_dir = save_dir

        self.initialized = False
        self.kf_indices = []
        self.monocular = config["Training"]["monocular"]
        self.iteration_count = 0
        self.occ_aware_visibility = {}
        self.current_window = []

        self.reset = True
        self.requested_init = False
        self.requested_keyframe = 0
        self.use_every_n_frames = 1

        self.gaussians = None
        self.cameras = dict()
        self.device = "cuda:0"
        self.pause = False

        self.model = model  # MASt3R Model

        # ğŸ”§ ä¿®å¤ï¼šå°†thetaåˆå§‹åŒ–ä¸ºtensorè€Œä¸æ˜¯æ•´æ•°
        self.theta = torch.tensor(0.0, device=self.device)

        # åˆå§‹åŒ–å¢å¼ºçš„åŠ¨æ€ç‰©ä½“é®ç½©å™¨ï¼ˆåŒ…å«å®Œæ•´çš„åŠ¨é™æ€å¯è§†åŒ–ï¼‰
        self.enable_dynamic_filtering = config.get("dynamic_filtering", {}).get("enabled", True)
        self.filter_initialization = config.get("dynamic_filtering", {}).get("filter_initialization", True)
        self.save_masked_images = config.get("dynamic_filtering", {}).get("save_masked_images", True)
        self.use_ground_segmentation = config.get("dynamic_filtering", {}).get("use_ground_segmentation", True)

        if self.enable_dynamic_filtering and self.filter_initialization:
            # è®¾ç½®ä¿å­˜ç›®å½•
            mask_save_dir = config.get("dynamic_filtering", {}).get("save_dir", "./masked_images")
            scene_type = config.get("dynamic_filtering", {}).get("scene_type", "outdoor_street")
            grounding_dino_model = config.get("dynamic_filtering", {}).get("grounding_dino_model",
                                                                           "IDEA-Research/grounding-dino-tiny")

            # ğŸ¨ æ–°å¢ï¼šå½©è‰²å¯è§†åŒ–å’Œé™æ€å¯¹è±¡æ£€æµ‹é…ç½®
            enable_colorful_vis = config.get("dynamic_filtering", {}).get("enable_colorful_visualization", True)
            detect_static_objects = config.get("dynamic_filtering", {}).get("detect_static_objects", True)

            self.dynamic_masker = EnhancedDynamicObjectMasker(
                device=self.device,
                use_sam=config.get("dynamic_filtering", {}).get("use_sam", True),
                save_dir=mask_save_dir,
                save_images=self.save_masked_images,
                use_ground_segmentation=self.use_ground_segmentation,
                scene_type=scene_type,
                grounding_dino_model=grounding_dino_model,
                enable_colorful_vis=enable_colorful_vis,
                detect_static_objects=detect_static_objects
            )

            # ä»é…ç½®è®¾ç½®åœºæ™¯
            self.dynamic_masker.set_scene_from_config(config)

            print(f"ğŸ¯ Enhanced Dynamic Filtering with Complete Visualization:")
            print(f"  - Enabled: {self.enable_dynamic_filtering}")
            print(f"  - Filter initialization: {self.filter_initialization}")
            print(f"  - SAM: {config.get('dynamic_filtering', {}).get('use_sam', True)}")
            print(f"  - Ground segmentation: {self.use_ground_segmentation}")
            print(f"  - Model: {grounding_dino_model}")
            print(f"  - Colorful visualization: {enable_colorful_vis}")
            print(f"  - Detect static objects: {detect_static_objects}")
            print(f"  - Save images: {self.save_masked_images}")
        else:
            print("âŒ Dynamic filtering is DISABLED - dynamic objects will appear in reconstruction")

    def set_hyperparams(self):
        self.save_dir = self.config["Results"]["save_dir"]
        self.save_results = self.config["Results"]["save_results"]
        self.save_trj = self.config["Results"]["save_trj"]
        self.save_trj_kf_intv = self.config["Results"]["save_trj_kf_intv"]

        self.tracking_itr_num = self.config["Training"]["tracking_itr_num"]
        self.kf_interval = self.config["Training"]["kf_interval"]
        self.window_size = self.config["Training"]["window_size"]
        self.single_thread = self.config["Training"]["single_thread"]

    def _expand_dynamic_mask(self, dynamic_mask, kernel_size=5):
        """æ‰©å±•åŠ¨æ€ç‰©ä½“maskï¼Œé¿å…è¾¹ç•Œå¤„çš„é«˜æ–¯ä½“ç”Ÿæˆ"""
        mask_np = dynamic_mask.cpu().numpy().astype(np.uint8)
        kernel = np.ones((kernel_size, kernel_size), np.uint8)
        expanded_mask_np = cv2.dilate(mask_np, kernel, iterations=1)
        expanded_mask = torch.from_numpy(expanded_mask_np).to(dynamic_mask.device).bool()
        return expanded_mask

    def add_new_keyframe(self, cur_frame_idx, depth=None, opacity=None, init=False):
        """æ·»åŠ æ–°å…³é”®å¸§ï¼Œä½¿ç”¨å®Œæ•´çš„åŠ¨é™æ€ç‰©ä½“æ£€æµ‹å’Œå¯è§†åŒ–"""
        rgb_boundary_threshold = self.config["Training"]["rgb_boundary_threshold"]
        if len(self.kf_indices) > 0:
            last_kf = self.kf_indices[-1]
            viewpoint_last = self.cameras[last_kf]
            R_last = viewpoint_last.R

        self.kf_indices.append(cur_frame_idx)
        viewpoint = self.cameras[cur_frame_idx]

        gt_img = viewpoint.original_image.cuda()
        valid_rgb = (gt_img.sum(dim=0) > rgb_boundary_threshold)[None]

        # ===== æ ¸å¿ƒï¼šä½¿ç”¨å®Œæ•´çš„åŠ¨é™æ€ç‰©ä½“æ£€æµ‹å’Œå¯è§†åŒ– =====
        if self.enable_dynamic_filtering and (not init or self.filter_initialization):
            # è½¬æ¢å›¾åƒæ ¼å¼
            img_np = gt_img.permute(1, 2, 0).cpu().numpy()
            img_np = (img_np * 255).astype(np.uint8)

            # è·å–é™æ€maskã€ä¿®å¤å›¾åƒå’Œå®Œæ•´çš„æ£€æµ‹ç»“æœï¼ˆåŒ…å«å½©è‰²å¯è§†åŒ–ï¼‰
            static_mask_np, repaired_image_np, detected_objects, colorful_results = self.dynamic_masker.get_static_mask_for_gaussian_init(
                img_np, frame_idx=cur_frame_idx
            )

            # è½¬æ¢ä¸ºtorch tensors
            static_mask = torch.from_numpy(static_mask_np).to(self.device).bool()
            dynamic_mask = ~static_mask  # åŠ¨æ€maskæ˜¯é™æ€maskçš„åå‘
            repaired_image = torch.from_numpy(repaired_image_np).to(self.device).float() / 255.0

            # æ‰©å±•åŠ¨æ€maskè¾¹ç•Œï¼Œç¡®ä¿åŠ¨æ€ç‰©ä½“å®Œå…¨è¢«æ’é™¤
            expanded_dynamic_mask = self._expand_dynamic_mask(dynamic_mask, kernel_size=7)
            expanded_static_mask = ~expanded_dynamic_mask

            # å…³é”®ï¼šä»valid_rgbä¸­æ’é™¤åŠ¨æ€åŒºåŸŸ
            valid_rgb = valid_rgb & expanded_static_mask[None]

            # å­˜å‚¨ä¿¡æ¯
            viewpoint.dynamic_mask = dynamic_mask
            viewpoint.expanded_dynamic_mask = expanded_dynamic_mask
            viewpoint.static_mask = static_mask
            viewpoint.expanded_static_mask = expanded_static_mask
            viewpoint.detected_objects = detected_objects
            viewpoint.repaired_image = repaired_image
            viewpoint.colorful_results = colorful_results  # ğŸ¨ æ–°å¢ï¼šå­˜å‚¨å½©è‰²å¯è§†åŒ–ç»“æœ

            # æ‰“å°ç»Ÿè®¡
            static_ratio = static_mask.float().mean().item()
            expanded_static_ratio = expanded_static_mask.float().mean().item()
            dynamic_count = len([obj for obj in detected_objects if obj.get('type') == 'dynamic'])
            static_count = len([obj for obj in detected_objects if obj.get('type') == 'static'])

            print(f"ğŸ”§ Frame {cur_frame_idx} keyframe processing:")
            print(f"  - Detected {dynamic_count} dynamic objects, {static_count} static objects")
            print(f"  - Original static ratio: {static_ratio:.1%}")
            print(f"  - After expansion: {expanded_static_ratio:.1%}")
            print(f"  - Dynamic objects MASKED OUT from reconstruction")
            print(f"  - Static objects PRESERVED for reconstruction")

            if colorful_results:
                total_visualized = len(colorful_results.get('dynamic_color_map', {})) + len(
                    colorful_results.get('static_color_map', {}))
                print(f"  - Colorful visualization: âœ… {total_visualized} classes visualized")

            if expanded_static_ratio < 0.3:
                print(f"âš ï¸  WARNING: Only {expanded_static_ratio:.1%} of frame will be reconstructed!")
                print("    Most of the frame contains dynamic objects.")
        # ============================================================

        if self.monocular:
            if depth is None:
                initial_depth = torch.from_numpy(viewpoint.mono_depth).unsqueeze(0)
                print(f"Initial depth map stats for frame {cur_frame_idx}:",
                      f"Max: {torch.max(initial_depth).item():.3f}",
                      f"Min: {torch.min(initial_depth).item():.3f}",
                      f"Mean: {torch.mean(initial_depth).item():.3f}")

                # å°†æ— æ•ˆåŒºåŸŸï¼ˆåŒ…æ‹¬åŠ¨æ€åŒºåŸŸï¼‰æ·±åº¦è®¾ä¸º0
                # ç”±äºvalid_rgbå·²ç»æ’é™¤äº†åŠ¨æ€åŒºåŸŸï¼Œè¿™é‡Œä¼šè‡ªåŠ¨å¤„ç†
                initial_depth[~valid_rgb.cpu()] = 0

                # é¢å¤–çš„ç»Ÿè®¡ä¿¡æ¯
                if 'dynamic_mask' in locals():
                    total_pixels = initial_depth[0].numel()
                    valid_pixels = (initial_depth[0] > 0).sum().item()
                    print(
                        f"  Valid depth pixels: {valid_pixels}/{total_pixels} ({valid_pixels / total_pixels * 100:.1f}%)")

                return initial_depth[0].numpy()
            else:
                depth = depth.detach().clone()
                opacity = opacity.detach()
                initial_depth = depth

                # æ·±åº¦å¤„ç†
                render_depth = initial_depth.cpu().numpy()[0]
                initial_depth, scale_factor, error_mask, num_accurate_pixels = process_depth(
                    render_depth, viewpoint.mono_depth,
                    last_depth=viewpoint_last.mono_depth,
                    im1=viewpoint_last.original_image,
                    im2=viewpoint.original_image,
                    model=self.model,
                    patch_size=self.config["depth"]["patch_size"],
                    mean_threshold=self.config["depth"]["mean_threshold"],
                    std_threshold=self.config["depth"]["std_threshold"],
                    error_threshold=self.config["depth"]["error_threshold"],
                    final_error_threshold=self.config["depth"]["final_error_threshold"],
                    min_accurate_pixels_ratio=self.config["depth"]["min_accurate_pixels_ratio"]
                )

                viewpoint.mono_depth = viewpoint.mono_depth * scale_factor

                # åº”ç”¨å®Œæ•´çš„æ©ç ï¼ˆåŒ…æ‹¬RGBè¾¹ç•Œå’ŒåŠ¨æ€ç‰©ä½“ï¼‰
                valid_rgb_np = valid_rgb.cpu().numpy() if isinstance(valid_rgb, torch.Tensor) else valid_rgb
                if initial_depth.shape == valid_rgb_np.shape[1:]:
                    initial_depth[~valid_rgb_np[0]] = 0

            return initial_depth

        # ä½¿ç”¨ground truthæ·±åº¦
        initial_depth = torch.from_numpy(viewpoint.depth).unsqueeze(0)
        # åº”ç”¨æ©ç ï¼ˆvalid_rgbå·²ç»åŒ…å«äº†åŠ¨æ€ç‰©ä½“æ’é™¤ï¼‰
        initial_depth[~valid_rgb.cpu()] = 0

        return initial_depth[0].numpy() if 'initial_depth' in locals() else None

    def tracking(self, cur_frame_idx, viewpoint):
        """è·Ÿè¸ªå‡½æ•°ï¼ŒåŒ…å«å®Œæ•´çš„åŠ¨é™æ€ç‰©ä½“æ£€æµ‹å’Œå¯è§†åŒ–"""
        # ç”ŸæˆåŠ¨æ€ç‰©ä½“é®ç½©å’Œå®Œæ•´çš„å¯è§†åŒ–ï¼ˆä¸»è¦ç”¨äºç»Ÿè®¡å’Œå¯è§†åŒ–ï¼‰
        if self.enable_dynamic_filtering:
            gt_img = viewpoint.original_image
            img_np = gt_img.permute(1, 2, 0).cpu().numpy()
            img_np = (img_np * 255).astype(np.uint8)

            # è·å–å®Œæ•´çš„æ£€æµ‹å’Œå¯è§†åŒ–ç»“æœ
            static_mask_np, repaired_image_np, detected_objects, colorful_results = self.dynamic_masker.get_static_mask_for_gaussian_init(
                img_np, frame_idx=cur_frame_idx
            )

            dynamic_mask = torch.from_numpy(1 - static_mask_np).to(self.device).bool()
            static_mask = torch.from_numpy(static_mask_np).to(self.device).bool()

            viewpoint.dynamic_mask = dynamic_mask
            viewpoint.static_mask = static_mask
            viewpoint.colorful_results = colorful_results  # ğŸ¨ å­˜å‚¨å½©è‰²å¯è§†åŒ–ç»“æœ

            # å­˜å‚¨æ£€æµ‹åˆ°çš„ç‰©ä½“ä¿¡æ¯
            if detected_objects:
                viewpoint.detected_objects = detected_objects

            static_ratio = viewpoint.static_mask.float().mean().item()
            dynamic_count = len([obj for obj in detected_objects if obj.get('type') == 'dynamic'])
            static_count = len([obj for obj in detected_objects if obj.get('type') == 'static'])

            print(f"ğŸ¬ Tracking frame {cur_frame_idx}: Static ratio={static_ratio:.1%}")
            print(f"    Objects: {dynamic_count} dynamic, {static_count} static detected")

        # åŸæœ‰çš„è·Ÿè¸ªé€»è¾‘
        prev = self.cameras[cur_frame_idx - self.use_every_n_frames]
        pose_prev = getWorld2View2(prev.R, prev.T)

        last_keyframe_idx = self.current_window[0]
        last_kf = self.cameras[last_keyframe_idx]
        pose_last_kf = getWorld2View2(last_kf.R, last_kf.T)
        img1 = last_kf.original_image

        img2 = viewpoint.original_image
        rel_pose, render_depth = get_pose(
            img1=img1, img2=img2, model=self.model,
            dist_coeffs=self.dataset.dist_coeffs,
            viewpoint=last_kf, gaussians=self.gaussians,
            pipeline_params=self.pipeline_params, background=self.background
        )

        viewpoint.mono_depth = get_depth(img2, img2, self.model)

        identity_matrix = torch.eye(4, device=self.device)
        rel_pose = torch.from_numpy(rel_pose).to(self.device).float()

        if torch.allclose(rel_pose, identity_matrix, atol=1e-6):
            pose_init = rel_pose @ pose_last_kf
            viewpoint.update_RT(prev.R, prev.T)
        else:
            pose_init = rel_pose @ pose_last_kf
            viewpoint.update_RT(pose_init[:3, :3], pose_init[:3, 3])

        # ä¼˜åŒ–å‚æ•°
        opt_params = []
        opt_params.append({
            "params": [viewpoint.cam_rot_delta],
            "lr": self.config["Training"]["lr"]["cam_rot_delta"],
            "name": "rot_{}".format(viewpoint.uid),
        })
        opt_params.append({
            "params": [viewpoint.cam_trans_delta],
            "lr": self.config["Training"]["lr"]["cam_trans_delta"],
            "name": "trans_{}".format(viewpoint.uid),
        })
        opt_params.append({
            "params": [viewpoint.exposure_a],
            "lr": 0.01,
            "name": "exposure_a_{}".format(viewpoint.uid),
        })
        opt_params.append({
            "params": [viewpoint.exposure_b],
            "lr": 0.01,
            "name": "exposure_b_{}".format(viewpoint.uid),
        })

        pose_optimizer = torch.optim.Adam(opt_params)

        for tracking_itr in range(self.tracking_itr_num):
            render_pkg = render(
                viewpoint, self.gaussians, self.pipeline_params, self.background
            )
            image, depth, opacity = (
                render_pkg["render"],
                render_pkg["depth"],
                render_pkg["opacity"],
            )

            pose_optimizer.zero_grad()

            # ===== åŠ¨æ€æ„ŸçŸ¥çš„æŸå¤±è®¡ç®— =====
            # æŸå¤±å‡½æ•°ç°åœ¨ä¼šè‡ªåŠ¨æ’é™¤åŠ¨æ€åŒºåŸŸï¼Œä¸é«˜æ–¯ä½“ç”Ÿæˆä¿æŒä¸€è‡´
            loss_tracking = get_loss_tracking(self.config, image, depth, opacity, viewpoint)
            # ===================================

            loss_tracking.backward()

            with torch.no_grad():
                pose_optimizer.step()
                converged = update_pose(viewpoint)

            if tracking_itr % 10 == 0:
                self.q_main2vis.put(
                    gui_utils.GaussianPacket(
                        current_frame=viewpoint,
                        gtcolor=viewpoint.original_image,
                        gtdepth=viewpoint.depth if not self.monocular
                        else np.zeros((viewpoint.image_height, viewpoint.image_width)),
                    )
                )
            if converged:
                break

        self.median_depth = get_median_depth(depth, opacity)
        return render_pkg

    def save_keyframe_mask(self, viewpoint, cur_frame_idx):
        """ä¿å­˜å…³é”®å¸§çš„æ©ç å¯è§†åŒ–ï¼Œç°åœ¨åŒ…å«å®Œæ•´çš„åŠ¨é™æ€å¯è§†åŒ–"""
        if (not self.enable_dynamic_filtering or
                not self.save_masked_images or
                not hasattr(viewpoint, 'dynamic_mask')):
            return

        try:
            # åˆ›å»ºå…³é”®å¸§ç›®å½•
            kf_dir = os.path.join(self.dynamic_masker.save_dir, "keyframes")
            os.makedirs(kf_dir, exist_ok=True)

            # è·å–åŸå§‹å›¾åƒ
            gt_image = viewpoint.original_image  # [3, H, W] tensor
            img_np = gt_image.permute(1, 2, 0).cpu().numpy()
            img_np = (img_np * 255).astype(np.uint8)
            img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)

            # è·å–masks
            dynamic_mask = viewpoint.dynamic_mask.cpu().numpy().astype(np.uint8)
            static_mask = viewpoint.static_mask.cpu().numpy().astype(np.uint8)

            if hasattr(viewpoint, 'expanded_dynamic_mask'):
                expanded_mask = viewpoint.expanded_dynamic_mask.cpu().numpy().astype(np.uint8)
            else:
                expanded_mask = dynamic_mask

            # åˆ›å»ºå¯è§†åŒ–
            vis_img = img_bgr.copy()

            # é™æ€åŒºåŸŸï¼ˆå°†è¢«é‡å»ºï¼‰ï¼šç»¿è‰²åŠé€æ˜å åŠ 
            static_overlay = np.zeros_like(vis_img)
            static_overlay[static_mask > 0] = [0, 255, 0]  # ç»¿è‰²
            vis_img = cv2.addWeighted(vis_img, 0.7, static_overlay, 0.3, 0)

            # åŠ¨æ€åŒºåŸŸï¼ˆè¢«maskæ‰ï¼‰ï¼šçº¢è‰²
            vis_img[dynamic_mask > 0] = [0, 0, 255]  # çº¢è‰²

            # æ‰©å±•åŒºåŸŸï¼ˆå®‰å…¨è¾¹ç•Œï¼‰ï¼šé»„è‰²
            vis_img[(expanded_mask > 0) & (dynamic_mask == 0)] = [0, 255, 255]  # é»„è‰²

            # æ·»åŠ æ–‡å­—è¯´æ˜
            cv2.putText(vis_img, f"Frame {cur_frame_idx} - Keyframe",
                        (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            cv2.putText(vis_img, "Green: Static (Reconstructed)",
                        (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            cv2.putText(vis_img, "Red: Dynamic (Masked Out)",
                        (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)

            # å¦‚æœæœ‰æ£€æµ‹åˆ°çš„ç‰©ä½“ï¼Œæ·»åŠ æ ‡ç­¾
            if hasattr(viewpoint, 'detected_objects'):
                y_offset = 120
                dynamic_count = len([obj for obj in viewpoint.detected_objects if obj.get('type') == 'dynamic'])
                static_count = len([obj for obj in viewpoint.detected_objects if obj.get('type') == 'static'])

                cv2.putText(vis_img, f"Dynamic Objects: {dynamic_count}",
                            (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 1)
                y_offset += 25
                cv2.putText(vis_img, f"Static Objects: {static_count}",
                            (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1)
                y_offset += 25

                # æ˜¾ç¤ºå‰å‡ ä¸ªæ£€æµ‹åˆ°çš„å¯¹è±¡
                for obj in viewpoint.detected_objects[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                    type_icon = "ğŸ”´" if obj.get('type') == 'dynamic' else "ğŸ”µ"
                    label = f"{type_icon} {obj['label']}: {obj['score']:.2f}"
                    cv2.putText(vis_img, f"  {label}",
                                (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
                    y_offset += 20

            # ä¿å­˜å›¾åƒ
            kf_path = os.path.join(kf_dir, f"keyframe_{cur_frame_idx:06d}_mask_vis.jpg")
            cv2.imwrite(kf_path, vis_img)

            print(f"ğŸ’¾ Saved enhanced keyframe visualization for frame {cur_frame_idx}")

        except Exception as e:
            print(f"Warning: Failed to save keyframe mask for frame {cur_frame_idx}: {e}")

    def is_keyframe(self, cur_frame_idx, last_keyframe_idx, cur_frame_visibility_filter, occ_aware_visibility):
        """è€ƒè™‘å®Œæ•´åŠ¨é™æ€ç‰©ä½“æ£€æµ‹çš„å…³é”®å¸§é€‰æ‹©ç­–ç•¥"""
        kf_translation = self.config["Training"]["kf_translation"]
        kf_min_translation = self.config["Training"]["kf_min_translation"]
        kf_overlap = self.config["Training"]["kf_overlap"]

        curr_frame = self.cameras[cur_frame_idx]
        last_kf = self.cameras[last_keyframe_idx]

        # è®¡ç®—ç›¸æœºè¿åŠ¨
        pose_CW = getWorld2View2(curr_frame.R, curr_frame.T)
        last_kf_CW = getWorld2View2(last_kf.R, last_kf.T)
        last_kf_WC = torch.linalg.inv(last_kf_CW)
        dist = torch.norm((pose_CW @ last_kf_WC)[0:3, 3])

        dist_check = dist > kf_translation * self.median_depth
        dist_check2 = dist > kf_min_translation * self.median_depth

        # è®¡ç®—é‡å åº¦
        union = torch.logical_or(
            cur_frame_visibility_filter, occ_aware_visibility[last_keyframe_idx]
        ).count_nonzero()
        intersection = torch.logical_and(
            cur_frame_visibility_filter, occ_aware_visibility[last_keyframe_idx]
        ).count_nonzero()

        # ===== åŠ¨æ€åœºæ™¯çš„å…³é”®å¸§ç­–ç•¥è°ƒæ•´ =====
        adjusted_overlap = kf_overlap

        if hasattr(curr_frame, 'expanded_static_mask'):
            # æ£€æŸ¥æ‰©å±•åçš„é™æ€åŒºåŸŸæ¯”ä¾‹
            static_ratio = curr_frame.expanded_static_mask.float().mean().item()
            if static_ratio < 0.3:
                # é™æ€åŒºåŸŸå¤ªå°‘ï¼ˆåŒ…æ‹¬åŠ¨é™æ€å®Œæ•´æ£€æµ‹åï¼‰ï¼Œæ›´ç§¯æåˆ›å»ºå…³é”®å¸§
                adjusted_overlap = kf_overlap * 0.7
                print(
                    f"ğŸ”„ Limited static region ({static_ratio:.1%}) after complete detection, adjusted overlap: {adjusted_overlap:.3f}")
        # ==========================================

        point_ratio_2 = intersection / union
        return (point_ratio_2 < adjusted_overlap and dist_check2) or dist_check

    def add_to_window(self, cur_frame_idx, cur_frame_visibility_filter, occ_aware_visibility, window):
        # åŸæœ‰å®ç°ä¿æŒä¸å˜
        N_dont_touch = 2
        window = [cur_frame_idx] + window
        curr_frame = self.cameras[cur_frame_idx]
        to_remove = []
        removed_frame = None

        for i in range(N_dont_touch, len(window)):
            kf_idx = window[i]
            intersection = torch.logical_and(
                cur_frame_visibility_filter, occ_aware_visibility[kf_idx]
            ).count_nonzero()
            denom = min(
                cur_frame_visibility_filter.count_nonzero(),
                occ_aware_visibility[kf_idx].count_nonzero(),
            )
            point_ratio_2 = intersection / denom
            cut_off = self.config["Training"].get("kf_cutoff", 0.4)
            if not self.initialized:
                cut_off = 0.4
            if (point_ratio_2 <= cut_off) and (len(window) > self.config["Training"]["window_size"]):
                to_remove.append(kf_idx)

        if to_remove:
            window.remove(to_remove[-1])
            removed_frame = to_remove[-1]

        kf_0_WC = torch.linalg.inv(getWorld2View2(curr_frame.R, curr_frame.T))

        if len(window) > self.config["Training"]["window_size"]:
            inv_dist = []
            for i in range(N_dont_touch, len(window)):
                inv_dists = []
                kf_i_idx = window[i]
                kf_i = self.cameras[kf_i_idx]
                kf_i_CW = getWorld2View2(kf_i.R, kf_i.T)
                for j in range(N_dont_touch, len(window)):
                    if i == j:
                        continue
                    kf_j_idx = window[j]
                    kf_j = self.cameras[kf_j_idx]
                    kf_j_WC = torch.linalg.inv(getWorld2View2(kf_j.R, kf_j.T))
                    T_CiCj = kf_i_CW @ kf_j_WC
                    inv_dists.append(1.0 / (torch.norm(T_CiCj[0:3, 3]) + 1e-6).item())
                T_CiC0 = kf_i_CW @ kf_0_WC
                k = torch.sqrt(torch.norm(T_CiC0[0:3, 3])).item()
                inv_dist.append(k * sum(inv_dists))

            idx = np.argmax(inv_dist)
            removed_frame = window[N_dont_touch + idx]
            window.remove(removed_frame)

        return window, removed_frame

    def request_keyframe(self, cur_frame_idx, viewpoint, current_window, depthmap):
        msg = ["keyframe", cur_frame_idx, viewpoint, current_window, depthmap, self.theta]
        self.backend_queue.put(msg)
        self.requested_keyframe += 1

    def reqeust_mapping(self, cur_frame_idx, viewpoint):
        msg = ["map", cur_frame_idx, viewpoint]
        self.backend_queue.put(msg)

    def request_init(self, cur_frame_idx, viewpoint, depth_map):
        msg = ["init", cur_frame_idx, viewpoint, depth_map]
        self.backend_queue.put(msg)
        self.requested_init = True

    def sync_backend(self, data):
        self.gaussians = data[1]
        occ_aware_visibility = data[2]
        keyframes = data[3]
        self.occ_aware_visibility = occ_aware_visibility

        for kf_id, kf_R, kf_T in keyframes:
            self.cameras[kf_id].update_RT(kf_R.clone(), kf_T.clone())

    def cleanup(self, cur_frame_idx):
        self.cameras[cur_frame_idx].clean()
        if cur_frame_idx % 10 == 0:
            torch.cuda.empty_cache()

    def initialize(self, cur_frame_idx, viewpoint):
        self.initialized = not self.monocular
        self.kf_indices = []
        self.iteration_count = 0
        self.occ_aware_visibility = {}
        self.current_window = []

        while not self.backend_queue.empty():
            self.backend_queue.get()

        viewpoint.update_RT(viewpoint.R_gt, viewpoint.T_gt)

        img = viewpoint.original_image
        viewpoint.mono_depth = get_depth(img, img, self.model)

        # åœ¨åˆå§‹åŒ–é˜¶æ®µå°±åº”ç”¨å®Œæ•´çš„åŠ¨é™æ€ç‰©ä½“æ£€æµ‹å’Œå¯è§†åŒ–
        print(f"ğŸ”„ INITIALIZING with frame {cur_frame_idx}")
        if self.enable_dynamic_filtering and self.filter_initialization:
            print("  âœ… Complete Dynamic/Static Object Detection ENABLED for initialization")
            print("  ğŸ¨ Colorful visualization for both dynamic and static objects")
            print("  ğŸ› ï¸  Ground shadows will be repaired automatically")
            print(f"  ğŸ¯ Scene type: {self.dynamic_masker.prompt_manager.current_scene}")
        elif self.enable_dynamic_filtering and not self.filter_initialization:
            print("  âš ï¸  Dynamic filtering enabled but SKIPPING initialization frame")
        else:
            print("  âŒ Dynamic filtering DISABLED - cars may appear as ghosts!")

        self.kf_indices = []
        depth_map = self.add_new_keyframe(cur_frame_idx, init=True)
        self.request_init(cur_frame_idx, viewpoint, depth_map)
        self.reset = False

    def run(self):
        # ä¸»æ‰§è¡Œå¾ªç¯é›†æˆäº†å®Œæ•´çš„åŠ¨é™æ€ç‰©ä½“æ£€æµ‹å’Œå½©è‰²å¯è§†åŒ–
        cur_frame_idx = 0
        projection_matrix = getProjectionMatrix2(
            znear=0.01, zfar=100.0,
            fx=self.dataset.fx, fy=self.dataset.fy,
            cx=self.dataset.cx, cy=self.dataset.cy,
            W=self.dataset.width, H=self.dataset.height,
        ).transpose(0, 1)
        projection_matrix = projection_matrix.to(device=self.device)

        tic = torch.cuda.Event(enable_timing=True)
        toc = torch.cuda.Event(enable_timing=True)

        while True:
            # å¤„ç†GUIæ¶ˆæ¯
            if self.q_vis2main.empty():
                if self.pause:
                    continue
            else:
                data_vis2main = self.q_vis2main.get()
                self.pause = data_vis2main.flag_pause
                if self.pause:
                    self.backend_queue.put(["pause"])
                    continue
                else:
                    self.backend_queue.put(["unpause"])

            if self.frontend_queue.empty():
                tic.record()

                # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                if cur_frame_idx >= len(self.dataset):
                    if self.save_results:
                        eval_ate(self.cameras, self.kf_indices, self.save_dir, 0,
                                 final=True, monocular=self.monocular)
                        save_gaussians(self.gaussians, self.save_dir, "final", final=True)

                    # ğŸ¨ ä¿å­˜æœ€ç»ˆçš„å½©è‰²å¯è§†åŒ–ç»Ÿè®¡æŠ¥å‘Š
                    if self.enable_dynamic_filtering and self.save_masked_images:
                        self._save_final_colorful_statistics()

                    break

                # ç­‰å¾…åˆå§‹åŒ–
                if self.requested_init:
                    time.sleep(0.01)
                    continue

                if self.single_thread and self.requested_keyframe > 0:
                    time.sleep(0.01)
                    continue

                if not self.initialized and self.requested_keyframe > 0:
                    time.sleep(0.01)
                    continue

                # å¤„ç†å½“å‰å¸§
                viewpoint = Camera.init_from_dataset(
                    self.dataset, cur_frame_idx, projection_matrix
                )
                viewpoint.compute_grad_mask(self.config)
                self.cameras[cur_frame_idx] = viewpoint

                # åˆå§‹åŒ–
                if self.reset:
                    self.initialize(cur_frame_idx, viewpoint)
                    self.current_window.append(cur_frame_idx)
                    cur_frame_idx += 1
                    continue

                self.initialized = self.initialized or (len(self.current_window) == self.window_size)

                # è·Ÿè¸ª
                render_pkg = self.tracking(cur_frame_idx, viewpoint)
                current_window_dict = {}
                current_window_dict[self.current_window[0]] = self.current_window[1:]
                keyframes = [self.cameras[kf_idx] for kf_idx in self.current_window]

                self.q_main2vis.put(
                    gui_utils.GaussianPacket(
                        gaussians=clone_obj(self.gaussians),
                        current_frame=viewpoint,
                        keyframes=keyframes,
                        kf_window=current_window_dict,
                    )
                )

                if self.requested_keyframe > 0:
                    self.cleanup(cur_frame_idx)
                    cur_frame_idx += 1
                    continue

                # å…³é”®å¸§é€‰æ‹©
                last_keyframe_idx = self.current_window[0]
                check_time = (cur_frame_idx - last_keyframe_idx) >= self.kf_interval
                curr_visibility = (render_pkg["n_touched"] > 0).long()

                create_kf = self.is_keyframe(
                    cur_frame_idx, last_keyframe_idx,
                    curr_visibility, self.occ_aware_visibility,
                )

                if len(self.current_window) < self.window_size:
                    union = torch.logical_or(
                        curr_visibility, self.occ_aware_visibility[last_keyframe_idx]
                    ).count_nonzero()
                    intersection = torch.logical_and(
                        curr_visibility, self.occ_aware_visibility[last_keyframe_idx]
                    ).count_nonzero()
                    point_ratio = intersection / union
                    create_kf = (check_time and
                                 point_ratio < self.config["Training"]["kf_overlap"])

                if self.single_thread:
                    create_kf = check_time and create_kf

                if create_kf:
                    self.current_window, removed = self.add_to_window(
                        cur_frame_idx, curr_visibility,
                        self.occ_aware_visibility, self.current_window,
                    )
                    depth_map = self.add_new_keyframe(
                        cur_frame_idx,
                        depth=render_pkg["depth"],
                        opacity=render_pkg["opacity"],
                        init=False,
                    )
                    self.request_keyframe(
                        cur_frame_idx, viewpoint, self.current_window, depth_map
                    )

                    # ä¸ºå…³é”®å¸§ä¿å­˜ç‰¹æ®Šæ ‡è®°çš„æ©ç å›¾åƒï¼ˆåŒ…å«å®Œæ•´çš„åŠ¨é™æ€å¯è§†åŒ–ï¼‰
                    self.save_keyframe_mask(viewpoint, cur_frame_idx)
                else:
                    self.cleanup(cur_frame_idx)

                cur_frame_idx += 1

                # è¯„ä¼°
                if (self.save_results and self.save_trj and create_kf and
                        len(self.kf_indices) % self.save_trj_kf_intv == 0):
                    Log("Evaluating ATE at frame: ", cur_frame_idx)
                    eval_ate(self.cameras, self.kf_indices, self.save_dir,
                             cur_frame_idx, monocular=self.monocular)

                toc.record()
                torch.cuda.synchronize()
                if create_kf:
                    duration = tic.elapsed_time(toc)
                    time.sleep(max(0.01, 1.0 / 3.0 - duration / 1000))

            else:
                # å¤„ç†åç«¯æ¶ˆæ¯
                data = self.frontend_queue.get()
                if data[0] == "sync_backend":
                    self.sync_backend(data)
                elif data[0] == "keyframe":
                    self.sync_backend(data)
                    self.requested_keyframe -= 1
                elif data[0] == "init":
                    self.sync_backend(data)
                    self.requested_init = False
                elif data[0] == "stop":
                    Log("Frontend Stopped.")
                    break

    def _save_final_colorful_statistics(self):
        """ä¿å­˜æœ€ç»ˆçš„å½©è‰²å¯è§†åŒ–ç»Ÿè®¡æŠ¥å‘Š"""
        try:
            stats_path = os.path.join(self.dynamic_masker.save_dir, "final_colorful_statistics.txt")

            # æ”¶é›†æ‰€æœ‰æ£€æµ‹åˆ°çš„å¯¹è±¡ç»Ÿè®¡
            all_dynamic_objects = {}
            all_static_objects = {}
            total_frames = 0

            for frame_idx, camera in self.cameras.items():
                if hasattr(camera, 'detected_objects'):
                    total_frames += 1
                    for obj in camera.detected_objects:
                        obj_type = obj.get('type', 'unknown')
                        label = obj['label']

                        if obj_type == 'dynamic':
                            if label not in all_dynamic_objects:
                                all_dynamic_objects[label] = {'count': 0, 'total_confidence': 0.0}
                            all_dynamic_objects[label]['count'] += 1
                            all_dynamic_objects[label]['total_confidence'] += obj['score']
                        elif obj_type == 'static':
                            if label not in all_static_objects:
                                all_static_objects[label] = {'count': 0, 'total_confidence': 0.0}
                            all_static_objects[label]['count'] += 1
                            all_static_objects[label]['total_confidence'] += obj['score']

            with open(stats_path, 'w') as f:
                f.write("Final Colorful Segmentation Statistics\n")
                f.write("=" * 50 + "\n\n")
                f.write(f"Total Processed Frames: {total_frames}\n")
                f.write(f"Scene Type: {self.dynamic_masker.prompt_manager.current_scene}\n\n")

                # åŠ¨æ€å¯¹è±¡ç»Ÿè®¡
                f.write("Dynamic Objects Summary:\n")
                f.write("-" * 30 + "\n")
                if all_dynamic_objects:
                    for label, stats in sorted(all_dynamic_objects.items(), key=lambda x: x[1]['count'], reverse=True):
                        avg_conf = stats['total_confidence'] / stats['count']
                        freq = stats['count'] / total_frames * 100
                        f.write(
                            f"{label:20}: {stats['count']:4d} detections ({freq:5.1f}% frames) - Avg Conf: {avg_conf:.3f}\n")
                else:
                    f.write("No dynamic objects detected.\n")

                f.write("\nStatic Objects Summary:\n")
                f.write("-" * 30 + "\n")
                if all_static_objects:
                    for label, stats in sorted(all_static_objects.items(), key=lambda x: x[1]['count'], reverse=True):
                        avg_conf = stats['total_confidence'] / stats['count']
                        freq = stats['count'] / total_frames * 100
                        f.write(
                            f"{label:20}: {stats['count']:4d} detections ({freq:5.1f}% frames) - Avg Conf: {avg_conf:.3f}\n")
                else:
                    f.write("No static objects detected.\n")

                # æ€»ä½“ç»Ÿè®¡
                total_dynamic_detections = sum(stats['count'] for stats in all_dynamic_objects.values())
                total_static_detections = sum(stats['count'] for stats in all_static_objects.values())

                f.write("\nOverall Summary:\n")
                f.write("-" * 20 + "\n")
                f.write(f"Total Dynamic Object Classes: {len(all_dynamic_objects)}\n")
                f.write(f"Total Static Object Classes: {len(all_static_objects)}\n")
                f.write(f"Total Dynamic Detections: {total_dynamic_detections}\n")
                f.write(f"Total Static Detections: {total_static_detections}\n")
                f.write(
                    f"Average Detections per Frame: {(total_dynamic_detections + total_static_detections) / total_frames:.2f}\n")

            print(f"ğŸ“Š Saved final colorful statistics to: {stats_path}")

        except Exception as e:
            print(f"âŒ Failed to save final colorful statistics: {e}")


# ä½¿ç”¨ç¤ºä¾‹å’Œé…ç½®å»ºè®®
def create_enhanced_config_example():
    """
    åˆ›å»ºå¢å¼ºé…ç½®æ–‡ä»¶ç¤ºä¾‹ï¼ŒåŒ…å«å®Œæ•´çš„åŠ¨é™æ€ç‰©ä½“æ£€æµ‹å’Œå½©è‰²å¯è§†åŒ–åŠŸèƒ½
    """
    config_example = {
        "dynamic_filtering": {
            "enabled": True,
            "filter_initialization": True,
            "save_masked_images": True,
            "use_ground_segmentation": True,
            "scene_type": "outdoor_street",  # å¯é€‰: parking_lot, highway, residential, indoor, construction, campus

            # Grounding DINO é…ç½®
            "grounding_dino_model": "/path/to/your/groundingdino_swint_ogc.pth",  # æœ¬åœ°æ¨¡å‹è·¯å¾„
            # "grounding_dino_model": "IDEA-Research/grounding-dino-tiny",  # æˆ–ä½¿ç”¨ Hugging Face æ¨¡å‹
            "grounding_dino_config": "/path/to/config/GroundingDINO_SwinT_OGC.py",  # é…ç½®æ–‡ä»¶è·¯å¾„

            # SAM é…ç½®
            "use_sam": True,
            "sam_checkpoint": "/home/zwk/ä¸‹è½½/S3PO-GS-main/sam_vit_h_4b8939.pth",

            # ä¿å­˜å’Œå¯è§†åŒ–é…ç½®
            "save_dir": "./enhanced_detection_results",

            # ğŸ¨ æ–°å¢ï¼šå®Œæ•´çš„å½©è‰²å¯è§†åŒ–é…ç½®
            "enable_colorful_visualization": True,
            "detect_static_objects": True,
            "colorful_overlay_alpha": 0.6,
            "save_class_separated": True,
            "create_legend": True,
        },

        # å…¶ä»–åŸæœ‰é…ç½®ä¿æŒä¸å˜
        "Training": {
            "monocular": True,
            "rgb_boundary_threshold": 0.01,
            # ... å…¶ä»–è®­ç»ƒå‚æ•°
        },

        "Results": {
            "save_results": True,
            "save_dir": "./results",
            # ... å…¶ä»–ç»“æœå‚æ•°
        }
    }

    return config_example


# ä¸»å‡½æ•°ç¤ºä¾‹
def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨å¢å¼ºçš„åŠ¨æ€ç‰©ä½“é®ç½©å™¨"""

    print("ğŸš€ Starting Enhanced Dynamic Object Masking with Complete Visualization")
    print("=" * 70)

    # 1. åŠ è½½é…ç½®
    config = create_enhanced_config_example()

    # 2. åˆå§‹åŒ–æ¨¡å‹å’Œæ•°æ®é›†
    model = None  # åŠ è½½ä½ çš„ MASt3R æ¨¡å‹
    dataset = None  # åŠ è½½ä½ çš„æ•°æ®é›†

    # 3. åˆ›å»º FrontEnd å®ä¾‹
    frontend = FrontEnd(config, model, save_dir=config["Results"]["save_dir"])

    print("ğŸ¯ Configuration Summary:")
    print(f"  - Dynamic filtering: {config['dynamic_filtering']['enabled']}")
    print(f"  - Colorful visualization: {config['dynamic_filtering']['enable_colorful_visualization']}")
    print(f"  - Static object detection: {config['dynamic_filtering']['detect_static_objects']}")
    print(f"  - Scene type: {config['dynamic_filtering']['scene_type']}")
    print(f"  - Ground segmentation: {config['dynamic_filtering']['use_ground_segmentation']}")
    print(f"  - SAM enabled: {config['dynamic_filtering']['use_sam']}")

    # 4. è¿è¡Œå¤„ç†
    try:
        frontend.run()
        print("âœ… Processing completed successfully!")

        # 5. è¾“å‡ºç»“æœè·¯å¾„
        results_dir = config["dynamic_filtering"]["save_dir"]
        print(f"\nğŸ“ Results saved to: {results_dir}")
        print("ğŸ“Š Generated visualizations:")
        print(f"  - Colorful combined: {results_dir}/colorful_combined/")
        print(f"  - Dynamic objects only: {results_dir}/colorful_dynamic_only/")
        print(f"  - Static objects only: {results_dir}/colorful_static_only/")
        print(f"  - Overlay visualization: {results_dir}/colorful_overlay/")
        print(f"  - With legends: {results_dir}/colorful_legend/")
        print(f"  - Analysis reports: {results_dir}/colorful_analysis/")
        print(f"  - Keyframe visualizations: {results_dir}/keyframes/")

    except Exception as e:
        print(f"âŒ Error during processing: {e}")
        raise


if __name__ == "__main__":
    main()